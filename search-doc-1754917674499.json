{"searchDocs":[{"title":"Contributors","type":0,"sectionRef":"#","url":"/docs/contribute/06_02_community","content":"","keywords":"","version":"Next"},{"title":"Facilitation and core team​","type":1,"pageTitle":"Contributors","url":"/docs/contribute/06_02_community#facilitation-and-core-team","content":" CB Cathleen Berger Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   CF Charlotte Freihse Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   CR Clara Ruthardt Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   JM Johannes Müller &amp;effect data solutions GmbH Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   ","version":"Next","tagName":"h2"},{"title":"Knowledge contributors​","type":1,"pageTitle":"Contributors","url":"/docs/contribute/06_02_community#knowledge-contributors","content":" AW Abel Wabella Inform Africa Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   AN Andreas Neumeier SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   AP Alessandro Polidoro attorney at law &amp; digital rights advocate Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   CA Christoph M. Abels University of Potsdam Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   CT Clarice Tavares InternetLab Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   CA CorrelAid e.V. We are bringing data into civil society Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   FV Fabio Votta ASCoR | University of Amsterdam Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   HT Heather Dannyelle Thompson Democracy Reporting International Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   IJ Iná Jost InternetLab Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   JR Jasmin Riedl SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   JB Johannes Breuer Senior Researcher, GESIS - Leibniz Institute for the Social Sciences Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   JH Josef Holnburger Center for Monitoring, Analysis and Strategy (CeMAS) Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   MD Martin Degeling Stiftung Neue Verantwortung Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   OKFN Open Knowledge Foundation OKFN Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   MD Philipp Darius Hertie School of Governance Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   RS Richard Schwenn polisphere Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   VH Valerie Hase Postdoctoral Scholar, LMU Munich Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   WD Wiebke Drews SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky  ","version":"Next","tagName":"h2"},{"title":"The Data Knowledge Hub for Researching Online Discourse","type":0,"sectionRef":"#","url":"/docs/background-rationale","content":"","keywords":"","version":"Next"},{"title":"Background and rationale: What, why, and how does it help you?​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/docs/background-rationale#background-and-rationale-what-why-and-how-does-it-help-you","content":"   CB Cathleen Berger Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   CF Charlotte Freihse Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   The Data Knowledge Hub for Researching Online Discourse (Data Knowledge Hub) is an initiative that aims to provide a central resource for researchers, social scientists, data scientists, journalists, practitioners, and policy makers interested in independently researching social media and online discourse more broadly.    ","version":"Next","tagName":"h2"},{"title":"Why do we feel this is necessary?​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/docs/background-rationale#why-do-we-feel-this-is-necessary","content":" Online discourse has changed how we inform ourselves, what and who to trust, as well as how information is quite simply accessed. Notably on online platforms and social media, recommender systems and other design features can be gamed to fuel disinformation, hate speech, and outrage. In addition, messaging services and alternative platforms are increasingly falling risk to exploitation and provide agitators with vast audiences to spread falsehoods. But how and why exactly this is happening remains under-researched and merely anecdotally illustrated. If we want to strengthen our information ecosystem and increase each other’s ability to decide what’s trustworthy and what’s not, we need to move away from anecdotes towards broad, continuous, and ideally real-time data-driven insight.  ","version":"Next","tagName":"h3"},{"title":"The challenge​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/docs/background-rationale#the-challenge","content":" Due to the increasing number of social media and other digital platforms as well as the huge amounts of data to analyse, it is critical to enable and empower more researchers, social as well as data scientists, on two fronts:   To conduct independent research of social media and online discourse on a technical level To assess the data from a socio-political context  There are already renowned, well-established organisations that do incredible work on Social Media Monitoring, including CeMAS, Democracy Reporting International, the SPARTA Project of the Bundeswehr University Munich, or the Institute for Strategic Dialogue. Yet even these established players face several challenges, among others:  the multitude of digital platforms;the sheer amount of data and necessary server capacities;fast-developing and constantly changing narratives;new and changing actors and agitators.  ","version":"Next","tagName":"h3"},{"title":"Building a foundation for solving these challenges​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/docs/background-rationale#building-a-foundation-for-solving-these-challenges","content":" To reduce the obstacles and lower the threshold to independently researching online discourse, we are launching this Data Knowledge Hub. Hosted open source and under a Creative Commons license on GitHub, it continuously welcomes contributions of new data, code, and written content, fostering a collaborative environment for all. Cooperation and collaboration on development, design, content, and scope among established actors is key to turning this Data Knowledge Hub into a useful tool and an enabler for future research.  For first publication in September 2023, we gathered initial contributions on legal basis and ethical standards, good practices and exemplary research for webscraping, data collection on Twitter and TikTok as well as code samples to monitor various platforms. This Data Knowledge Hub will be continuously updated and reviewed, and, with the help of community and crowdsourced contributions, we hope to include a broad range of samples and organic input, over time providing all relevant information for researching and understanding the dynamics of online discourse.  You can help and contribute, too We welcome additional contributions on a rolling basis. Right now, we would be particularly interested in including and discussing chapters on: Social media usage: users worldwide, number of posts/messages, regional differences etc.Data access and ethics: How to deal with dark socials?Data access rights beyond the European Union and the U.S. Data collection: sock puppet, snowball sampling and other innovative approachesExamples of data collection: Facebook, Instagram, Reddit, Fediverse and othersExamples of data analysis: Topic modelling, sentiment analysis, geospational analysis, infrastructure as code, and othersAdditional aspects that benefit from monitoring as a research method  ","version":"Next","tagName":"h3"},{"title":"Living document - How to navigate the Data Knowledge Hub​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/docs/background-rationale#living-document---how-to-navigate-the-data-knowledge-hub","content":"   JM Johannes Müller &amp;effect data solutions GmbH Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   The Data Knowledge Hub is hosted on a GitHub repository. For better usability we use a documentation framework which allows users to switch to a static website for easier reading, accessing content as a digital book. This means that all text content is created using Markdown. Code projects are included as a single file (e.g. a Jupyter Notebook) or in folders that can be pulled from GitHub. We intend to continuously update content and invite contributions on additional aspects of independently researching social media and online discourse. A first version was published in September 2023, chapters that are already in the pipeline are marked as “forthcoming”, a list of invited contributions can be found in the “editorial”.  All contributors are listed here as well as named in their respective chapters.  ","version":"Next","tagName":"h2"},{"title":"Code projects​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/docs/background-rationale#code-projects","content":" Here is a table with all projects that are currently included in the Data Knowledge Hub. Click on the link to go to the project page.  Project\tDescription\tLanguage\tPlattform\tCodetiktok-scraping\tCollect data on TikTok using puppeteer\tJavaScript\tTikTok\tCode tiktok-hashtag-analysis\tAnalyse TikTok hashtags\tPython\tTikTok\tCode blog-webscraping\tWebscraping using rvest and selenium\tR\tBlogs\tCode twitter-streaming\tLarge-scale data collection on X (Twitter)\tPython\tTwitter / X\tCode twitter-social-network\tSocial Network Analysis with R\tR\tTwitter / X\tCode  ","version":"Next","tagName":"h3"},{"title":"Design principles​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/docs/background-rationale#design-principles","content":" The editorial team has adopted four guiding principles for content on the Data Knowledge Hub:  From general to specific Cater to different target groups by starting each chapter with a general and easy-to-follow introduction. More specific topics such as content on use cases, projects, or code examples will be added throughout the project. We use three labels to indicate difficulty that will help users to orientate themselves: no code, beginners, advanced.  Rich links Enable non-linear interaction with internal and external links, highlighting diverse initiatives, projects, or code libraries.  Reproducibility For code examples, we focus on Python and R due to their widespread use in data science (however use cases in other languages are also welcome such as JavaScript, Julia or Rust). All code should be reproducible.  Open Source Content and code will be accessible on GitHub under a CC BY License.  ","version":"Next","tagName":"h3"},{"title":"Structure of the Data Knowledge Hub​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/docs/background-rationale#structure-of-the-data-knowledge-hub","content":" The structure of the hub is based on the different stages of a data analysis project:   Getting started Overview of legal and ethical considerations as well as tools you may use during your project. Data access and permissions Information on data access options available for each platform. Data collection methods Summary of data collection methods and tools, along with challenges, limitations, and potential. Data analysis projects Introduction to research designs and methods like natural language processing, network analysis, and machine learning. Literature and illustrative research Overview of literature and selected research and studies. How to contribute? Now it's your turn. Information on how you can support us and make your research available to the data knowledge community.  ","version":"Next","tagName":"h3"},{"title":"License​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/docs/background-rationale#license","content":"   This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.  Questions and improvements If you have any questions or ideas, please do not hesitate to contact us at upgrade.democracy@bertelsmann-stiftung.de. ","version":"Next","tagName":"h3"},{"title":"Roadmap","type":0,"sectionRef":"#","url":"/docs/contribute/06_04_roadmap","content":"","keywords":"","version":"Next"},{"title":"Releases​","type":1,"pageTitle":"Roadmap","url":"/docs/contribute/06_04_roadmap#releases","content":" Open for contributions We welcome contributions on a rolling basis. Have you done an anlysis or project you want to share? Get in touch with us at upgrade.democracy@bertelsmann-stiftung.de  ","version":"Next","tagName":"h2"},{"title":"Version 1.4 (Q1 2025)​","type":1,"pageTitle":"Roadmap","url":"/docs/contribute/06_04_roadmap#version-14-q1-2025","content":"  Update overall design and user interface Add more custom elements to the Data Knowledge Hub Update structure of the Data Knowledge Hub to improve user experience  ","version":"Next","tagName":"h3"},{"title":"Version 1.3 (Q4 2024)​","type":1,"pageTitle":"Roadmap","url":"/docs/contribute/06_04_roadmap#version-13-q4-2024","content":"  Add more ways to contribute to the Data Knowledge Hub New feature for more dynamic contribution panels Collect user feedback and update content library for a better user journey  ","version":"Next","tagName":"h3"},{"title":"Version 1.2 (Q3 2024)​","type":1,"pageTitle":"Roadmap","url":"/docs/contribute/06_04_roadmap#version-12-q3-2024","content":"  Release of open source repository with Data Knowledge Hub Projects  ","version":"Next","tagName":"h3"},{"title":"Version 1.1 (Q1 2024)​","type":1,"pageTitle":"Roadmap","url":"/docs/contribute/06_04_roadmap#version-11-q1-2024","content":"  Add brief tutorials and/or explainer videos to improve accessibility of content  ","version":"Next","tagName":"h3"},{"title":"Version 1.0 (Q4 2023)​","type":1,"pageTitle":"Roadmap","url":"/docs/contribute/06_04_roadmap#version-10-q4-2023","content":"  Publish all chapters of the initital Data Knowledge Hub Fixed some formatting issues  ","version":"Next","tagName":"h3"},{"title":"Version 0.9 (Q3 2023)​","type":1,"pageTitle":"Roadmap","url":"/docs/contribute/06_04_roadmap#version-09-q3-2023","content":"  Publish the Data Knowledge Hub Initial content from expert contributors Initial content from the Upgrade Democracy team Initial design of the Data Knowledge Hub ","version":"Next","tagName":"h3"},{"title":"Data access and permissions","type":0,"sectionRef":"#","url":"/docs/data-access/","content":"Data access and permissions Once you're familiar with the relevant ethical considerations and legal considerations for researching online discourse, the next step is accessing the actual data. To support your research, we’ve compiled an overview of the data access options available for each platform, helping you navigate the specific processes and policies that govern how data can be retrieved and used: Accessing data for independent research This chapter provides an introduction to accessing data for social media monitoring through the lens of platform regulations and policies. Learn more Challenges in accessing data in the Global South This chapter explores how limitations in data access are disproportionately affecting researchers in the Global South—highlighting 7 key obstacles that threaten academic freedom, data transparency, and equitable knowledge production. Learn more Open for contributions We welcome contributions on a rolling basis. At the moment, we particularly welcome chapters dealing with the following questions Data access rights beyond the European Union and the U.S.A collection of links to all existing APIsAn introduction to data donations as an alternative to API access","keywords":"","version":"Next"},{"title":"How to contribute?","type":0,"sectionRef":"#","url":"/docs/contribute/06_01_how-to-contribute","content":"","keywords":"","version":"Next"},{"title":"Options for contributing to the data knowledge hub​","type":1,"pageTitle":"How to contribute?","url":"/docs/contribute/06_01_how-to-contribute#options-for-contributing-to-the-data-knowledge-hub","content":" The Data Knowledge Hub is a collaborative open-source project. We always welcome contributions from the community – either via GitHub or by contacting us directly. To make participation in our community an open, welcoming, diverse, inclusive, and healthy experience for everyone, the Data Knowledge Hub is released with a Code of Conduct. By contributing to this project, you agree to abide by its terms.  ","version":"Next","tagName":"h2"},{"title":"Option 1: Pull the project and create a merge request on GitHub​","type":1,"pageTitle":"How to contribute?","url":"/docs/contribute/06_01_how-to-contribute#option-1-pull-the-project-and-create-a-merge-request-on-github","content":" Here's how you can contribute directly over GitHub:   1 Open the Data Knowledge Hub on GitHub. 2 Fork your own copy of the repository to your personal account. 3 Make your changes and commit them. 4 Open a pull request on GitHub and fill out all the necessary information. 5 Wait for the maintainers to review your changes. 6 Once your changes are approved, they will be merged into the main branch.  Please note Especially minor edits, corrections or smaller additions can be submitted very conveniently via GitHub. For longer contributions or a series of changes we would recommend that you reach out to us via Option 2.  ","version":"Next","tagName":"h3"},{"title":"Option 2: Contact us and we will integrate your changes and content together​","type":1,"pageTitle":"How to contribute?","url":"/docs/contribute/06_01_how-to-contribute#option-2-contact-us-and-we-will-integrate-your-changes-and-content-together","content":" We welcome contributions on a rolling basis. Whether you have a practical example you want to share, or just an idea for any content that is missing.  Get in touch with us at  upgrade.democracy@bertelsmann-stiftung.de  ","version":"Next","tagName":"h3"},{"title":"What content are we looking for currently?​","type":1,"pageTitle":"How to contribute?","url":"/docs/contribute/06_01_how-to-contribute#what-content-are-we-looking-for-currently","content":" The Data Knowledge Hub is a collaborative open-source project. We always welcome contributions from the community – either via GitHub or by contacting us directly. To make participation in our community an open, welcoming, diverse, inclusive, and healthy experience for everyone, the Data Knowledge Hub is released with a Code of Conduct. By contributing to this project, you agree to abide by its terms.  Contributions can take on a lot of different forms. In general, we are looking for:  Python or R Notebooks: Well-commented notebooks illustrating libraries or specific analysis processes.Practical Guidelines and Processes: Short blog posts (~1000-2000 words) with code examples, checklists and descriptions of processes. We are also interested in overviews or link collections, e.g. data access points or insightful research.Introductory Content and Context: Essays (~3000 words) providing context and practical information - such as legal and ethical guidelines, profiles for specific platforms, or research methodologies.  Please note Right now, we would be particularly interested in including and discussing chapters on: Data access and ethics: Data access rights beyond the European Union and the U.S.How to deal with dark socials? Data collection: sock puppet, snowball sampling and other innovative approachesExamples of data collection: Facebook, Instagram, Reddit, Fediverse and othersExamples of data analysis: Topic modelling, sentiment analysis, geospatial analysis, infrastructure as code, and othersAdditional aspects that benefit from monitoring as a research method ","version":"Next","tagName":"h2"},{"title":"Accessing data for independent research: Availability, limitations, and outlook","type":0,"sectionRef":"#","url":"/docs/data-access/02_02_overview-access","content":"","keywords":"","version":"Next"},{"title":"Introduction: A matter of access​","type":1,"pageTitle":"Accessing data for independent research: Availability, limitations, and outlook","url":"/docs/data-access/02_02_overview-access#introduction-a-matter-of-access","content":" Social media is integral to modern communication and democratic discourse. It reveals us to ourselves, documenting our emotions, opinions, political stances, and behaviours, albeit often to an extreme and distorted extent. Since the rise of social media, researchers from varied disciplines and institutions have sought to study it and its impact on, or reflection of, society.  And yet, researchers, particularly non-academic and independent researchers have struggled for meaningful research access to platform data. A variety of researchers can only serve public interest by varying available analyses and results by length of study, research scope, context, and language, to name a few.  After years of debate, the European Union’s Digital Services Act (DSA) entered into force on August 25th, 2023. It begins the work of regulating Big Tech, particularly social media platforms. Among other imperatives, the DSA addresses the existing information asymmetry between the public and VLOPs (very large online platforms) by compelling them to grant data access to researchers (see Article 40, Data access and scrutiny).  Importantly, VLOPs are no longer the arbiters of data access. Digital Service Coordinators (DSCs), designated by each Member State, are tasked to vet independent researchers for access. This separation from platform control will allow for research from a critical standpoint on the platforms’ design and the occurrence of ‘systemic risks’ (DSA Article 34).  This opens more opportunities for civil society to participate in quantitative social media research with fewer barriers to access. By analysing the engagement, sentiment, and reach of content on social platforms, non-academic researchers can gauge ongoing and evolving public sentiment and empower policymakers to make informed decisions by responding actively to changing social dynamics.  ","version":"Next","tagName":"h2"},{"title":"Availability: What can you access and how?​","type":1,"pageTitle":"Accessing data for independent research: Availability, limitations, and outlook","url":"/docs/data-access/02_02_overview-access#availability-what-can-you-access-and-how","content":" With the permanent shutdown of the crowd listening tool CrowdTangle provided for Meta products, the main avenue of legal data access is through application programming interfaces, or APIs. APIs require coding skills, but their use allows researchers a lot of flexibility in the way they can gather data and to gather it more precisely.  All major social media platforms (Meta, Twitter/X, TikTok, YouTube, and Telegram) offer API access, though some have accessibility restrictions. TikTok, for instance, used to only offers an API for developers. A parallel TikTok API for researchers is now available for European researchers, but continues to be irreliable and has significant data quality issues.  Here is an overview of data access by each platform. See more details in DRI’s data access series.    Overview of data access by each platform  ","version":"Next","tagName":"h2"},{"title":"Limitations: What is missing?​","type":1,"pageTitle":"Accessing data for independent research: Availability, limitations, and outlook","url":"/docs/data-access/02_02_overview-access#limitations-what-is-missing","content":" With the DSA being in force, mandating access to data, everything is solved, right? Not so much. There are still limitations, ambiguity, and technicalities that need to be ironed out.  It is not yet clear how new researchers will be vetted. While the DSA has described the ultimate direction, the practical implementation is left to what is called a ‘Delegated Act’. In response, civil society has responded with recommendations and public petitions. At DRI, ours include providing access to all public data, regular updates to APIs, granting access to non-academic researchers, not just developers, and better-informed Terms of Service adhering to the DSA. The German DSC has promised to adopt its Delegated Act by mid-April 2025.  In the meantime, many platforms currently still limit the type of data accessible through their APIs, even data that is public in nature. The primary blind spots are features like the comment sections under posts (which notoriously tend to hold more problematic or illegal content than the post itself), profiles set to ‘public’ on Facebook, new features such as ‘Stories’. See below for a breakdown of the type of data you can access by platform.    Overview of the type of data accessible by platform  The biggest gap now comes from the newly minted X.com (the artist formerly known as Twitter). In 2024, X imposed a paywall for its API access which is prohibitively expensive (the lowest, and most limited tier in terms of quantity of data, is 200 U.S. dollars per month). This year X has left a trail of non-compliance both with the impending DSA and the voluntary Code of Practice on Disinformation, representing a stark contrast to the changing tide. The platform will likely face fines from the EU for its actions.  ","version":"Next","tagName":"h2"},{"title":"Outlook: What can you do now (including in terms of advocacy)?​","type":1,"pageTitle":"Accessing data for independent research: Availability, limitations, and outlook","url":"/docs/data-access/02_02_overview-access#outlook-what-can-you-do-now-including-in-terms-of-advocacy","content":" The Delegated Act detailing the new provisions for vetting and securing access is expected to be released in early 2025. In the meantime, social media research is still a vital area of research.  If you are interested in improving research access and social media research on global online political discourse, sign up for DRI's newsletter, The Digital Drop and reach out to the Data Knowledge Hub team to strengthen our network efforts. ","version":"Next","tagName":"h2"},{"title":"Data analysis projects","type":0,"sectionRef":"#","url":"/docs/data-analysis/","content":"Data analysis projects last updated on 01.08.2025 by Clara Ruthardt To help you get started and inspired with social media analyses, we’ve collected a first set of practical projects that provide a starting point. There are a myrdiad of projects and methods discussion around social media data. Therefore, we welcome additional content and have added a selection of chapters below that would make for great contributions. Analysis methods and tools This chapter introduces you to rang of different data analysis methods and tools, the perfect starting point if you're new to this kind of research. Learn more TikTok hashtag analysis with JavaScript and Phyton In this chapter you will learn about the different steps necessary to conduct a hashtag analysis on TikTok: From collecting the data to analysing it with Python and visualising it. Learn more Social network analysis in R and Gephi This chapter is an introductory tutorial to the broad field of social network analysis. The tutorial includes an example of how to use social network analysis to detect communities in online debates on social platforms like X (Twitter). Learn more Data analysis using the DISARM Framework This chapter provides a hands-on guide for using the DISARM Framework effectively in disinformation research, particularly in data preprocessing and categorization. Learn more Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package The metatargetr R package offers an alternative, unofficial route to this information, by retrieving and archiving ad data directly from Meta’s public-facing Ad Library. This tutorial focuses on how to use metatargetr to retrieve and, most importantly, analyze this targeting data. Learn more Open for contributions We welcome contributions on a rolling basis. Have you done an analysis or a research project you want to share? Do you want to contribute a primer for a specific method? Get in touch with us at upgrade.democracy@bertelsmann-stiftung.de","keywords":"","version":"Next"},{"title":"Latin America: Researcher access to platform data, challenges to academic freedom and transparency","type":0,"sectionRef":"#","url":"/docs/data-access/02_03_data-access-global-south","content":"","keywords":"","version":"Next"},{"title":"Global disparities in data access and academic collaboration​","type":1,"pageTitle":"Latin America: Researcher access to platform data, challenges to academic freedom and transparency","url":"/docs/data-access/02_03_data-access-global-south#global-disparities-in-data-access-and-academic-collaboration","content":" The case of MonitorA is not unique. Other research projects have been discontinued or compromised due to data access policies for researchers. Although some of these policies affect researchers worldwide, the impacts are different and more significant in countries in the Global South due to the absence of alternatives beyond the APIs provided by the platforms. In some cases, researchers from the Global North gain access to data through alternative means, such as qualitative methods via interviews or informal conversations with big tech employees, or through partnerships with platforms, not relying exclusively on access to public APIs.  There are numerous records of partnerships between US and European universities and technology companies, specifically aimed at conducting academic research, in which social media platforms offer benefits, such as data sharing and promoting dialogue between the company and researchers. Such partnerships were not found in peer universities in Latin America, Africa or Asia. For example, Meta used to support universities in the United States, such as New York University, Arizona State University, and Ryerson University, through research groups and laboratories under the Academic Partnerships and Data for Good at Meta project. In the same Meta project, universities in Europe including the London School of Economics, University of Catalunya, Mercator Research Center, and Max Planck Institute, were also involved.  This type of partnership between platforms and universities—or even direct contact with platforms—is much more restricted for researchers who are not affiliated with research centres in the Global North. Considering the context of a data blackout and the specific challenges faced by researchers from the Global South, during April, May, and June 2023, InternetLab conducted 3 in-depth interviews and 2 focus groups with 14 Latin American researchers to understand the specific challenges encountered in the region regarding academic freedom and transparency on platforms.  ","version":"Next","tagName":"h2"},{"title":"Breaking down the issue: 7 main obstacles for researchers​","type":1,"pageTitle":"Latin America: Researcher access to platform data, challenges to academic freedom and transparency","url":"/docs/data-access/02_03_data-access-global-south#breaking-down-the-issue-7-main-obstacles-for-researchers","content":" From these conversations, the main obstacles and risks faced by Latin American researchers conducting research on platforms were:  Infrastructure and funding: structural difficulties in Latin America for the collection and processing of research data. Despite relatively high economic classifications, research investments are low compared to the Global North, leading to fewer publications and citations. Latin American researchers often rely on platform APIs and tools like CrowdTangle because they lack the resources for advanced data scraping and storage, making them vulnerable to policy changes by the platforms. This dependence not only hampers independent research but also fosters a competitive environment where data sharing is limited by contractual and security concerns. API policy changes and database erasure. Constant modifications in API policies (such as X policy changes or the closing of CrowndTangle) force researchers to adapt quickly, often outpacing academic timelines and hindering the collection of retroactive data. For instance, one researcher noted unpredictable shifts in Facebook’s data-sharing protocols, which compounded difficulties in storing and retrieving essential information. These issues, combined with limited funding and infrastructure, place Latin American scholars at a considerable disadvantage compared to their Global North counterparts. Filters and quantity of databases made available. According to our interviews, platforms filter and limit the data available through their APIs, leaving researchers without complete datasets or the ability to request modifications to the filtering process. Furthermore, API access keys restrict the amount of data that can be retrieved, deepening dependency on the platforms’ willingness to provide data. Once again, although these limitations are not limited to Latin American researchers, they have a greater impact on the region when we consider the other obstacles. The quality of the data made available and the possibility of cross-referencing data from different platforms. Platforms often filter their data for APIs based on commercial interests instead of research purposes. This results in incomplete or biased datasets that limit cross-platform research. Moreover, the diverse formats and aggregation methods employed by different platforms hinder cross-platform analysis, while language barriers further complicate access to key documents like terms of use. Legal liability: treatment of personal data and strategies employed by researchers to ensure compliance with legal requirements regarding data processing. The legal requirements regarding data processing vary from country to country. In countries that do have a set of data protection rules, the interviews showed that there is concern among researchers about how to comply with such laws, specifically regarding ensuring proper anonymization of data and treating data in a way that individuals being researched are not identified. One point of concern among our participants is that these risks are not limited to researchers who may violate data protection laws. Rather, it primarily poses a risk to the users of the platforms involved in the research, who may have their privacy violated. Additionally, there are countries that do not have a data protection framework, leaving both researchers and platform users in a limbo of uncertainty. Personal risk: political violence and psychological damages. Interviews pointed out that researchers face significant risks related to physical and psychological well-being, political pressure, and exposure to violent content. For instance, those studying messaging groups or extremist communities encounter threats, retaliation, and potential doxxing. Exposure to discriminatory and violent material also impacts researchers' mental health, particularly those from marginalized groups. Additionally, scholars report concerns over government or law enforcement attempts to access research data, raising ethical and security challenges. These factors highlight the precarious conditions under which research on sensitive topics is conducted. Lack of ethical standards for research on social media. Researchers mentioned facing ethical dilemmas about collecting and handling certain types of data, such as personal data and private data. For instance, they are affected by the lack of protocols for collecting information, as well as the shortage of methods for researching in messaging groups, as these are not considered public data and obtaining free and informed consent from users can be challenging.  The perception that the limitations of accessing data in Latin America are more extensive when compared to the United States and Europe is a consensual point among researchers from the region. As shown, the reasons for these contrasts between Global North and South are multiple and interconnected, involving economic inequalities across regions, structural aspects of universities in different areas, as well as limitations and variations in the ways of engaging with platforms.  ","version":"Next","tagName":"h2"},{"title":"Where do we go from here?​","type":1,"pageTitle":"Latin America: Researcher access to platform data, challenges to academic freedom and transparency","url":"/docs/data-access/02_03_data-access-global-south#where-do-we-go-from-here","content":" Two conclusions are taken from this scenario. First, the development and expansion of evidence-based academic research in Latin America on social media platforms require coordination among all the involved actors: universities, researchers themselves, and the platforms. Universities, research centers, and ethics committees have to update themselves on the challenges and nuances of conducting research on platforms, as the methods and necessary support differ from those used in research involving individuals or in the natural and exact sciences. Researchers also recognize the need to consolidate protocols and establish best practices for data protection and privacy safeguards to better request and address the datasets they have access to.  Finally, regarding platforms, beyond the necessity to formulate and implement new transparency practices specifically designed for the Global South, there is a demand to improve existing ones. Strengthening the transparency framework for researchers involves identifying barriers and strategies to mitigate them, developing training and capacity-building, standardizing the availability of data across platforms, and increasing engagement and presence in the region. ","version":"Next","tagName":"h2"},{"title":"Overview: How to analyse social media data","type":0,"sectionRef":"#","url":"/docs/data-analysis/04_01_data-analysis-methods","content":"","keywords":"","version":"Next"},{"title":"Introduction to text-as-data methods​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#introduction-to-text-as-data-methods","content":" Social media data offer the possibility to study, for example, citizens’ behaviour online or the content they are exposed to on digital platforms. The large quantity of such data, however, can necessitate the use of automated methods. In addition to existing overviews on data collection methods and analysis methods like network analysis, this chapter introduces text-as-data methods.  Text-as-data methods are approaches where text analysis is, to some extent, automatised through programming scripts (Hase, 2023) – though this still requires human input, for example, to inspect and validate results. The method is called “text-as-data” or “automated content analysis”, as text is broken down to and analysed in a numeric format (e.g., word counts as “features”). Typically, text-as-data methods include three key steps: (1) preprocessing text (i.e., preparing text for analysis), (2) text analysis (conducting the analysis), and (3) test against quality criteria (i.e., checking the quality of results).    Key steps of text-as-data methods  ","version":"Next","tagName":"h2"},{"title":"Preprocessing​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#preprocessing","content":" Preprocessing describes the cleaning and normalisation of text for analysis. On the one hand, social media posts may include irrelevant context (e.g., boilerplate content such as URLs if retrieved via scraping), which researchers need to remove when cleaning the data in preparation for their analysis. On the other hand, social media content often contains “unstructured” text, where users employ different spelling or punctuation. That means, that researchers have to normalise data, for example by harmonising different spelling of similar features. While relevant preprocessing steps vary across methods and data sets, these often include:  Removal of boilerplate content (e.g., excluding formatting) Tokenisation (breaking texts down into its individual features, like words) Lower casing (removing capitalisation) Removing symbols, punctuation, or numbers Reducing features to word stems/base forms (e.g., via “stemming” or “lemmatisation”) Relative pruning (removing frequent/rare words, such as “stop words”)  Take, for example, the sentence “This is an introduction to automated content analysis!”. If we run a selection of preprocessing steps on this sentence (e.g., tokenisation, stemming, relative pruning), it is reduced to the count of what we assume to be its most relevant features: “introduct”, “autom”, “content”, “analysi” (see related preprocessing tutorials in R and Python).    Example of preprocessing pipeline  To turn this into a numeric representation, researchers transform texts into a document-feature-matrix (DFM). Here, rows identify the unit of analysis (e.g., sentences), columns identify features (e.g., “introduct”, “autom”), and cell values identify the frequency of features in each sentence. To illustrate, let’s compare the numeric representation of “This is an introduction to automated content analysis!” (Sentence 1) and “This is an introduction to automated visual analysis” (Sentence 2) in a DFM: We can see that these share features, such as “introduct”, “autom”, but also differ concerning others, namely “content” vs. “visual”. DFM-representations rely on the “bag-of-words” assumption, meaning that features are interpreted independently of their order in texts or their context.    Numeric representation: Document-feature-matrix  ","version":"Next","tagName":"h2"},{"title":"Analysis​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#analysis","content":" Next, researchers must choose a text-as-data approach to analyse preprocessed texts. Here, I will discuss five popular methods: (1) dictionaries, (2) semantic network analysis, (3) supervised machine learning, (4) topic modelling, and (5) large language models.  Method\tDescription\tTutorialsDictionary\tLists of relevant features to identify things like sentiments Tutorial R Tutorial Python Semantic Network Analysis\tNetwork analysis to analyse and visualise the co-occurrence of features Tutorial R Tutorial Python Supervised Machine Learning\tTraining of algorithms on manually annotated texts to classify known variables in new texts Tutorial R Tutorial Python Topic Modelling\tUsing feature co-occurrences to identify previously unknown topics in texts Tutorial R Tutorial Python Large Language Models (LLMs)\tUsing prompts to identify known variables in new texts using little or no training data Tutorial R Tutorial Python  ","version":"Next","tagName":"h2"},{"title":"Dictionaries​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#dictionaries","content":" Dictionaries describe lists of features that researchers presume describe latent concepts. Using dictionaries, researchers count the occurrence of these features to identify more abstract concepts. A widely known example is sentiment analysis, where negative or positive features – in their simplest form: words like “bad” or “good” – are used to identify positive or negative sentiment. Researchers can rely on off-the-shelf dictionaries (i.e., existing lists of features often developed in other contexts) or organic dictionaries (i.e., news lists of features they develop for their specific context and data). However, especially off-the-shelf dictionaries often fail to capture more complex aspects of language, such as irony or negation (Boukes et al., 2020), indicating that organic dictionaries or other text-as-data approaches may often be the more suitable choice.  ","version":"Next","tagName":"h3"},{"title":"Semantic network analysis​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#semantic-network-analysis","content":" Semantic network analysis, a particular case of network analysis, describes and visualises the co-occurrence of features, meaning how often different words occur in the same text. In such networks, nodes (data points) describe features, while edges (connections between them) describe the frequency of their co-occurrence. Take the example below, which illustrates how often men vs. women are described with specific features in an example corpus: Both are similarly often described as “young”. However, women are more often characterised with words like “love” and men with words like “fight,” which may indicate gender stereotypes.    Understanding gender stereotypes via semantic network analysis  ","version":"Next","tagName":"h3"},{"title":"Supervised Machine Learning​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#supervised-machine-learning","content":" Supervised machine learning describes an approach where algorithms learn how to classify texts based on their training of manually annotated training data. Usually, human coders annotate previously defined variables, such as, whether a text is positive or negative. Through training on this data, algorithms learn which features predict positive vs. negative sentiment. The same classifier is then used to apply this knowledge and classify new corpora. Here, researchers can choose from various algorithms (e.g., Naïve Bayes, Random Forest) or combine these as ensembles. As classifiers can take into account many different features in training data, they often perform better than dictionaries, which are more restricted in the number of features they consider (Barberá et al., 2021).  ","version":"Next","tagName":"h3"},{"title":"Topic modelling​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#topic-modelling","content":" Topic modelling describes an exploratory approach where algorithms use the co-occurrences of features in texts to identify previously unknown topics. In short, the approach understands texts as a mixture of topics (e.g., “politics” vs. “economy” vs. “sports”) characterised by distributions over words (e.g., “politics” as being described by features like “politician”, “election”, and “government”). Distributions of topics in texts and distributions of features in topics are learned exploratively from the specific corpus and not defined beforehand. To run a topic model, researchers have to define how many topics they want to identify and interpret what these mean – indicating large degrees of freedom in setting up such analyses (Maier et al., 2018).  ","version":"Next","tagName":"h3"},{"title":"Large Language Models​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#large-language-models","content":" Large Language Models (LLMs) have become increasingly important in recent years. These approaches go beyond the “bag-of-words” approach in that they can assign features context-dependent meanings. For example, the word “bank” may take on a different meaning in the sentence “He went to the outer bank of the river” compared to “He got a loan from the bank”, something LLMs can account for. LLMs are trained on vast amounts of text using the transformer architecture, where text is used as training data as a form of self-supervised learning to understand which features likely predict the next (Wankmüller, 2024). This pre-trained knowledge has enabled the rise of LLMs, which rely on few- or zero-shot learning: Researchers can use little or no training data to annotate text based on short instructions, so-called prompts (Törnberg, 2024). In turn, researchers have to write effective prompts to train LLMs, known as prompt-tuning, and can adapt LLM parameters to specific tasks or data, referred to asfine-tuning (for an overview of R and Python packages, see Demszky et al., 2023).  ","version":"Next","tagName":"h3"},{"title":"Test against quality criteria​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#test-against-quality-criteria","content":" Finally, researchers need to test the quality of their results. This includes the reproducibility of results (i.e., whether re-running code leads to the same results) or the reliability of methods (i.e., whether different methodological parameters lead to comparable results). Most importantly, researchers need to guarantee the validity of their results. Here, they often compare the result of their automated analysis to a manual annotation of the same text. The evaluation of validation is done based on metrics such as precision (i.e., how good is the text-as-data method at not identifying too many irrelevant texts) and recall (i.e., how good is the text-as-data method at identifying all relevant texts?; for a tutorial, see here).  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Overview: How to analyse social media data","url":"/docs/data-analysis/04_01_data-analysis-methods#references","content":" Barberá, P., Boydstun, A. E., Linn, S., McMahon, R., &amp; Nagler, J. (2021). Automated Text Classification of News Articles: A Practical Guide. Political Analysis, 29(1), 19–42. https://doi.org/10.1017/pan.2020.8 Boukes, M., van de Velde, B., Araujo, T., &amp; Vliegenthart, R. (2020). What’s the Tone? Easy Doesn’t Do It: Analyzing Performance and Agreement Between Off-the-Shelf Sentiment Analysis Tools. Communication Methods and Measures, 14(2), 83–104. https://doi.org/10.1080/19312458.2019.1671966 Demszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., Chandhok, S., Eichstaedt, J. C., Hecht, C., Jamieson, J., Johnson, M., Jones, M., Krettek-Cobb, D., Lai, L., JonesMitchell, N., Ong, D. C., Dweck, C. S., Gross, J. J., &amp; Pennebaker, J. W. (2023). Using large language models in psychology. Nature Reviews Psychology. https://doi.org/10.1038/s44159-023-00241-5 Hase, V. (2023). Automated Content Analysis. In F. Oehmer, S. H. Kessler, E. Humprecht, K. Sommer, &amp; L. Castro Herrero (eds.), Handbook of Standardized Content Analysis: Applied Designs to Research Fields of Communication Science. VS Springer (pp. 23–36). https://doi.org/10.1007/978-3-658-36179-2_3 Maier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., &amp; Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2–3), 93–118. https://doi.org/10.1080/19312458.2018.1430754 Törnberg, P. (2024). Large Language Models Outperform Expert Coders and Supervised Classifiers at Annotating Political Social Media Messages. Social Science Computer Review, 08944393241286471. https://doi.org/10.1177/08944393241286471 Wankmüller, S. (2024). Introduction to Neural Transfer Learning With Transformers for Social Science Text Analysis. Sociological Methods &amp; Research, 53(4), 1676–1752. https://doi.org/10.1177/00491241221134527 ","version":"Next","tagName":"h2"},{"title":"Guide: How to format content using Markdown","type":0,"sectionRef":"#","url":"/docs/contribute/06_03_markdown-features","content":"","keywords":"","version":"Next"},{"title":"Front Matter​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#front-matter","content":" Markdown documents have metadata at the top called Front Matter:  my-doc.md --- id: my-doc-id title: My document title description: My document description slug: /my-custom-url --- ## Markdown heading Markdown text with [links](./hello.md)   ","version":"Next","tagName":"h2"},{"title":"Links​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#links","content":" Regular Markdown links are supported, using url paths or relative file paths.  Let's see how to [see &quot;Contribute&quot; section](06_01_how-to-contribute).   Let's see how to [see &quot;Contribute&quot; section](../06_01_how-to-contribute).   Result: Let's see how to see &quot;Contribute&quot; section.  ","version":"Next","tagName":"h2"},{"title":"Images​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#images","content":" Regular Markdown images are supported. You have to use the relative path to the image.  ![](/../static/img/get-started/get-started-banner-2.png)   Image:  ","version":"Next","tagName":"h2"},{"title":"Code Blocks​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#code-blocks","content":" Markdown code blocks are supported with Syntax highlighting.  src/components/HelloDocusaurus.js function HelloDocusaurus() { return ( &lt;h1&gt;Hello, Docusaurus!&lt;/h1&gt; ) }   src/components/HelloDocusaurus.js function HelloDocusaurus() { return &lt;h1&gt;Hello, Docusaurus!&lt;/h1&gt;; }   ","version":"Next","tagName":"h2"},{"title":"Admonitions (NEW)​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#admonitions-new","content":" For this project we created some custom admonitions. You can use them like this:  Community note example  :::community[ My community note] This is a community note. It should be used to highlight important information in contributions and articles on the hub. :::   Community note This is a community note. It should be used to highlight important information in contributions and articles on the hub.  Contact note example  :::contact[ Contact note ] This is a contact note. It should be used to communicate contact information either on a Hub contribution or on a general overview page. :::   Contact note This is a contact note. It should be used to communicate contact information either on a Hub contribution or on a general overview page.  Hub Note example  :::hub-note[ My hub note] lorem ipsum dolor sit amet :::   My hub note lorem ipsum dolor sit amet  About Note example  :::about[ My About note] lorem ipsum dolor sit amet :::   My About note lorem ipsum dolor sit amet  Contribute Note example  :::contribute[ Contribute Note ] lorem ipsum dolor sit amet :::   Contribute Note lorem ipsum dolor sit amet  ","version":"Next","tagName":"h2"},{"title":"Expandable Sections (NEW)​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#expandable-sections-new","content":" If you have a lot of content, you can use expandable sections to make it more readable. For this we repurposed the HTML &lt;details&gt; and &lt;summary&gt; elements. If you want to create an expandable section you can just use multiple &lt;details&gt; elements with a &lt;summary&gt; element inside - the borders will be adjusted automatically.  &lt;details&gt; &lt;summary&gt;Toggle me!&lt;/summary&gt; This is the detailed content You can use Markdown here including **bold** and _italic_ text, and [inline link](https://docusaurus.io) &lt;/details&gt; &lt;details&gt; &lt;summary&gt;Toggle me!&lt;/summary&gt; This is the detailed content You can use Markdown here including **bold** and _italic_ text, and [inline link](https://docusaurus.io) &lt;/details&gt;   Toggle me! This is the detailed content You can use Markdown here including bold and italic text, and inline link  Toggle me! This is the detailed content You can use Markdown here including bold and italic text, and inline link  If you want the borders to have a different color you can include the following style section in the .mdx file.  &lt;style&gt; {` details { border-top-color: black !important; border-bottom-color: black !important; `} &lt;/style&gt;   ","version":"Next","tagName":"h2"},{"title":"Icon Lists (NEW)​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#icon-lists-new","content":" You can use icon lists to make your content more visually appealing. The individual items can be created using the IconListItem component. The component has the following properties:  text: The text that is displayed below the titletitle: The title of the itemicon: The path to the icon that is displayed. The icons are stored in the static/img/icons folder.blob: The path to the blob that is displayed. There are 7 different blue blobs and 4 green blobs available in the static/img/icons folder.  The Icons need to be wrapped in an IconList component.  lorem ipsum dolor sit amet  &lt;IconList&gt; &lt;IconListItem text=&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt &quot; title=&quot;Anonymisation&quot; icon=&quot;/img/icons/Icon_Anonymisation.png&quot; blob=&quot;/img/icons/blue-blob-1.svg&quot; /&gt; &lt;IconListItem text=&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt &quot; title=&quot;Data-Minimisation&quot; icon=&quot;/img/icons/Icon_Data-Minimisation_.png&quot; blob=&quot;/img/icons/blue-blob-2.svg&quot; /&gt; &lt;IconListItem text=&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt &quot; title=&quot;Data-Retention&quot; icon=&quot;/img/icons/Icon_Data-Retention.png&quot; blob=&quot;/img/icons/blue-blob-3.svg&quot; /&gt; &lt;IconListItem text=&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt &quot; title=&quot;Safe-Data-Storage&quot; icon=&quot;/img/icons/Icon_Safe-Data-Storage.png&quot; blob=&quot;/img/icons/blue-blob-4.svg&quot; /&gt; &lt;IconListItem text=&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt &quot; title=&quot;Harm-Mitigation&quot; icon=&quot;/img/icons/Icon_Harm-Mitigation.png&quot; blob=&quot;/img/icons/blue-blob-5.svg&quot; /&gt; &lt;IconListItem text=&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt &quot; title=&quot;Icon_Advocacy&quot; icon=&quot;/img/icons/Icon_Advocacy.png&quot; blob=&quot;/img/icons/blue-blob-6.svg&quot; /&gt; &lt;IconListItem text=&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt &quot; title=&quot;Funding-Transparency&quot; icon=&quot;/img/icons/Icon_Research-Funding-Transparency.png&quot; blob=&quot;/img/icons/blue-blob-7.svg&quot; /&gt; &lt;/IconList&gt;    Anonymisation Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt Data-Minimisation Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt Data-Retention Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt Safe-Data-Storage Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt Harm-Mitigation Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt Icon_Advocacy Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt Funding-Transparency Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt   You can also create icon items without an icon or a title.  &lt;IconList&gt; &lt;IconListItem text=&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt &quot; blob=&quot;/img/icons/blue-blob-1.svg&quot; /&gt; &lt;IconListItem text=&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt &quot; blob=&quot;/img/icons/green-blob-2.svg&quot; /&gt; &lt;/IconList&gt;    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt   ","version":"Next","tagName":"h2"},{"title":"Custom components (NEW)​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#custom-components-new","content":" For this project we created some custom components that can be used in the markdown files. Make sure to use .mdx files for this. You can rename your .md files to .mdx files to use the custom components.  ","version":"Next","tagName":"h2"},{"title":"Author Card​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#author-card","content":" You can use the AuthorCard component to display information about the author of a contribution. The component has the following properties:  name: The name of the authoravatarSrc: The path to the avatar of the authorposition: The position of the authorwebsite: The website of the authorlinkedin: The LinkedIn profile of the authortwitter: The Twitter / X profile of the authormastodon: The GitHub profile of the authorscholar: The Google Scholar profile of the author  &lt;AuthorCard name=&quot;Open Knowledge Foundation&quot; avatarSrc={require(&quot;@site/static/img/contributors/okfn.png&quot;).default} position=&quot;OKFN&quot; website=&quot;https://okfn.org/en/&quot; linkedin=&quot;https://www.linkedin.com/company/okfn/&quot; /&gt;   Open Knowledge Foundation OKFN Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky   ","version":"Next","tagName":"h3"},{"title":"Article Card​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#article-card","content":" You can use the ArticleCard component to display information about an article or research. The component has the following properties:  heading: The heading of the articleauthor: The author of the articleteaser: A short teaser text about the articlelink: The link to the articleorganization: The organization of the author  &lt;ArticleCard heading=&quot;OSoMe Toolkit for Social Media Research&quot; author=&quot;Indiana University Observatory on Social Media&quot; organization=&quot;Indiana University&quot; teaser=&quot;Exciting tools created by the OSoMe team made available for research, such as the OSoMe Mastodon Search or tools for network visualization and annotation.Take a look!&quot; link=&quot;https://osome.iu.edu/resources/tools&quot; /&gt;   OSoMe Toolkit for Social Media Research by Indiana University Observatory on Social Media | Indiana University Exciting tools created by the OSoMe team made available for research, such as the OSoMe Mastodon Search or tools for network visualization and annotation.Take a look! Read more   ","version":"Next","tagName":"h3"},{"title":"Chips​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#chips","content":" There are three kind of chips available: LevelChip to indicate which level of programming experience this article is suitable for, LanguageChip which indicates the programming language and PlatformChip which indicates which social media platform the article relates to. You can use them like this:  &lt;LevelChip level=&quot;Beginner&quot; /&gt; &lt;LanguageChip lang=&quot;Python&quot; /&gt; &lt;PlatformChip platform=&quot;TikTok&quot; /&gt;   Level: Beginner  Language: Python  Platform: TikTok  ","version":"Next","tagName":"h3"},{"title":"Last Updated Chip​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#last-updated-chip","content":" You can use the LastUpdatedByChip component to display information about the last update of the article. The component has the following properties:  authorOriginal: The original author of the articlecreatedOn: The date the article was createdauthorLastUpdate: The author of the last updateupdatedOn: The date of the last update  If you leave out the authorLastUpdate and updatedOn properties, the component will only display the original author and creation date.  &lt;LastUpdatedByChip authorOriginal=&quot;Clara Ruthardt&quot; createdOn=&quot;30.09.2024&quot; authorLastUpdate=&quot;Cathleen Berger&quot; updatedOn=&quot;06.12.2024&quot; /&gt;   Result:   Original post on 30.09.2024 by Clara Ruthardt last updated on 06.12.2024 by Cathleen Berger  ","version":"Next","tagName":"h3"},{"title":"Create new React components​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/docs/contribute/06_03_markdown-features#create-new-react-components","content":" MDX can make your documentation more interactive and allows using any React components inside Markdown:  export const Highlight = ({children, color}) =&gt; ( &lt;span style={{ backgroundColor: color, borderRadius: '20px', color: '#fff', padding: '10px', cursor: 'pointer', }} onClick={() =&gt; { alert(`You clicked the color ${color} with label ${children}`) }}&gt; {children} &lt;/span&gt; ); This is &lt;Highlight color=&quot;#25c2a0&quot;&gt;Docusaurus green&lt;/Highlight&gt; ! This is &lt;Highlight color=&quot;#1877F2&quot;&gt;Facebook blue&lt;/Highlight&gt; !     This is Docusaurus green !  This is Facebook blue ! ","version":"Next","tagName":"h3"},{"title":"Essentials of DISARM: A Practical Approach to Researching Information Disorder","type":0,"sectionRef":"#","url":"/docs/data-analysis/04_04_disarm-framework","content":"","keywords":"","version":"Next"},{"title":"Overview of the DISARM Framework in research​","type":1,"pageTitle":"Essentials of DISARM: A Practical Approach to Researching Information Disorder","url":"/docs/data-analysis/04_04_disarm-framework#overview-of-the-disarm-framework-in-research","content":" While the DISARM Framework’s core principles remain focused on understanding and countering disinformation tactics, this guide is specifically tailored to assist in applying the framework to research environments. For disinformation researchers, using DISARM goes beyond theory—it becomes an essential tool for organizing, categorizing, and cleaning data to extract meaningful insights.  Key benefits of using the DISARM Framework in research:  Codifying disinformation techniques and countermeasures for research purposes: Data cleaning is the first crucial step in the research process using the DISARM framework. Researchers prepare datasets by identifying and removing irrelevant data such as spam, duplicates, and misleading sources. Following this, data categorisation allows for organising cleaned data into specific tactics like misinformation or coordinated propaganda. This categorisation enables effective analysis of patterns, sources, and targets, proving invaluable for identifying key narratives and the types of influence operations at play.Applying standardised definitions for research consistency: The framework helps ensure consistent interpretation of terms such as “misinformation,” “coordinated campaigns,” and “narratives.” Standardised definitions reduce ambiguity in data labelling, providing more accurate and reliable results. This consistency is crucial for collaborative efforts and long-term data analysis projects.Empowering collaborative research with a unified methodology: DISARM fosters a common understanding across different (international) research teams, regardless of whether they focus on sentiment analysis, topic modelling, or other types of content analysis. By utilising the same framework, teams can more easily share and compare their findings, leading to better-informed conclusions about the nature and impact of disinformation campaigns.  ","version":"Next","tagName":"h2"},{"title":"Integrating different object types for comprehensive analysis​","type":1,"pageTitle":"Essentials of DISARM: A Practical Approach to Researching Information Disorder","url":"/docs/data-analysis/04_04_disarm-framework#integrating-different-object-types-for-comprehensive-analysis","content":" The DISARM framework contains many object types, including tactic stages (steps in an incident), and techniques (activities at each tactic stage). To effectively use the DISARM framework, researchers can integrate the analysis of tactics (broad strategies used by disinformers) with the concrete actions within those strategies. This combination allows for a more nuanced understanding of disinformation campaigns and enhances the ability to develop tailored counterstrategies.  Example: When analysing a disinformation operation, researchers may identify the tactic of &quot;target audience manipulation&quot; and link it to techniques such as the &quot;use of emotional or polarising content.&quot; This specific terminology helps categorise data, facilitates a more structured analysis, and enables more precise communication among researchers and stakeholders about the nature and mechanics of the threat.  ","version":"Next","tagName":"h2"},{"title":"Phases of a disinformation campaign​","type":1,"pageTitle":"Essentials of DISARM: A Practical Approach to Researching Information Disorder","url":"/docs/data-analysis/04_04_disarm-framework#phases-of-a-disinformation-campaign","content":" The DISARM Framework helps to understand and analyse disinformation campaigns using four key phases: (1) Plan, (2) Prepare, (3) Execute, and (4) Assess.  1. Plan: Setting strategic objectives In the planning phase, strategic goals for the influence operation are established. Actors determine their key messages, targets, and desired outcomes, laying the groundwork for actions designed to manipulate public perception or achieve political gains.  2. Prepare: Establishing the operational framework Preparation involves building the necessary infrastructure for the disinformation campaign. This includes creating digital assets like fake social media profiles and recruiting operatives to manage these profiles and develop misleading content.  3. Execute: Implementing the disinformation campaign During the execution phase, disinformation is actively disseminated across selected channels to maximise exposure and impact, with strategies adapted in real time based on audience interaction.  4. Assess: Evaluating campaign effectiveness The assessment phase involves analysing the campaign's effectiveness and impact on the target audience. This helps determine whether the campaign’s goals were achieved and gathers insights to refine future operations.  ","version":"Next","tagName":"h2"},{"title":"How to use the Red and Blue Frameworks in disinformation research​","type":1,"pageTitle":"Essentials of DISARM: A Practical Approach to Researching Information Disorder","url":"/docs/data-analysis/04_04_disarm-framework#how-to-use-the-red-and-blue-frameworks-in-disinformation-research","content":" DISARM has two main frameworks: The DISARM Red Framework, for describing incident creator behaviours, and the DISARM Blue Framework, to describe potential response behaviours. Their distinction as well as their application in research are illustrated in the following:  ","version":"Next","tagName":"h2"},{"title":"The Red Framework: Analysing disinformation tactics​","type":1,"pageTitle":"Essentials of DISARM: A Practical Approach to Researching Information Disorder","url":"/docs/data-analysis/04_04_disarm-framework#the-red-framework-analysing-disinformation-tactics","content":" The Red Framework focuses on the tactics and techniques employed by disinformation actors. It helps researchers and analysts dissect and understand the offensive manoeuvers used in information-influenced operations. By categorising these tactics, the Red Framework aids researchers in mapping out the attack vectors and strategies disinformers use.  Example: In a disinformation campaign to influence an election, researchers can use the Red Framework to identify specific tactics, such as creating false news stories, using automated bots to amplify certain narratives, or strategically releasing hacked information. By applying standardised terminology from the DISARM framework, such as &quot;fabricated content&quot; or &quot;coordinated inauthentic behaviour,&quot; researchers can clearly define and categorise the disinformation techniques observed.  ","version":"Next","tagName":"h3"},{"title":"The Blue Framework: Crafting countermeasures​","type":1,"pageTitle":"Essentials of DISARM: A Practical Approach to Researching Information Disorder","url":"/docs/data-analysis/04_04_disarm-framework#the-blue-framework-crafting-countermeasures","content":" Conversely, the Blue Framework is designed to help practitioners develop and implement strategies to counter disinformation. This framework guides the creation of defensive measures that can effectively neutralise or mitigate the impact of disinformation campaigns. It encourages the adoption of a proactive stance, empowering organisations to not only respond to disinformation but to anticipate and prevent it.  Example: Using insights from the Red Framework analysis, a team can employ the Blue Framework to design educational campaigns informing the public about disinformation signs. They might also collaborate with social media platforms to adjust algorithms that detect and flag false content or work with policymakers to enforce stricter regulations on digital transparency.  ","version":"Next","tagName":"h3"},{"title":"Practical applications and limitations​","type":1,"pageTitle":"Essentials of DISARM: A Practical Approach to Researching Information Disorder","url":"/docs/data-analysis/04_04_disarm-framework#practical-applications-and-limitations","content":" There are several examples of how the DISARM framework has been successfully utilised, such as studies examining the spread of disinformation related to the COVID-19 pandemic and electoral processes in several countries. However, while the DISARM framework offers a valuable structure for analysing and countering disinformation, it is crucial to acknowledge its limitations. Researchers may encounter difficulties in definitively categorising and labelling disinformation tactics due to their constantly evolving nature and limitations in accessing data from closed online communities or encrypted platforms. In addition, it is essential to recognise that the framework primarily focuses on online disinformation. It may not fully capture the complexities of disinformation spread through offline channels or in contexts with restricted internet access.  ","version":"Next","tagName":"h2"},{"title":"Conclusion: A collaborative and practical approach to disinformation research​","type":1,"pageTitle":"Essentials of DISARM: A Practical Approach to Researching Information Disorder","url":"/docs/data-analysis/04_04_disarm-framework#conclusion-a-collaborative-and-practical-approach-to-disinformation-research","content":" The DISARM Framework is more than a set of theoretical concepts—it is a practical tool for researchers working to combat disinformation. By adapting the framework to tasks like data cleaning, categorixation, and analysis, researchers can better understand the landscape of information manipulation. The key takeaway is that collaboration, standardised terminology, and structured methodologies are the bedrock of effective disinformation research.  For more detailed methodologies, case studies, and research tools, visit the DISARM Framework Resource. ","version":"Next","tagName":"h2"},{"title":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","type":0,"sectionRef":"#","url":"/docs/data-analysis/04_02_hashtag-analysis","content":"","keywords":"","version":"Next"},{"title":"Scraping Data on TikTok with puppeteer​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/docs/data-analysis/04_02_hashtag-analysis#scraping-data-on-tiktok-with-puppeteer","content":" For this analysis we will leverage the web version of TikTok accessible with any browser on TikTok.com.  While there are various “unofficial APIs” or libraries available that allow you to scrape content from TikTok, the purpose of this chapter is to build a scraper from scratch.  Below, you will find a basic scraping built on pupeteer to open the web version of the FYP.  ","version":"Next","tagName":"h2"},{"title":"Tech Setup​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/docs/data-analysis/04_02_hashtag-analysis#tech-setup","content":" To run this project, you should be able to open a command line on your computer and install software. For the first part you need to have nodejs installed. After you have created a folder and navigated to it with the console run npm install pupeteer in it.  A basic scraper that opens the TikTok website should look like this:  const puppeteer = require(&quot;puppeteer&quot;); async function run() { // Setting up the browser const browser = await puppeteer.launch({ executablePath: &quot;/usr/bin/google-chrome&quot;, //we want to run chrome. You need to adjust this depening on your operating system headless: false, // we want to see what pupeteer is doing, headless:true would mean the browser is not shown defaultViewport: null, args: [&quot;--window-size=1920,1080&quot;], }); const page = await browser.newPage(); // opening a new tab await page.goto(&quot;https://tiktok.com/foryou&quot;); // open TikTok } run();   This should bring up a browser windows like this:    This basic process, already allows you to garner information about the videos TikTok would like to show you- even though you can’t see them. You can right-click on the page and select “view source” (or press ctrl+u) to see the HTML source code of the site. Search for “ItemModule” and you will be able to access information about the videos in a structured JSON-format.  The JSON object contains information about the first eight videos that are available or displayed on the FYP. The structure of each JSON object is shown here. You can extract that data automatically and put it on an internal list like this:  // once the page is loaded we will search for the javascript block with the ID #SIGI_STATE and parse it's content as JSON const sigistate = await page.$eval(&quot;#SIGI_STATE&quot;, (el) =&gt; el.innerHTML); initinfo = JSON.parse(sigistate); videos = []; // Iterate through the list of videos and add it to our own data structure Object.keys(initinfo.ItemModule).forEach(async (el, c) =&gt; { videos.push(initinfo.ItemModule[el]); });   If you research requires you to get data on more videos, you have to interact with the website. When accessing the site, you will be initially prompted by a login popup, which you can close automatically.    If you are working with Chrome, you can use the developer settings to find out how you can manipulate the button with puppeteer. First open the developer console with F12, then right click the close icon, select “Inspect” and find the parent element that seems unique. In this case there is a div element with the attribute data-e2e=&quot;modal-close-inner-button&quot;. In puppeteer, you can use this attribute to identify the “X” and click it like this:  let logindialog = await page.$('div[data-e2e=&quot;modal-close-inner-button&quot;]'); // waiting until the page is loaded to the point where the dialog shows up await logindialog.click(page, 'div[data-e2e=&quot;modal-close-inner-button&quot;]'); // click on the X   After that we can scroll the page forever, tapping the key-down every two seconds.  while (true) { // this loops runs indefinitivly, if we don't close it await page.waitForTimeout(2000); // wait 2000 milliseconds await page.keyboard.press(&quot;ArrowDown&quot;); // pree the ArrowDown Key }   When watching the bot scroll through TikTok and looking at the network traffic in the developer console you’ll notice that there is a infrequent call to a TikTok API https://www.tiktok.com/api/recommend/item_list/ that returns a list of the next 30 videos. Finding this request and understanding what it is might be more complicated on other platforms or services. In some cases, undocumented APIs can be a key element of data-driven investigations to help make sense of what you are collecting.  With puppeteer, you can intercept network traffic and grab the list of recommended videos for your analysis - since it is running from within the browser you do not have to worry about any encryption. You can intercept the responses to this specific URL and process the video metadata as you like:  await page.setRequestInterception(true); // setting up pupeteer to monitor requests and responses page.on(&quot;request&quot;, (request) =&gt; request.continue()); // we do not care about requests send to servers, so we just continue on those page.on(&quot;response&quot;, (response) =&gt; { // responses are parsed by the following script try { if (response.url().indexOf(&quot;api/recommend/item_list/item_list&quot;) &gt; 0) { //if data comes from the API we have identified we process it response.json().then((data) =&gt; { // parse it as json data.itemList.forEach(async (el, c) =&gt; { // and iterate over the videos in the list to store. videos.push({ id: el.id, textExtra: el.textExtra }); }); }); console.log(&quot;Received Video Data. Seen &quot; + videos.length + &quot; so far&quot;); } } catch (e) { console.log(e); } });   You have now successfully created a scraper for the TikTok public ForYouPage with less than 50 lines of code. Review the full here  ","version":"Next","tagName":"h3"},{"title":"Storing​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/docs/data-analysis/04_02_hashtag-analysis#storing","content":" To analyze a specific data set over a longer period, you need to store and process it. For this example project, you can use a file-based database like node-json-db that stores everything in a single JSON file. If you plan to run a project like this on a larger scale, you should consider regular database servers like Redis or MongoDB. You can install it with npm install node-json-db similar to the installation of puppeteer above. You can than run the full code from here  Note that in this example you only store the video ID, hashtags and statistics as you do not need all the additional (personal) information and should ensure cleaning your scraped data for legal and ethical reasons.  ","version":"Next","tagName":"h3"},{"title":"Analysis​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/docs/data-analysis/04_02_hashtag-analysis#analysis","content":" ","version":"Next","tagName":"h2"},{"title":"Tech Setup​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/docs/data-analysis/04_02_hashtag-analysis#tech-setup-1","content":" To help analyze your data, consider using jupyter notebooks with python. Experience shows that miniconda works well to run python and install dependencies. You can use the conda environment published with this tutorial.  ","version":"Next","tagName":"h3"},{"title":"Import Data​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/docs/data-analysis/04_02_hashtag-analysis#import-data","content":" First, you can import the JSON database into a python dictionary:  import json with open('../../../tiktok-fyp-videos.json') as file: database = json.load(file) print(len(database.keys()), 'videos in database')   This will result in something like: 474 videos in database (if you let the scraper run longer or multiple times, you will of course have more videos in your dataset).  At this point you can have a look at the data you have collected so far.  import pprint pprint.pprint(next(iter(database.values())))   This will print the first video in the database. You can find an example of the structure of the video object here. It contains a lot of technical information, e.g. about the video quality and URLs of cover pictures, avatars, music and subtitles. But also the name of the author in authorID some statics in stats and of course the hashtags in the textExtra field. Let's first look at the video statistics. We can iterate over all videos and collect the views in an array. And then let the numpy package do the math:  video_views = [] for video in database.values(): video_views.append(video[&quot;stats&quot;][&quot;playCount&quot;]) average = np.mean(video_views) print(&quot;Average views:&quot;, average)   This will result in Average views: 20519932.70042194, or in plainer terms: more than 20 million views per video on average. That seems like a lot. A histogram plot shows that the data is not evenly distributed. There are a few videos with a huge number of views. Outliers from the rest.    Because of this distribution the median will therefore give a better idea of the 'average'  median = np.median(video_views) print(&quot;Median views:&quot;, median)   You can also limit the histogram plot to up to 10 million views and see that the majority of videos shown on the FYP for non-logged in web users is focused on videos that already had a large audience.    ","version":"Next","tagName":"h3"},{"title":"Hashtag Analysis​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/docs/data-analysis/04_02_hashtag-analysis#hashtag-analysis","content":" You can now iterate over all videos in the database to count hashtag occurrences and videos without hashtags to better understand the quality of the data.  from itertools import combinations import numpy as np videos_wo_hashtags = 0 hashtag_occurences = {} for video in database.values(): video_views.append(video[&quot;stats&quot;][&quot;playCount&quot;]) if &quot;textExtra&quot; in video: for hashtag in video[&quot;textExtra&quot;]: if hashtag[&quot;hashtagName&quot;] not in hashtag_occurences: hashtag_occurences[hashtag[&quot;hashtagName&quot;]] = 1 else: hashtag_occurences[hashtag[&quot;hashtagName&quot;]] += 1 else: videos_wo_hashtags += 1 # Use of hashtags print(&quot;There are&quot;, videos_wo_hashtags, &quot;videos without hashtags (&quot;, videos_wo_hashtags*100/len(database.keys()), &quot;%)&quot;)   You will find that 17% of the videos do not use hashtags.  If you go ahead and plot the network of hashtag combinations, you will get a graph like this:    Similarly, you can also look at the distribution of hashtags by sorting the list and printing the 20 most common hashtags.  hashtags_sorted = dict(sorted(hashtag_occurences.items(), key=lambda item: item[1], reverse=True)) print(&quot;The ten most common hashtags:&quot;) print(list(hashtags_sorted)[:20])   You see that the most common hashtags are related to TikTok itself and do not describe its content. To learn more about what type of videos are shown on the FYP you should exclude these hashtags: ['fyp', '', 'viral', 'foryou', 'foryoupage', 'fy', 'fyp シ', 'funny', 'funnyvideos', 'tiktok', 'fürdich', 'trending', 'trend', 'viralvideo']  A network graph consists of nodes (the circles) and edges (connections). Again you iterate over all videos to get a list of hashtags (nodes) and store as an edge if they occurred together on a video.  exclude_hashtags = ['fyp', '', 'foryou', 'viral', 'foryoupage', 'fypシ', 'fy', 'fürdich', 'trending', 'foryoupage', 'tiktok', 'viralvideo', 'fürdichseiteシ', '4u'] hashtag_nodes = {} hashtag_edges = [] for video in database.values(): listofhashtags = [] if &quot;textExtra&quot; in video: for hashtag in video[&quot;textExtra&quot;]: if hashtag[&quot;hashtagName&quot;] not in exclude_hashtags: listofhashtags.append(hashtag[&quot;hashtagName&quot;]) if hashtag[&quot;hashtagName&quot;] not in hashtag_nodes: hashtag_nodes[hashtag[&quot;hashtagName&quot;]]=len(hashtag_nodes.keys())+1 for combination in combinations(listofhashtags, 2): hashtag_edges.append([hashtag_nodes[combination[0]], hashtag_nodes[combination[1]]])   and visualize it with pyvis:  net = pyvis.network.Network(notebook=False, cdn_resources='local') node_count = 1 for hashtag in hashtag_nodes: net.add_node(node_count, label=hashtag) node_count+=1 net.add_edges(hashtag_edges) net.show_buttons(filter_='physics') net.show('selected_hashtags.html')     ","version":"Next","tagName":"h3"},{"title":"What to do with the results​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/docs/data-analysis/04_02_hashtag-analysis#what-to-do-with-the-results","content":" Congratulations you've set up a TikTok scraper that collects data on videos shown on the FYP page and visualized them in a network. You can now start to dive deep into the data and identify topic clusters or learn what topics are dominating the network. ","version":"Next","tagName":"h2"},{"title":"Data collection methods","type":0,"sectionRef":"#","url":"/docs/data-collection/","content":"Data collection methods last updated on 01.08.2025 by Clara Ruthardt Data collection on social media and digital platforms comes with a range of specificities and nuances that, unfortunately, differ across each platform. To facilitate research and give you an idea of what’s possible on which platform, this section outlines data collection approaches and introduces examples for TikTok, X (Twitter), and blogs (including code). Additional examples or contributions to data collections methods are welcome, suggestions are listed under (4) call for contributions. Social media data types: An overview If you are wondering what kinds of data you can access and anylse on social media, this chapter will provide orientation and clarity around terminology. Learn more Common data collection methods Focusing on TikTok as a case study, this chapter offers insights into the myriad ways one can audit an online platform. It underscores the importance of aligning the chosen method with the research question at hand. Drawing on our experiences with auditing recommender systems, this chapter presents a holistic understanding of TikTok’s practices. Learn more Platform-specific guidelines, e.g. X API, Streaming data This chapter outlines platform-specific guidelines for data collection across X (Twitter), YouTube, Rumble, and Meta Ads on Facebook and Instagram. Each guideline includes step-by-step tutorials with code samples and an introduction to potential analysis methods. [Learn more](03_00_platform-specific guidelines) Webscraping techniques with R In a constantly changing digital environment, adaptability is key. As various social networks have begun to limit access to their APIs—either monetising them or closing them altogether — researchers face the challenge of capturing crucial data. The significance of webscraping has thus resurged, offering an alternative means of data collection. Learn more Open for contributions Contributions are welcome, particularly on additional data collection tactics and methods.","keywords":"","version":"Next"},{"title":"Introduction to social network analysis in R and Gephi","type":0,"sectionRef":"#","url":"/docs/data-analysis/04_03_social-network-analysis","content":"","keywords":"","version":"Next"},{"title":"Where to start: An introduction to Social Network Analysis​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#where-to-start-an-introduction-to-social-network-analysis","content":" Social network analysis (SNA) is a method used to study and better understand the relationships and interactions among individuals, groups, or entities. In social network analysis, you represent these relationships as a network, where each individual or entity is a node, and the connections between them are represented as edges. Depending on the terminology sometimes nodes are also called vertices.  SNA can help you uncover valuable insights about how information, resources, and influence flow within networked systems. It allows you to analyze patterns, identify key individuals or groups, and understand the overall structure and dynamics of a social network.  Early examples from sociology for instance, identified patterns of the spread of sexual diseases at high schools. Today in a digitalized world, human behavior leaves traces in many ways whether we write messages to someone on X (Twitter) or WhatsApp or walk through the city with our mobile phone. This certainly raises privacy concerns that are addressed by data protection regulations like the European Union’s GDPR. Privacy concerns and further ethical consideration need to be considered during all steps of a research project; when designing the methodological approach, during data collection and analysis as well as during the writing and publication process. Now, let’s consider a few examples of how social network analysis can be applied on real-world data:  Online Social Networks: Imagine you have data from a social media platform like Facebook or X (Twitter). You can use social network analysis in R to study patterns of communication, identify influential users, detect communities, and understand information diffusion. For instance, you can analyze how ideas or trends spread across a network of X (Twitter) users.Collaboration Networks: In academic or professional settings, social network analysis can be used to study collaboration patterns among researchers, employees, or organizations. For instance, each mention of another research creates a link to the author and the referenced article (or the article URL). By analyzing co-authorship networks or co-worker networks, you can identify key players, measure the impact of collaborations, and uncover potential research or business opportunities.Organizational Networks: Social network analysis can also be applied to understand the structure and dynamics of organizations. By examining communication networks within a company, you can identify bottlenecks, assess information flow, and evaluate the effectiveness of teams or departments. This analysis can help organizations optimize their structure and improve collaboration. This, however, comes with high ethical and privacy concerns, and you should ensure that your research is covered by employment or data protection laws.  In this tutorial, you will primarily learn about community detection on X (Twitter), how to analyze its networked structure, detect community structures based on users’ communication behavior. Some measures and terms that you will hear frequently and help to assess and understand network structures are the following:  Degree Centrality: The degree of a node is the number of other nodes that a node is connected to. Important nodes tend to have more connections to other nodes. Highly connected nodes are interpreted to have high degree centrality. In the later case of X (Twitter) “retweet networks” a high degree of incoming edges (indegree) represents being retweeted frequently and can be used to identify influential accounts in the given debate. The number of outgoing edges (outdegree) represents the number of times a given account or user retweeted other accounts. Regarding, retweet behavior, this may indicate partisan support or if the number is very high potentially inauthentic behavior of an account (e.g. through bots). Eigenvector Centrality: The extent to which adjacent nodes are connected also indicate importance (out differently: important nodes increase the importance of other nodes). This measure basically assesses how connected a node is on higher degrees of connection such as second and third-degree connections. Closeness centrality: Closeness centrality measures how many steps are required to access every other node from a given node. In other words, nodes with a high closeness centrality have easy access to other nodes given multiple connections. Betweenness Centrality: The betweenness centrality measure ranks the nodes based on the flow of connections through the network. Importance is demonstrated through high frequency of connection with multiple other nodes. Nodes with high levels of betweenness tend to serve as a bridge for multiple sets of other important nodes. Modularity: Modularity is a measure of the structure of networks which measures the strength of segments of a network into communities (also called groups, clusters or modules). Networks with high modularity have relatively densely connected components, that means there are dense connections between the nodes within communities but sparse connections between nodes in different communities (Newman 2006). Modularity is often used in optimization methods for detecting community structure in networks such as the Louvain algorithm (Blondel et al. 2008).  ","version":"Next","tagName":"h2"},{"title":"Getting started with social network analysis​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#getting-started-with-social-network-analysis","content":" ","version":"Next","tagName":"h2"},{"title":"Your setup​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#your-setup","content":" To perform social network analysis in R or Python, you can use packages and libraries like igraph, network, and sna. These packages provide functions to import network data, create network objects, visualize networks, and analyze network properties such as centrality measures (e.g., degree, betweenness) or community detection. By applying these techniques, you can gain sometimes mesmerizing insights into the social relationships and interactions that shape our lives and communities. Social network analysis in R allows you to explore, visualize, and understand complex social networks, ultimately helping you make informed decisions, improve social systems and in this tutorial’s example better understand large-scale political discourses online and the potential amplification fo false information.  ","version":"Next","tagName":"h3"},{"title":"Recommended reading​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#recommended-reading","content":" Remember, social network analysis is a vast field with numerous advanced methods and techniques. So, if you find it intriguing, there is a lot more to explore and learn! As introductory books I would recommend Carrington et al. (2005), Scott (2017) or Yang (2016). Other scholars and online researchers have also created tutorials introducing network analysis and its application based on a variety of tools from programming and data analysis software like R and Python to open-source OSINT tools like Gephi.  ","version":"Next","tagName":"h3"},{"title":"Parameters of example case​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#parameters-of-example-case","content":" The following tutorial is based on X (Twitter) data that was accessible for researchers via X’s (Twitter’s) Academic API until February 2023 when Elon Musk and Twitter closed the free Research API access to X’s (Twitter) data. The data used for this tutorial was collected using an R package called AcademicTwitter developed by colleagues from the research community.  Considering the implementation of the EU Digital Services Act, we can expect extended research data access for so-called vetted researchers at the latest by September 2024. Moreover, there should be APIs with limited research data access for a broad research community. Here you can find a policy brief arguing for the importance of research data access for policymaking and better understanding social implications of digital online platforms and search engines (https://opus4.kobv.de/opus4-hsog/frontdoor/deliver/index/docId/4947/file/Implementing_Data_Access_Darius_Stockmann_2023.pdf).  ","version":"Next","tagName":"h3"},{"title":"Step-by-Step tutorial on X (Twitter) networks in R​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#step-by-step-tutorial-on-x-twitter-networks-in-r","content":" But now let’s get started creating and analyzing our first X (Twitter) networks!  As selection criteria for your data collection, it is possible to filter for key words, hashtags or collect the timelines (messages shared) by users via user lists. This is mostly used for political elites like politicians, new network elites like political influencers or profiles of media outlets (whatever type of accounts you are interested in).  To illustrate this work, this tutorial replicated the work from Darius and Urquhart (2021), in which we collected data by choosing hashtags as selection criteria. When interested in information diffusion, the spread of information and messages on social networking sites like X (Twitter), retweeting is the most often used mechanism to amplify or share messages by other users. Retweeting also creates a link (edge) between two accounts (nodes), constituting a retweet network. These retweet networks often cluster into multiple communities, and for political hashtags, these communities represent different ideologies or opinions on a topic (Conover et al., 2012).  Thus, retweet networks provide the chance to assess ideological alignment and opinion leaders/influencers within communities where people self-sort via their retweeting behavior (Conover et al., 2012; Bruns et al., 2016). This self-sorting into different aligned communities happens because most users retweet in support of messages (Boyd et al., 2010; Metaxas et al., 2015). Moreover, users adopt retweets quicker than using hashtags in individual tweets (Oliveira et al., 2021). Thus, the analysis focuses on the retweet network of the vaccination hashtag. The following tutorial is based on X (Twitter) data collected during the Covid-19 pandemic and focuses on accounts that retweeted or posted messages using #vaccines during the observation period. Social network analysis enables the visualization and identification of communities. Thereafter, a qualitative inspection of most shared content in each community enables the identification of those that amplify disinformation or conspiracy narratives as a specific form of disinformation. In this tutorial, however, the qualitative inspection is limited because X’s (Twitter) data protection guidelines prohibit the sharing of screennames, and a lot of the accounts have been deleted. Thus, the data only includes pseudo-identifiers, and you may be better served by replicating the analysis and network creation with your self-collected data or research data from archives.  ","version":"Next","tagName":"h3"},{"title":"Tutorial on Social Network Analysis of Twitter Networks in R​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#tutorial-on-social-network-analysis-of-twitter-networks-in-r","content":" R is a free software environment for statistical computing and graphics that is used by researchers from a variety of disciplines. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS. You can download the software here and can find a step-by-step tutorial on installing R and R Studio here.  In this case you will use R to create and analyze a network data edge list from X (Twitter) data.  First Tab:  d &lt;- df_vaccines # create variable d$isRetweet d$RT &lt;- gsub(&quot;([A-Za-z]+).*&quot;, &quot;\\\\1&quot;, d$text) d$isRetweet &lt;- ifelse(d$RT == &quot;RT&quot;, TRUE, FALSE) # Split into retweets and original tweets sp = split(d, d$isRetweet) orig = sp[['FALSE']] # Extract the retweets and pull the original author's screenname rt = mutate(sp[['TRUE']], sender = substr(text, 5, regexpr(':', text) - 1)) # Adjust retweets to create an edgelist for network el = as.data.frame(cbind(sender = tolower(rt$sender), receiver = tolower(rt$author_id))) class(el$sender) class(el$receiver) el = count(el, sender, receiver) # create duplicate from edgelist df_el &lt;- el # rename columns for import into Gephi (Gephi requires a &quot;Source&quot; and &quot;Target&quot; column in order to read edgelists of directed networks) names(df_el)[1] &lt;- &quot;Target&quot; names(df_el)[2] &lt;- &quot;Source&quot; df_el$Weight &lt;- df_el$n # save the edgelist in csv-format (comma-spearated values) for easy import to Gephi write.csv(df_el, file = &quot;Vaccines_retweet_network.csv&quot;, row.names = FALSE)   Now you can 1) continue the analysis in R OR 2) import the edgelist into the open-source software Gephi to visualize and further analyze the network.  The most frequently used packages for network analysis in R are network (https://cran.r-project.org/web/packages/network/index.html), igraph (https://cran.r-project.org/web/packages/igraph/index.html), and sna (https://cran.r-project.org/web/packages/sna/index.html).  Hint A great introductory tutorial on Igraph is available here created by Katherine Ognyanova or here created by the developers of the igraph package.  Start by loading your edgelist to R:  # install packages install.packages('twitteR', 'dplyr', 'ggplot2', 'lubridate', 'network', 'sna', 'igraph', 'network', 'qdap', 'tm') install.packages(&quot;remotes&quot;) remotes::install_github(&quot;analyxcompany/ForceAtlas2&quot;) # Once installed, load the packages using the library() function: library(igraph) library(network) library(sna) library(dplyr) library(utils) library(ForceAtlas2) &quot;Step 2: Importing network data Next, you'll need to import your network data into R. The format of your data will depend on the specific network analysis you want to perform. For example, if you have an edge list (a list of connections between nodes), you can import it using the read.table() function:&quot; # Importing edge list (will be from a GitHub Page; GDrive or Dropbox) edge_list &lt;- read_csv(&quot;~/Documents/Hertie/Freelance/Bertelsmann Stiftung/Knowledge Hub/Bertelsmann Knowledge Hub_SNA Tutorial_Darius/Vaccine_edgelist_rtnet.csv&quot;) edge_list &lt;- Vaccine_edgelist_rtnet # make sure there are no NAs in your edgelist edge_list &lt;- na.omit(edge_list) # write_csv(edge_list, &quot;Vaccine_edgelist_rtnet.csv&quot;) &quot;If you have data in a different format, you may need to use a different function to import it. Step 3: Creating a network object once you have imported your network data, you need to create a network object that R can work with. The specific function you use will depends on the package you are using.&quot; # Create an igraph network object network_igraph &lt;- graph_from_data_frame(d = edge_list, directed = TRUE) # If you are using the network package, you can create a network object from the edge list using the network() function: rtnet &lt;- network(edge_list, directed = TRUE, loops = TRUE, multiple = TRUE) # Step 4: Basic network visualization To visualize the network, you can use the plot() function from the igraph package: # Visualize the network plot(rtnet, remove.multiple = F, remove.loops = T, edge.arrow.size=.4,vertex.label=NA, vertex.size2 = 0.1) plot(network_igraph, remove.multiple = F, remove.loops = T, edge.arrow.size=.4,vertex.label=NA, vertex.size2 = 0.1) # this shows only a 'ball' if nodes but we can see already a clustered structure # Large Graph Layout layout_with_lgl(network_igraph) # try ForceAtlas2 layout algorithm (this is computationally expansive an will take some time due to the size of the network) layout &lt;- layout.forceatlas2(network_igraph, iterations=3000, plotstep=100) plot(network_igraph, layout=layout) &quot;Step 5: Analyzing network properties Now that you have your network object, you can analyze various network properties. Here we focus on degree centrality and betweenness centrality: Degree centrality: The degree centrality of a node is the number of connections it so the number of edges adjacent to it. In case of directed networks, we can also define in-degree (the number of edges pointing towards the node/vertex) and out-degree (the number of edges originating from the node/vertex).&quot; # Network measures with Igraph (alternatively you can also calculate them with other network analysis packages for R like &quot;network&quot;, here we are using IGraph) degree(network_igraph) # Betweenness centrality: The betweenness centrality of a node measures the extent to which it lies on the shortest paths between other nodes. You can calculate the betweenness centrality using the betweenness() function: edge_betweenness(network_igraph) # the network package also allows for easy estimation of further centrality measures such as eigenvector centrality or closeness centrality (see 6.7 Centrality &amp; Centralization in https://kateto.net/netscix2016.html)   If you are new to network analysis, you may want to start by exploring the network graph in Gehpi and then turning to further analysis in R or Python. Gephi works well with large network data sets and produces extremely good-looking graphs providing color palettes, and a variety of layout algorithms and network measures.  ","version":"Next","tagName":"h3"},{"title":"Tutorial on Social Network Analysis of Twitter Networks in Python​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#tutorial-on-social-network-analysis-of-twitter-networks-in-python","content":" This is a short introduction on social network analysis in Python. Python is a high-level, general-purpose programming language used by many researchers and data scientists. In this example you run the script in Google Colab but you can also run the script in Jupyter Notebooks or an integrated development environment (IDE) of your choice.  # Intro2SNA - Analysing Social Networks with Python # Step 1: Install and load the igraph package First, make sure you have the igraph package installed. Run the following code to install it: !pip install python-igraph !pip install pandas !pip install pycairo !pip install cairocffi # Then, load the package using the import statement: import igraph as ig # Step 2: Import the edgelist data Assuming you have an edgelist CSV file with columns 'source' and 'target' representing retweet connections, you can import it using the read() function from the pandas library: import pandas as pd # mount Drive (if in Colabs or load from local disk or GitHub) from google.colab import drive drive.mount('/content/drive') # Import the edgelist data edgelist = pd.read_csv('/content/drive/MyDrive/Vaccine_edgelist_rtnet.csv') # Step 3: Create a network object Next, create a network object using igraph's Graph() function. You can pass the source and target columns from the edgelist DataFrame as arguments: # Create a graph from the edgelist data network = ig.Graph.TupleList(edgelist.itertuples(index=False), directed=True) # Step 4: Basic network visualization To visualize the network, use the plot() function: ig.plot(network, bbox=(2000, 2000)) # This will generate a basic plot of the retweet network. # Step 5: Analyzing network properties Now, you can analyze various network properties using igraph. Here are a few examples: # Degree centrality: Calculate the in-degree centrality (number of retweets received) using the indegree() function: indegree_centrality = network.indegree() print(indegree_centrality) #plt.plot(indegree_centrality) # Betweenness centrality: Calculate the betweenness centrality using the betweenness() function: betweenness_centrality = network.betweenness() print(betweenness_centrality) # Community detection: Detect communities within the network using edge-betweenness for directed graphs: community_EB = network.community_edge_betweenness() print(community_EB) # Igraph and other packages provide many more community detection algorithms that you can compare https://igraph.org/python/doc/api/igraph.Graph.html#community_multilevel # Step 6: To visualize network properties, you can use different plotting functions. Here are a few examples: # Visualizing in-degree centrality ig.plot(network, vertex_size=indegree_centrality, bbox=(4000, 4000)) # Visualizing betweenness centrality: ig.plot(network, vertex_size=betweenness_centrality, bbox=(4000, 4000)) # As we can see the network graphs look rather messy but by adjusting the layout, node size and gravity we can make them look nicer. In Gephi, however, the visualization of large networks is much more straight-forward.   You can find more extensive tutorials for further investigation of networks in Python on YouTube, GitHub or by searching the Web. These are well done examples:  Network Analysis and Community Detection on Political Tweets by Kristina Popova on MediumIGraph TutorialNetworkx Tutorial  ","version":"Next","tagName":"h2"},{"title":"Implementing Social Network Analysis of X (Twitter) Networks in Gephi​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#implementing-social-network-analysis-of-x-twitter-networks-in-gephi","content":" Gephi is an open-source software tool for (social) network analysis and alllows for great-looking visualizations, even of large-scale networks. You can download Gephi for free here.  Now import our edgelist file Vaccine_edgelist_rtnet.csv in Gephi as a spreadsheet: File &gt; Import spreadsheet” and select “Separator = Comma” (because we are importing a comma-separated values file), “Import as: Edges table” and “Charset = UTF-8”, which should be the default anyway. In the second import step you need to select a time representation, which is irrelevant in this case but matters if you are importing dynamic network data that inspects changes over time. You can select “Time representation = Intervals” and press “Finish”.  Now the import report window should open and show you 4593 nodes and 8554 edges that were correctly read (you canignore all values with missing source or target nodes IDs). Choose “Graph Type = Directed”, because retweeting someone has a direction and we are interested in who was most retweeted by others and who most frequently retweeted others to detect patterns in accounts’ behavior. Select “New workspace”, press “More options…” and “Edges merge strategy = SUM”, then press “OK” at the bottom right corner.  Now the imported network data appears as a black square on your Gephi window (see Screenshot 1 below).    On the right-hand side (red circle) of the Gephi interface you can run analyses by pressing the “Run” button. For this tutorial press “RUN” for “Average Degree”, “Avg. Weighted Degree” and “Modularity” as a community detection method based on the Louvain algorithm. In the Box that opens click “Randomize” and “Use Weights”, then choose “Resolution: 5.0”. Increasing the resolution parameter, based on Lambiotte et al. (2009), results in less-fractioned communities, if they seem too large for you, you can re-run the modularity algorithm with a lower resolution parameter.  On the left-hand side (blue circle) of the Gephi interface, you can choose a layout algorithm to visualize the network. Select ForceAtlas2 and press “Run” with the default parameters. Now the black square starts to disintegrate as the layout algorithms force unconnected nodes to repel each other. Let the network expand until it appears to not change anymore and converge.  Now you can already see several distinct clusters where nodes are more densely connected with each other than with nodes in other clusters. The next step assigns a color to network nodes based on the modularity groups that were identified with the Louvain algorithm.  In screenshot 2 the green circle indicates where to select “Partition” by “Modularity Class” in the top left corner to assign modularity class color to nodes in the network. You can change the colors by clicking on the color squares and choosing from a color palette for each community or choose a pre-saved palette by clicking on “Palette” in the bottom right corner. Choose colors or a palette with sufficient contrast to the background to help with accessibility.    After applying the modularity class-based coloring you can see that that community membership largely collides with the network clusters. Now you use qualitative content analysis on a sample of accounts or the most retweeted account (those with the highest indegree) to get an idea of the sort of content that is shared in each of the identified communities. To do so, click on the top bar on “Data Laboratory” and sort for weighted indegree, which is the sum of times an account was retweeted. Now you can inspect the accounts that were most retweeted, their position in the network graph and look them up on X (Twitter). For this tutorial we have dehydrated or anonymized for privacy protection and to comply with X’s (Twitter’s) data sharing regulations. Within your self-collected data, you can look up accounts via usernames or X (Twitter)-IDs.  The following video briefly illustrates the steps of analysis in Gephi:    Hint On YouTube and other platforms there are many more great Gephi video tutorials. For instance, I would recommend Martin Grandjean's tutorial for the start, which you can find here.  ","version":"Next","tagName":"h2"},{"title":"Research Example​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#research-example","content":" In Darius and Urquhart (2021) we inspect the emergence and growth of conspiracy communities on UK Twitter during the Covid-19 pandemic using social network analyis and qualitative content analysis with which we identify those communities and accounts that issue and share messages aligned with conspiracy narratives about vaccination (e.g. that vaccinations implant micro-chips to control the population). In general the detection of communities with community detection algrorithms should be accompanied with automated text analysis or qualitative analysis of content circulated in the identified communities. In the study we identified a high presence of tweets referencing conspiracy narratives and noted an overproportional growth of conspiracy communities. The following figure from Darius and Urquhart (2021) illustraes the anti-vaccination community before and after the introduction of so-called lockdown policies to reduce the spread of Covid-19. The graph illustrate that the anti-vaccination movement constitues about a third of retweets with regard to the #Vaccination during the later observation period.    This resonated with studies by others, that identifed a growing presence of conspiracy narratives about vaccinations but also with respect to 5G during the Covid19 pandemic (Ahmed et al., 2020; Yang et al., 2021). You can find a lot of interdisciplinary research using network approaches in the journals “Social Networks”, “Online Social Networks and Mining”, or “Online Social Networks and Media”. Have fun exploring the world of social and complex networks!  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/docs/data-analysis/04_03_social-network-analysis#references","content":" Ahmed, Wasim, Josep Vidal-Alaball, Joseph Downing, and Francesc López Seguí. “COVID-19 and the 5G Conspiracy Theory: Social Network Analysis of Twitter Data.” Journal of Medical Internet Research 22, no. 5 (May 6, 2020): e19458. https://doi.org/10.2196/19458. Blondel, Vincent D., Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. “Fast Unfolding of Communities in Large Networks.” Journal of Statistical Mechanics: Theory and Experiment 2008, no. 10 (October 9, 2008): P10008. https://doi.org/10.1088/1742-5468/2008/10/P10008. Boyd, Danah, Scott Golder, and Gilad Lotan. “Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter.” In 2010 43rd Hawaii International Conference on System Sciences, 1–10, 2010. https://doi.org/10.1109/HICSS.2010.412. Bruns, Axel, Brenda Moon, Avijit Paul, and Felix Münch. “Towards a Typology of Hashtag Publics: A Large-Scale Comparative Study of User Engagement across Trending Topics.” Communication Research and Practice 2, no. 1 (January 2, 2016): 20–46. https://doi.org/10.1080/22041451.2016.1155328. Caplan, Robyn, and Danah Boyd. “Who Controls the Public Sphere in an Era of Algorithms.” Mediation, Automation, Power, 2016, 1–19. Carrington, Peter J., John Scott, and Stanley Wasserman. Models and Methods in Social Network Analysis. Vol. 28. Cambridge university press, 2005. Conover, Michael D., Bruno Gonçalves, Alessandro Flammini, and Filippo Menczer. “Partisan Asymmetries in Online Political Activity.” EPJ Data Science 1, no. 1 (December 2012): 1–19. https://doi.org/10.1140/epjds6. Darius, Philipp, and Michael Urquhart. “Disinformed Social Movements: A Large-Scale Mapping of Conspiracy Narratives as Online Harms during the COVID-19 Pandemic.” Online Social Networks and Media 26 (November 2021): 100174. https://doi.org/10.1016/j.osnem.2021.100174. Gaisbauer, Felix, Armin Pournaki, Sven Banisch, and Eckehard Olbrich. “Grounding Force-Directed Network Layouts with Latent Space Models.” Journal of Computational Social Science, May 29, 2023. https://doi.org/10.1007/s42001-023-00207-w. Lambiotte, R., J.-C. Delvenne, and M. Barahona. “Laplacian Dynamics and Multiscale Modular Structure in Networks.” IEEE Transactions on Network Science and Engineering 1, no. 2 (July 1, 2014): 76–90. https://doi.org/10.1109/TNSE.2015.2391998. Lancichinetti, Andrea, and Santo Fortunato. “Community Detection Algorithms: A Comparative Analysis.” Physical Review E 80, no. 5 (November 30, 2009): 056117. https://doi.org/10.1103/PhysRevE.80.056117. Metaxas, Panagiotis, Eni Mustafaraj, Kily Wong, Laura Zeng, Megan O’Keefe, and Samantha Finn. “What Do Retweets Indicate? Results from User Survey and Meta-Review of Research.” Proceedings of the International AAAI Conference on Web and Social Media 9, no. 1 (2015): 658–61. Newman, M. E. J. “Modularity and Community Structure in Networks.” Proceedings of the National Academy of Sciences 103, no. 23 (June 6, 2006): 8577–82. https://doi.org/10.1073/pnas.0601602103. Oliveira, Jaqueline Faria de, Humberto Torres Marques-Neto, and Márton Karsai. “Measuring the Effects of Repeated and Diversified Influence Mechanism for Information Adoption on Twitter.” Social Network Analysis and Mining 12, no. 1 (December 8, 2021): 16. https://doi.org/10.1007/s13278-021-00844-x. Schoch, David, Franziska B. Keller, Sebastian Stier, and JungHwan Yang. “Coordination Patterns Reveal Online Political Astroturfing across the World.” Scientific Reports 12, no. 1 (March 17, 2022): 4572. https://doi.org/10.1038/s41598-022-08404-9. Scott, John. Social Network Analysis. 1 Oliver’s Yard, 55 City Road London EC1Y 1SP: SAGE Publications Ltd, 2017. https://doi.org/10.4135/9781529716597. Yang, Kai-Cheng, Francesco Pierri, Pik-Mai Hui, David Axelrod, Christopher Torres-Lugo, John Bryden, and Filippo Menczer. “The COVID-19 Infodemic: Twitter versus Facebook.” Big Data &amp; Society 8, no. 1 (January 2021): 205395172110138. https://doi.org/10.1177/20539517211013861. Yang, Song, Franziska B. Keller, and Lu Zheng. Social Network Analysis: Methods and Examples. SAGE Publications, 2016. ","version":"Next","tagName":"h2"},{"title":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","type":0,"sectionRef":"#","url":"/docs/data-analysis/04_05_metatargetr","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#introduction","content":" What political messages are being sent to which audiences? By using digital advertising tools provided by social media platforms, advertisers can select which audiences they want to reach and are able to send different messages for each one of them. That is why understanding the use of target audiences is as crucial as knowing what is being advertised. While Meta’s official Ad Library API provides a range of data, its restrictive access requirements and limited targeting granularity often fall short of the needs of researchers and journalists. The metatargetr R package offers an alternative, unofficial route to this information, by retrieving and archiving ad data directly from Meta’s public-facing Ad Library.  This tutorial focuses on how to use metatargetr to retrieve and, most importantly, analyze this targeting data. Unlike the official Meta Ad Library API, which provides data on an ad-by-ad basis with broad spending ranges, metatargetr offers a unique, advertiser-level perspective.  Here are the key advantages over Meta’s official Ad Library API:  Advertiser-Level Data: Instead of looking at single ads, we get a consolidated view of the entire targeting strategy of a Facebook page or Instagram account. Exact Spending per Criterion: The data provides the exact percentage of a page’s total spend allocated to each specific targeting criterion. This allows for precise analysis of budget allocation, a feature not available through the official API. Additional Audience Insights: The dataset goes beyond the demographic targeting that is available via the API, as it reveals spending on powerful tools like Detailed Targeting (e.g. interest profiles that Meta categorizes its users in), Custom Audiences (e.g., targeting a list of existing customers) and Lookalike Audiences (targeting users similar to an existing audience). Furthermore, while only demographic targeting (age, gender, location) is available for countries in the EU since the implementation of the Digital Services Act (DSA),metatargetr provides targeting insights for all advertising pages on Meta in the world.  This tutorial will walk you through installing the package, retrieving historical targeting data, and using tidyverse tools to create compelling visualizations that reveal these hidden strategies.  Finally, it is important to note that metatargetr relies on web scraping. This means it can be susceptible to changes in Facebook’s website structure. While powerful, it should be considered a complementary tool to the official API. For a guide on using the official Meta Ad Library API, please see the other tutorial in this series.  ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#installation","content":" First, you will need to install metatargetr and a few other helpful packages from the tidyverse ecosystem. Since metatargetr is hosted on GitHub, we will use the pak package for a smooth installation process.  # Install pak if you don't have it already if (!require(&quot;pak&quot;)) install.packages(&quot;pak&quot;) # Install `metatargetr` and other useful packages pak::pak(c( &quot;favstats/metatargetr&quot;, &quot;tidyverse&quot;, &quot;lubridate&quot;, &quot;scales&quot; ))   Now loading in the R packages:  library(metatargetr) library(tidyverse) library(lubridate) library(scales)   ","version":"Next","tagName":"h3"},{"title":"Retrieving Targeting Information for Recent Ads (get_targeting)​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#retrieving-targeting-information-for-recent-ads-get_targeting","content":" The core function for fetching live targeting data is get_targeting(). This function scrapes the “Audience” tab of a Page’s Ad Library section, providing insights into the age, gender, location but also custom and lookalike audience targeting of their recent ads (i.e. only for the last 7, 30, and 90 days).  Let us retrieve the targeting data for a specific Facebook Page. You will need the Page ID, which can be found in the URL of the page’s Ad Library entry or in the Meta Ad Library Report (which you can also query via metatargetr using get_ad_report).  For example, here is the URL of the Ad Library page of the U.S. Department of Homeland Security (DHS):  https://www.facebook.com/ads/library/?view_all_page_id=179587888720522  The Page ID is the number after view_all_page_id= → 179587888720522.  Here are the most important function parameters:  id: A character string representing the (Facebook) Page ID. timeframe: The time period for the data. Options are “LAST_7_DAYS”, “LAST_30_DAYS”, or “LAST_90_DAYS”. The default is set at “LAST_30_DAYS”.  Let us retrieve the targeting info from a page that is currently conducting large digital ad campaign, the U.S. Department of Homeland Security:  # Fetch targeting data for a specific page for the last 30 days dhs_targeting_data &lt;- get_targeting(id = &quot;179587888720522&quot;, timeframe = &quot;LAST_30_DAYS&quot;) # Inspect the structure of the returned data glimpse(dhs_targeting_data)    ## Rows: 91 ## Columns: 15 ## $ value &lt;chr&gt; &quot;All&quot;, &quot;Women&quot;, &quot;Men&quot;, &quot;Spanish&quot;, &quot;Boston (Manch… ## $ num_ads &lt;int&gt; 15, 0, 0, 15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ total_spend_pct &lt;dbl&gt; 1.00000000, 0.00000000, 0.00000000, 1.00000000, … ## $ type &lt;chr&gt; &quot;gender&quot;, &quot;gender&quot;, &quot;gender&quot;, &quot;language&quot;, &quot;locat… ## $ location_type &lt;chr&gt; NA, NA, NA, NA, &quot;geo_markets&quot;, &quot;COUNTY&quot;, &quot;BOROUG… ## $ num_obfuscated &lt;int&gt; NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,… ## $ is_exclusion &lt;lgl&gt; NA, NA, NA, NA, FALSE, FALSE, FALSE, FALSE, FALS… ## $ detailed_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ ds &lt;chr&gt; &quot;2025-07-24&quot;, &quot;2025-07-24&quot;, &quot;2025-07-24&quot;, &quot;2025-… ## $ main_currency &lt;chr&gt; &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;,… ## $ total_num_ads &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, … ## $ total_spend_formatted &lt;chr&gt; &quot;$347,094&quot;, &quot;$347,094&quot;, &quot;$347,094&quot;, &quot;$347,094&quot;, … ## $ is_30_day_available &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, … ## $ is_90_day_available &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, … ## $ page_id &lt;chr&gt; &quot;179587888720522&quot;, &quot;179587888720522&quot;, &quot;179587888…   The output will be a tibble where each row represents a specific demographic or location targeting segment for the ads run by that page in the specified timeframe. Key columns include:  value: The exact targeting criterion used. type: The category of targeting (e.g., “age”, “gender”, “location”, etc.). total_spend_pct: The proportion of the page’s ad spend directed at a target audience (value) within the timeframe specified. total_spend_formatted: The total ad spend of the page in the timeframe specified. main_currency: Information about the currency used by the page.  ","version":"Next","tagName":"h3"},{"title":"Historical Targeting Data at Scale (get_targeting_db)​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#historical-targeting-data-at-scale-get_targeting_db","content":" The get_targeting() function is enough for retrieving targeting info for specific pages that have run ads up to the last 90 days. However, once 90 days have passed, Meta does not provide access to this data anymore. This is where metatargetr’s true power lies: it retrieves, archives, and provides access to a vast repository of pre-collected targeting data for all pages running political advertisements in the world. This data can be accessed through the get_targeting_db()function, which downloads historical datasets have been collected since late 2023.  Here are the most important function parameters:  the_cntry: The two-letter ISO country code for the desired dataset (e.g., “DE”, “US”). tf: The timeframe in days (“LAST_7_DAYS”, “LAST_30_DAYS”, “LAST_90_DAYS”). ds: A date string in “YYYY-MM-DD” format, identifying the date of the specific archived dataset.  A tip before you download a historical dataset: theget_targeting_metadata() function allows you to see the available dates for a given country and timeframe as the automated retrieval process established by the package may have missed a certain date for a specific country (or because Meta skipped them).  # Get metadata for 90-day timeframe datasets in the US us_90_day_metadata &lt;- get_targeting_metadata(country_code = &quot;US&quot;, timeframe = &quot;90&quot;) # View the most recent available dates head(us_90_day_metadata)    ## # A tibble: 6 × 3 ## cntry ds tframe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 US 2025-07-24 last_90_days ## 2 US 2025-07-23 last_90_days ## 3 US 2025-07-22 last_90_days ## 4 US 2025-07-21 last_90_days ## 5 US 2025-07-20 last_90_days ## 6 US 2025-07-18 last_90_days   Once you have identified a dataset you want, you can download it withget_targeting_db(). Note that the archived data is typically available with a delay of a few days. This function allows for powerful longitudinal analysis across thousands of advertisers.  # Download the US 90-day targeting dataset from a specific date in the past us_targeting_archive &lt;- get_targeting_db(the_cntry = &quot;US&quot;, tf = &quot;90&quot;, ds = &quot;2025-06-30&quot;) # Inspect the archived data # It contains the same structure as the live data, but for thousands of pages glimpse(us_targeting_archive)    ## Rows: 690,219 ## Columns: 37 ## $ internal_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ no_data &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ tstamp &lt;dttm&gt; 2025-07-04 05:30:28, 2025-07-04 05:30:28, 20… ## $ page_id &lt;chr&gt; &quot;761832453834971&quot;, &quot;761832453834971&quot;, &quot;761832… ## $ cntry &lt;chr&gt; &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US… ## $ page_name &lt;chr&gt; &quot;League of Independent Voters of Texas&quot;, &quot;Lea… ## $ partyfacts_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ sources &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ country &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ party &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ left_right &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ tags &lt;glue&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ tags_ideology &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ disclaimer &lt;chr&gt; &quot;LEAGUE OF INDEPENDENT VOTERS OF TEXAS&quot;, &quot;LEA… ## $ amount_spent_usd &lt;chr&gt; &quot;405&quot;, &quot;405&quot;, &quot;405&quot;, &quot;405&quot;, &quot;405&quot;, &quot;405&quot;, &quot;40… ## $ number_of_ads_in_library &lt;chr&gt; &quot;14&quot;, &quot;14&quot;, &quot;14&quot;, &quot;14&quot;, &quot;14&quot;, &quot;14&quot;, &quot;14&quot;, &quot;14… ## $ date &lt;chr&gt; &quot;2025-06-29&quot;, &quot;2025-06-29&quot;, &quot;2025-06-29&quot;, &quot;20… ## $ path &lt;chr&gt; &quot;extracted/FacebookAdLibraryReport_2025-06-29… ## $ tf &lt;chr&gt; &quot;last_90_days&quot;, &quot;last_90_days&quot;, &quot;last_90_days… ## $ remove_em &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL… ## $ total_n &lt;int&gt; 35543, 35543, 35543, 35543, 35543, 35543, 355… ## $ amount_spent &lt;dbl&gt; 405, 405, 405, 405, 405, 405, 405, 405, 405, … ## $ value &lt;chr&gt; &quot;All&quot;, &quot;Women&quot;, &quot;Men&quot;, &quot;Bastrop, TX, United S… ## $ num_ads &lt;int&gt; 14, 0, 0, 7, 1, 6, 1, 0, 0, 0, 0, 0, 14, 14, … ## $ total_spend_pct &lt;dbl&gt; 1.00000000, 0.00000000, 0.00000000, 0.6351905… ## $ type &lt;chr&gt; &quot;gender&quot;, &quot;gender&quot;, &quot;gender&quot;, &quot;location&quot;, &quot;lo… ## $ location_type &lt;chr&gt; NA, NA, NA, &quot;CITY&quot;, &quot;CITY&quot;, &quot;countries&quot;, &quot;zip… ## $ num_obfuscated &lt;int&gt; NA, NA, NA, 2, 1, 0, 0, NA, NA, NA, NA, NA, N… ## $ is_exclusion &lt;lgl&gt; NA, NA, NA, FALSE, FALSE, FALSE, FALSE, NA, N… ## $ custom_audience_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ds &lt;chr&gt; &quot;2025-06-30&quot;, &quot;2025-06-30&quot;, &quot;2025-06-30&quot;, &quot;20… ## $ main_currency &lt;chr&gt; &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;USD&quot;, &quot;US… ## $ total_num_ads &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1… ## $ total_spend_formatted &lt;dbl&gt; 405, 405, 405, 405, 405, 405, 405, 405, 405, … ## $ is_30_day_available &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU… ## $ is_90_day_available &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU… ## $ detailed_type &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…   ","version":"Next","tagName":"h3"},{"title":"Analyzing DHS Ad Campaigns 🏛️​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#analyzing-dhs-ad-campaigns-️","content":" Now, let us apply what we have learned in a practical case study. We will analyze the ad campaigns of the U.S. Department of Homeland Security (DHS) to showcase how to combine historical data and visualize targeting strategies. In this case study we analyze audience segments derived from detailed targeting, how spend is allocated across those segments, the county-level geographic focus of the campaign, and which other advertisers use DHS-style proxy targeting. If you are interested in a similar analysis I have conducted for a recent blog post of mine, you can read the full post here.  Figure shows the ads that DHS runs which has DHS secretary Kristi Noem threaten &quot;if you are here illegally we will find and you and deport you&quot; with Spanish subtitles.  First, we will use get_targeting_db() to download two historical snapshots of U.S. ad data. By combining data from different time points, we can create a more comprehensive picture of an advertiser’s strategy over a longer period.  ","version":"Next","tagName":"h2"},{"title":"Retrieving and Combining Data​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#retrieving-and-combining-data","content":" # Download two datasets from different dates targeting_jun &lt;- get_targeting_db(the_cntry = &quot;US&quot;, tf = &quot;90&quot;, ds = &quot;2025-06-30&quot;) targeting_apr &lt;- get_targeting_db(the_cntry = &quot;US&quot;, tf = &quot;90&quot;, ds = &quot;2025-04-01&quot;) # Combine the datasets and filter for the DHS page # The DHS Facebook Page ID is &quot;179587888720522&quot; dhs_data_raw &lt;- targeting_jun %&gt;% bind_rows(targeting_apr) %&gt;% filter(page_id == &quot;179587888720522&quot;)   The raw data contains spending percentages relative to the total spend at the time of each snapshot. To get an accurate picture of the total spend over our combined period, we need to aggregate the data correctly. The helper function aggr_targeting recalculates the spending for each targeting criterion based on the new, combined total.  # Apply the function to our DHS data dhs_data_agg &lt;- aggr_targeting(dhs_data_raw)   Note The aggr_targeting() function creates a variable called spend_per which shows the spend per targeting criterion across the combined period. If you pull raw data with get_targeting_db(), you have to compute this measure yourself as total spending (total_spend_formatted) × share of spending (total_spend_pct).  ","version":"Next","tagName":"h3"},{"title":"Visualizing \"Detailed\" Targeting Criteria​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#visualizing-detailed-targeting-criteria","content":" One of the most powerful features of this dataset is the ability to see the specific interests and behaviors an advertiser targets – data that is not available via the typical API. Let us visualize what portion of the DHS budget was spent on these “detailed” targeting criteria.  # Filter for &quot;detailed&quot; targeting types and plot the top categories dhs_data_agg %&gt;% filter(type == &quot;detailed&quot;) %&gt;% # For better labels, combine the type and value mutate( display_value = str_wrap(paste0(str_to_title(detailed_type), &quot;: &quot;, value), width = 40) ) %&gt;% # Keep the top 10 criteria by spend slice_max(order_by = spend_per, n = 10) %&gt;% # Create the plot ggplot(aes(x = spend_per, y = fct_reorder(display_value, spend_per))) + geom_col(fill = &quot;#003366&quot;) + geom_text( aes(label = dollar(spend_per, accuracy = 1)), hjust = -0.1, size = 3.5, fontface = &quot;bold&quot; ) + scale_x_continuous( labels = label_dollar(), expand = expansion(mult = c(0, 0.15)) ) + labs( title = &quot;DHS Detailed Targeting: Top Spending Categories&quot;, subtitle = &quot;Spend on interests, behaviors, and demographic categories on Meta's platforms.&quot;, x = &quot;Estimated Ad Spend (USD)&quot;, y = NULL, caption = &quot;Data Source: Meta Ad Library via metatargetr&quot; ) + theme_minimal(base_size = 14) + theme( panel.grid.major.y = element_blank(), plot.title.position = &quot;plot&quot; )     This chart clearly shows the specific audience segments the DHS prioritized. We can see a significant focus on users who are interested in Mexican culture events and music, and whose language is set to Spanish. This provides concrete, data-driven insights into their campaign strategy that would be difficult to obtain otherwise.  Note When multiple targeting criteria show identical spend, that likely indicates they were applied together on the same underlying ads. Because the data that is retrieved is aggregated at the advertiser level it is hard to isolate ads that ran simultaneously. For a better measurement of spend we could divide the total spending by the number of targeting criteria that have the same data (same number of ads and spending) by assuming that spending was divided about equally to each targeting criterion.  ","version":"Next","tagName":"h3"},{"title":"Visualizing Location Targeting Criteria​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#visualizing-location-targeting-criteria","content":" Next, let us analyze where the money was spent. We can filter the data for location targeting and visualize the top-spending counties. This reveals the geographic focus of the campaigns.  # Filter for county-level location targeting and plot the top 15 dhs_data_agg %&gt;% filter(location_type == &quot;COUNTY&quot;) %&gt;% slice_max(order_by = spend_per, n = 15) %&gt;% mutate( location_label = str_remove(value, &quot;, United States&quot;) ) %&gt;% ggplot(aes(x = spend_per, y = fct_reorder(location_label, spend_per))) + geom_col(fill = &quot;#c00000&quot;) + geom_text( aes(label = dollar(spend_per, accuracy = 1)), hjust = -0.1, size = 3.5, fontface = &quot;bold&quot; ) + scale_x_continuous( labels = label_dollar(), expand = expansion(mult = c(0, 0.15)) ) + labs( title = &quot;Top 15 US Counties by DHS Ad Spend&quot;, subtitle = &quot;Estimated ad spending by the Department of Homeland Security on Meta's platforms.&quot;, x = &quot;Estimated Ad Spend (USD)&quot;, y = NULL, caption = &quot;Data Source: Meta Ad Library via metatargetr&quot; ) + theme_minimal(base_size = 14) + theme( panel.grid.major.y = element_blank(), plot.title.position = &quot;plot&quot; )     This visualization instantly reveals the geographic focus of the DHS’s advertising efforts. The spending is heavily concentrated in major metropolitan area, particularly in states like California, Texas, New York, and Florida with a large share of Latino citizen. This kind of analysis is invaluable for understanding the geographic scope and strategy of public information campaigns.  ","version":"Next","tagName":"h3"},{"title":"Visualizing Top 10 Spenders using Ethnic Proxies​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#visualizing-top-10-spenders-using-ethnic-proxies","content":" Who else is using targeting these targeting criteria that DHS utilizes as proxies to reach the Latino community?  First, the detailed targeting criteria used by the DHS ads are filtered, and all of the data is aggregated.  latino_targeting &lt;- dhs_data_agg %&gt;% filter(type == &quot;detailed&quot;) %&gt;% select(value, type) %&gt;% ## lets remove friends of football fans as that is not related by itself filter(value != &quot;Friends of football fans&quot;) us_aggr &lt;- targeting_jun %&gt;% bind_rows(targeting_apr) %&gt;% inner_join(latino_targeting) %&gt;% aggr_targeting()   Next, I am focusing only on the top 10 spenders that have used these targeting categories.  top_10_data_for_plot &lt;- us_aggr %&gt;% distinct(page_id, total_spend) %&gt;% arrange(desc(total_spend)) %&gt;% slice(1:10) %&gt;% select(page_id)   Finally, we can put everything together and reveal who the top spenders are:  us_aggr %&gt;% inner_join(top_10_data_for_plot) %&gt;% mutate(page_name = fct_reorder(page_name, spend_per)) %&gt;% ggplot(aes(x = page_name, y = spend_per)) + geom_boxplot() + # geom_col is better for this; position=&quot;stack&quot; is default scale_y_continuous( labels = dollar_format(prefix = &quot;$&quot;, scale = 1/1000, suffix = &quot;k&quot;), expand = c(0, 0) # Make the bars start at the y-axis ) + labs( title = &quot;Top 10 Advertisers Using DHS's 'Latino Proxy' Targeting&quot;, subtitle = &quot;Breakdown of ad spend by detailed targeting criterion&quot;, y = &quot;Total Ad Spend (USD)&quot;, x = NULL ) + theme_minimal(base_family = &quot;sans&quot;) + theme( legend.position = &quot;bottom&quot;, panel.grid.major.y = element_blank(), # Cleaner look panel.grid.minor.x = element_blank(), axis.text.y = element_text(face = &quot;bold&quot;) ) + coord_flip()     Beyond the Department of Homeland Security, the analysis reveals that the targeting criteria used as proxies for the Latino community are also employed by a diverse range of other major advertisers. The top 10 spenders using these methods include:  Public Health &amp; Advocacy Groups: Organizations like Planned Parenthood, Tobacco Free Florida, and The California Endowment use this targeting for outreach and awareness campaigns. Non-Profits and Charities: Pages such as Catholic Relief Services, Operation Smile, and The International Fellowship of Christians and Jews appear to use these criteria for fundraising and supporter engagement.  This confirms that while the DHS campaign is unique in its scale and messaging, the underlying methods for reaching the Latino community are common tools used by a wide array of organizations across the non-profit and public health sectors.  ","version":"Next","tagName":"h3"},{"title":"A Glimpse into Other metatargetr Features​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#a-glimpse-into-other-metatargetr-features","content":" Beyond its core focus on targeting, metatargetr includes a suite of functions for retrieving other valuable types of data, enabling a more holistic analysis of digital advertising. Below is a very short showcase of them:  ","version":"Next","tagName":"h2"},{"title":"Facebook and Instagram Page Information​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#facebook-and-instagram-page-information","content":" To complement the targeting data, you can retrieve detailed metadata about Facebook pages, such as like counts, creation dates, and contact information, using get_page_insights(). For historical page information and from many pages at once, the package also offersget_page_info_db().  dhs_page_info &lt;- get_page_insights(pageid = &quot;179587888720522&quot;, include_info = &quot;page_info&quot;) glimpse(dhs_page_info)    ## Rows: 1 ## Columns: 24 ## $ page_name &lt;chr&gt; &quot;Department of Homeland Security&quot; ## $ is_profile_page &lt;chr&gt; &quot;FALSE&quot; ## $ page_is_deleted &lt;chr&gt; &quot;FALSE&quot; ## $ page_is_restricted &lt;chr&gt; &quot;FALSE&quot; ## $ has_blank_ads &lt;chr&gt; &quot;FALSE&quot; ## $ hidden_ads &lt;chr&gt; &quot;0&quot; ## $ page_profile_uri &lt;chr&gt; &quot;https://facebook.com/homelandsecurity&quot; ## $ page_id &lt;chr&gt; &quot;179587888720522&quot; ## $ page_verification &lt;chr&gt; &quot;BLUE_VERIFIED&quot; ## $ entity_type &lt;chr&gt; &quot;PERSON_PROFILE&quot; ## $ page_alias &lt;chr&gt; &quot;homelandsecurity&quot; ## $ likes &lt;chr&gt; &quot;1011008&quot; ## $ page_category &lt;chr&gt; &quot;Government organisation&quot; ## $ ig_verification &lt;chr&gt; &quot;TRUE&quot; ## $ ig_username &lt;chr&gt; &quot;dhsgov&quot; ## $ ig_followers &lt;chr&gt; &quot;313631&quot; ## $ shared_disclaimer_info &lt;chr&gt; &quot;[]&quot; ## $ about &lt;chr&gt; &quot;Official Facebook page of the U.S. Department … ## $ event &lt;chr&gt; &quot;CREATION: 2010-12-01 20:39:04&quot; ## $ city &lt;chr&gt; &quot;Washington&quot; ## $ country &lt;chr&gt; &quot;United States of America&quot; ## $ postal_code &lt;chr&gt; &quot;20528&quot; ## $ state &lt;chr&gt; &quot;DC&quot; ## $ phone_number &lt;chr&gt; &quot;+12022817828&quot;   ","version":"Next","tagName":"h3"},{"title":"Google Ad Data​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#google-ad-data","content":" The package is not limited to Meta. While outside the scope of this tutorial, functions exist to retrieve spending data from Google’s Ad Library, allowing for powerful cross-platform comparisons.  For example, the get_ggl_ads function can get different tables that are provided by the Google Ad Transparency report, such as statistics on weekly spending per advertiser:  get_ggl_ads(&quot;weekly_spend&quot;) %&gt;% glimpse()    ## ℹ Downloading data bundle from Google... (This may take a moment) ## ✔ Download complete. Extracting files... ## ℹ Reading data from 'google-political-ads-advertiser-weekly-spend.csv'... ## indexing google-political-ads-advertiser-weekly-spend.csv [] 963.60GB/s, eta: 0s indexing google-political-ads-advertiser-weekly-spend.csv [=] 1.09GB/s, eta: 0s ## ✔ Processing complete. ## Rows: 296,883 ## Columns: 24 ## $ Advertiser_ID &lt;chr&gt; &quot;AR00000475401340059649&quot;, &quot;AR00000475401340059649&quot;, &quot;A… ## $ Advertiser_Name &lt;chr&gt; &quot;Vince Leach for Senate&quot;, &quot;Vince Leach for Senate&quot;, &quot;V… ## $ Election_Cycle &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ Week_Start_Date &lt;date&gt; 2020-08-23, 2020-08-30, 2020-09-06, 2020-09-13, 2020-… ## $ Spend_USD &lt;dbl&gt; 400, 500, 400, 400, 200, 400, 400, 300, 300, 300, 200,… ## $ Spend_EUR &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_INR &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_BGN &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_CZK &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_DKK &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_HUF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_PLN &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_RON &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_SEK &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_GBP &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_NZD &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_ILS &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_AUD &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_TWD &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_BRL &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_ARS &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_ZAR &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_CLP &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Spend_MXN &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …   These functions transform metatargetr from a simple targeting tool into a comprehensive ad intelligence package.  ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Analyzing Ad Targeting Insights: An Introduction to the metatargetr R Package","url":"/docs/data-analysis/04_05_metatargetr#conclusion","content":" The metatargetr package provides a powerful and accessible toolkit for researchers, journalists, and analysts looking to explore the nuances of digital advertising on digital platforms. As this tutorial has shown, by fetching and aggregating historical data, you can quickly move from raw numbers to compelling visualizations that uncover the strategic decisions behind major ad campaigns.  By offering a direct line to targeting data, both live and historical, and equipping users with the tools to analyze it effectively,metatargetr opens new avenues for understanding campaign strategies and their societal impact. Happy researching! ","version":"Next","tagName":"h2"},{"title":"Data Collection on Rumble","type":0,"sectionRef":"#","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble","content":"","keywords":"","version":"Next"},{"title":"A video-hub for fringe discourse​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#a-video-hub-for-fringe-discourse","content":" In recent years, Rumble has emerged as one of the central audiovisual platforms within alternative media ecosystems (Balci et al., 2024). Initially founded in 2013 as a video-sharing site in Canada with a focus on free speech, Rumble surged in popularity beyond its core audience (mainly from the U.S.) around 2020, capitalizing on growing distrust toward mainstream social media platforms like YouTube, Facebook, or X (Shaughnessy et al., 2024).  Today, Rumble functions as a central discourse space for the MAGA (Make America Great Again) community, positioning itself as a champion of &quot;uncensored&quot; content, creating a fertile ground for extremist, conspiracist, and other fringe communities. Users who were deplatformed from larger sites often find a new home on Rumble, enabling the platform to become an essential node in the broader disinformation ecosystem (Balci et al., 2024a; Mell-Taylor, Alex, 2021).  Prominent figures associated with conspiracy theories — ranging from COVID-19 denialism to election fraud narratives — have amassed large followings on Rumble. Content that would be heavily moderated or banned on larger platforms is often allowed to thrive here, posing the challenge for researchers to gain insights into the networks and narratives permeating the online space (Balci et al., 2024b; Thompson, 2024).  ","version":"Next","tagName":"h2"},{"title":"The challenges of website architectures​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#the-challenges-of-website-architectures","content":" Rumble is a good example for the heterogenous landscape of website architectures. Much of the relevant information is loaded dynamically via JavaScript, depending on specific trigger actions on the website, like the user clicking a button or scrolling something into view. This is where traditional solutions like “Beautiful Soup” in the Python world or “rvest” in the R world fall short, as they can’t fetch dynamically generated content.  Selenium is an open-source software framework widely used for automating web browsers. Originally designed for testing web applications, Selenium has become an essential tool for researchers, especially those involved in web scraping and data collection from online platforms.  At its core, Selenium allows a user to programmatically control a browser like Chrome just as a user would: clicking buttons, filling out forms, scrolling pages, and downloading content. This ability makes it invaluable for collecting data from dynamic websites that rely heavily on JavaScript and interactive elements, which traditional scraping methods often struggle to handle.  ","version":"Next","tagName":"h2"},{"title":"Ethical considerations​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#ethical-considerations","content":" Crucially, among all types of data collection, webscraping is the most intricate with legal as well as ethical consideration constantly evolving through legislation such as the EU General Data Protection Regulation (GDPR) or the Digital Services Act (DSA). Responsible scraping practices should always include data minimisation, anonymisation, and a clear purpose aligned with public interest or research. Be sure to always check the platform’s or website’s Terms of Service and look for structured alternatives such as APIs. See also the chapter on the current state of webscraping on the hub.  Note While Selenium offers considerable advantages regarding is adaptability, data collection with it is often resource-heavy with long compute and waiting times. Depending on your project and its demands, consider alternatives like those mentioned above. You can find great tutorials on puppeteer and rvest on the hub. Be sure, however, to check out the programming language they use. If limited to Python, the main alternative to Selenium is Beautiful Soup — a Python library for extracting data from HTML and XML sources.  ","version":"Next","tagName":"h2"},{"title":"What you will learn in this chapter:​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#what-you-will-learn-in-this-chapter","content":" This chapter teaches you how to use Selenium for webscraping, including:  Basic setup of a WebDriver instance;Core functions to find, fetch and interact with web elements;Collection of video information from Rumble.  It will walk through central areas of the video platform – the trending page, search queries and video pages.  Along with this tutorial, a custom Python package was developed to help you collect more complex data from Rumble. More information about its capabilities is provided at the end.  For this tutorial it is best to follow along using a Jupyter Notebook either in your browser or any common programming environments (IDEs) like (Peters, 2022).  ","version":"Next","tagName":"h2"},{"title":"Basic Setup​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#basic-setup","content":" We will first install the selenium package, as well as other packages needed later, via pip.  !pip install selenium requests #The exclamation point is used to signal to your machine that this shell command (“pip install something”) should be run externally   From the selenium package, we will import the WebDriver module and launch a web browser instance of Chrome. With the driver object, you can now control the browser. We use the get() function to navigate to the Rumble homepage.  from selenium import webdriver from selenium.webdriver.chrome.options import Options from selenium.webdriver.chrome.service import Service # Launch the browser (Chrome in this example) driver = webdriver.Chrome() # Navigate to Rumble's homepage driver.get(&quot;https://rumble.com&quot;)   Note While the selenium package should automatically download the necessary browser drivers such as geckodriver for using Firefox or the chromedriver, it is possible that your browser version is not compatible with any of those. If you want to use chrome, check your browser version and visit the chrome-for-testing dashboard to download the corresponding driver for the platform (OS) of your computer. You can the point to the newly downloaded driver file with the Selenium Service module.  CHROME_DRIVER_PATH = &quot;PATH TO YOUR DRIVER FILE&quot; # initialize the Service module and pass the path to the executable (the chromedriver file) custom_driver_path = Service(CHROME_DRIVER_PATH) # create a new instance of the Chrome driver with the specified path driver = webdriver.Chrome(service=custom_driver_path) # Navigate to Rumble's homepage driver.get(&quot;https://rumble.com&quot;)     Screenshot of Rumble’s landing page with automation disclaimer at the top  Hint At the top of the browser window, it reads “Chrome wird von automatisierter Testsoftware gesteuert” – This disclaimer signifies that your browser is controlled by Selenium. At the same time, this kind of information is also passed to the website servers, whenever you make a request. Some pages explicitly prohibit the usage of automated browser clients to visit their pages and have put in place different blockers and CAPTCHA forms. Reacting to this, more advanced instances of the Selenium WebDriver have been developed. For an introduction to the “undetected chromedriver” click here.  ","version":"Next","tagName":"h2"},{"title":"Interacting with elements on a webpage​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#interacting-with-elements-on-a-webpage","content":" Before heading to the breadth of extremist content on Rumble, the following ad appears at the bottom of the page. While we could simply close it manually, this presents a great opportunity to learn the core concept of data collection with Selenium – It’s all about the elements.    Screenshot of Rumble’s ad popup page and the associated DOM element  ","version":"Next","tagName":"h2"},{"title":"Understanding Web Elements and the DOM​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#understanding-web-elements-and-the-dom","content":" When a web browser loads a web page, it reads the HTML (HyperText Markup Language) and constructs a representation of the page in memory called the Document Object Model (DOM). The DOM is essentially a tree-like structure where each piece of HTML —such as &lt;div&gt;,&lt;a&gt;, &lt;p&gt;, or &lt;img&gt;— is represented as an object or node in that tree. Selenium lets us find data on the page by explicitly pointing to a web element. More information about how to use the DOM and DevTools can be found here.  Instead of manually clicking the “X” button, we find the corresponding element in the DOM, so we can point to it and let the WebDriver close it for us. We will use the find_element() function. Every web element has certain properties and attributes. Similarly, there are multiple viable ways to point to an element. Among them, css selectors and XPath are the most prominent ones, striking a good balance between ease of use and explicitness. In this tutorial, we will focus on an element’s XPath to find and interact with it.  Note If you want to explore the usage of CSS selectors for Selenium, you can find great resources here.  XPath uses path-like syntax. Upon inspecting the button “X” element, we find its node name to be “button”. However, for this ad pop up alone, there are three button nodes. So, we look for more unique attributes, like classes, id’s or aria-labels. Much like a file path on your computer, the longer the path, the more specific the kind of object it is pointing to. In this case, the element has an associated aria-label value “Close” we can use to find the element, store it in an object and use the click() function to trigger a mouse click action.  from selenium.webdriver.common.by import By #Most common XPath syntax: ‚//&lt;nodename&gt;[@&lt;attribute&gt;=“value“]‘ # Find the &quot;X&quot; button and store it as a webdriver element close_button = driver.find_element(By.XPATH, '//button[@aria-label=&quot;Close&quot;]') print(close_button)   This output shows how Selenium stores a web element:  &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;cb1235bf141111e2957a342a38a68670&quot;, element=&quot;f.CF46E617C588DFF5C8E2BDB69B5DF780.d.6FFCA08F353769BE84D96D5E08180AC4.e.23623&quot;)&gt;   Now we can click this button and see the pop-up being closed.  # Click the &quot;X&quot; button to close the pop-up close_button.click()   ","version":"Next","tagName":"h2"},{"title":"Rumble's trending page​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#rumbles-trending-page","content":" Rumble’s trending page is a good seismograph of the latest MAGA and extremist discourses in the US. After inspecting the menu bar on the left, you can see that the different sections are not represented as button elements but rather as hyperlinks with which we can navigate to the respective subdomain. Still, we could find and store the element through the href attribute and click it, like any user would do on the website. Alternatively, you can navigate to the subdomain directly via the get() function.  Note There is no general rule as to either click on hyperlink elements or load the page directly. However, the get() function will load the whole page, whereas sometimes click actions on dynamic webpages will only reload a certain part of that page. This is important for large-scale data collection with time constrains but can be neglected within the scope of this tutorial.    Screenshot of Rumble’s menu bar with &lt;a&gt; elements displaying the subdomains  # Find the &quot;Trending&quot; link and store it as a webdriver element, then click it trending_page = driver.find_element(By.XPATH, '//a[@href=&quot;/videos?sort=views&amp;date=today&quot;]') trending_page.click() #Alternatively, you can use the subdomain directly to navigate to it driver.get(&quot;https://rumble.com/videos?sort=views&amp;date=today&quot;)   By default, the trending page lists the most viewed videos of the day. On the right side, there are many different filter options to choose from. Instead of today, we want to inspect the most viewed videos of the past week. Again, we could find the filter option as an element and click it, or we slightly change our URL to directly navigate to that filtered page.    Screenshot of Rumble’s trending page with filter options  Looking at the video elements in the DOM, we can see clearly structured &lt;li&gt; items with the class “video-listing-entry”, which hold different child elements and attributes. On Rumble, there is no infinite scroll, as the content is separated into different pages. Each page contains 25 video items.    Screenshot of a video item with its associated DOM element  We can identify and store these video items by utilising the find_elements() function. While there are already valuable data points visible from the listing view alone, we don’t have access yet to the video description, its caption and the comment section. Therefore, we will only fetch the ‘href’ of each video and later visit each video page retrieve all data in detail.  Doing so, we write a small function that not only finds the elements but also extracts their values for the ‘href’ attribute. This can be easily done with the get_attribute() function.  def collect_video_links(driver): #Define an empty list to store the video links collected_links = [] #Find all video elements that contain the video links links = driver.find_elements(By.XPATH, &quot;//a[@class = 'video-item--a']&quot;) #For each link, get the href attribute and add it to the list for link in links: href = link.get_attribute(&quot;href&quot;) if href not in collected_links: collected_links.append(href) return collected_links video_links = collect_video_links(driver) print(video_links[:3])   ['https://rumble.com/v6ri2ut--01-04-2025-makeleio.gr.html', 'https://rumble.com/v6rjszf--why-the-medias-bombshell-deportation-story-is-one-big-lie.html', 'https://rumble.com/v6ro0xt-stock-market-bloodbath-after-china-places-34-tariff-on-us-trump-holds-firm-.html']```   If we want to retrieve the video items for multiple pages, we can do so by utilising keyboard actions to simulate user behaviour like scrolling. At the bottom of the trending page, there are page elements we can scroll to and click. Ultimately, we want to create a loop that goes through the pages and stores the video links. There are again multiple viable ways to do this, either specifying the number of pages we want to retrieve data from or the number of videos we want to retrieve. In this case, we loop over the pages until we reach the number of videos links that have been defined. We want to fetch the 50 most viewed videos of the last week.  In prior versions of the Selenium python package, many user actions like scrolling were performed by letting the WebDriver execute some lines of JavaScript code. Now, most common actions are nicely wrapped and easily executable through Python directly. Here, we need the “ActionsChains” classes and utilise the scroll_to_element().  from selenium.webdriver.common.action_chains import ActionChains def collect_multiple_pages(driver,limit:int=50): video_links = [] # Iterate over as many pages as needed to fetch the desired number of video links while len(video_links) &lt; limit: # Scroll to the &quot;Next&quot; button to ensure it's in view button = driver.find_element(By.XPATH,&quot;//li[@class='paginator--li paginator--li--next']&quot;) ActionChains(driver)\\ .scroll_to_element(button)\\ .perform() # Use the collect_video_links function to get video links from the current page page_video_links = collect_video_links(driver) # Add the new video links to the list video_links.extend(page_video_links) button.click() return video_links video_items = collect_multiple_pages(driver,limit=50)   ","version":"Next","tagName":"h2"},{"title":"Automate search queries​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#automate-search-queries","content":" Before turning to the detailed video data, we introduce one last core method for interacting with web pages via Selenium – sending keys or input. Whenever we have an input element like a search bar or a form to fill out, we can send user input automatically and thus automate a variety of processes. In this case, we want to utilise the search bar, insert a search query, and simulate a key press “ENTER” to automate the search on Rumble.    Screenshot of Rumble’s search bar with its associated DOM element  from selenium.webdriver.common.keys import Keys def search_query(driver,query:str): # Find the search input by its attribute search_bar = driver.find_element(By.XPATH, '//input[@type=&quot;search&quot;]') # Send your query search_bar.send_keys(query) # press Enter to submit the search search_bar.send_keys(Keys.ENTER) # Our search query is &quot;trump tarrifs&quot; search_query(driver,&quot;trump tarrifs&quot;)   By default, the resulting page will show the most relevant results based on your query. The same filter options we have introduced on the trending page apply here. You can now use the collect_multiple_pages() function to iterate over the result pages and collect the video links.  ","version":"Next","tagName":"h2"},{"title":"Collect video information​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#collect-video-information","content":" After we have collected video links from the trending page or through our search query, we can now inspect a single video page for relevant data points.    _Screenshot of a single video page with relevant metrics and data points _  For every video, Rumble provides a breadth of information we can collect. However, unlike datapoints such as the title or the author channel, much of the information is presented in a user-friendly way unfit for analyses. For instance, the view count is not a classic integer but abbreviated with a capital K to signify 680,000 views – so are the likes and dislikes for that video. Even after inspection of the DOM, one realises that we need to convert this data to make it usable for analysis.  We can create a function that fetches the view count and – depending on the letter – converts it to its associated integer value.   def collect_view_count(driver): #Find the view count element using its XPath view_count = driver.find_element(By.XPATH, '//div[@class=&quot;media-description-info-views&quot;]').get_attribute('outerText') # If the value is above 1,000 it will be converted to &quot;1K&quot;. So we can identify the k in the string. if &quot;K&quot; in view_count: converted_video_count = int(float(view_count.split('K')[0]) * 1000) # If the value is above 1,000,000 it will be converted to &quot;1M&quot;. So we can identify the M in the string. elif &quot;M&quot; in view_count: converted_video_count = int(float(view_count.split('M')[0]) * 1000000) # Any value below that is displayed as a classic integer and can be converted directly. else: converted_video_count = int(view_count) return converted_video_count single_video_view_count = collect_view_count(driver) print(single_video_view_count)   680000   Even after retrieving all visible data from the video page, there is limited insight into the video’s content, especially if it’s an hour-long stream. Luckily, Rumble provides captions we can fetch and store as text for content analyses. We will create a function that finds the element and makes a request with the help of the ‘requests’ Python package. It then encodes the response as text if successful or prints out the error message.  import requests def retrieve_captions(driver): # Find the captions track element and store the src attribute src_path = driver.find_element(By.XPATH, '//track[@kind=&quot;captions&quot;]').get_attribute('src') # Utilize the requests library to fetch the captions file caption_response = requests.get(src_path) # Check if the request was successful (status code 200) if caption_response.status_code == 200: return caption_response.text else: print(f&quot;Error: {caption_response.status_code}&quot;) return None captions = retrieve_captions(driver) print(captions[:90])   WEBVTT 00:00:47.340 --&gt; 00:00:49.340 Welcome, you're listening to the X-22 Report.   With the core methods shown in this tutorial, you are now able to navigate to Rumble or any other website (static or dynamic) with an automated browser and retrieve or interact with its web elements. Beyond this, you can expand the code to fetch more data points from the video page and automatically navigate through the different domains of interest on Rumble. Once your code is ready, you can switch to Python scripts and run them daily for monitoring purposes.  ","version":"Next","tagName":"h2"},{"title":"Advanced usage​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#advanced-usage","content":" The platform’s web page structure is complex, with many metrics and elements posing challenges for data collection. For instance, metrics like the date format depend on the content being a video or a live stream. The display of the description text depends on its length. Rumble channels can also limit the availability of their video’s comment section to logged-in users.  Therefore, we at polisphere have created an open-source Python package called “rumble-scraper” to help you with Rumble’s complexity and data collection obstacles. It includes the capabilities to  Collect videos with all filter options from the trending and browse pageCollect all visible data points from video pages, including video description and the comment sectionLog-in with user credentials  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Data Collection on Rumble","url":"/docs/data-collection/03_00_platform-specific guidelines/03_01_data-collection_rumble#references","content":" Balci, U., Patel, J., Balci, B., &amp; Blackburn, J. (2024a). iDRAMA-rumble-2024: A Dataset of Podcasts from Rumble Spanning 2020 to 2022. Workshop Proceedings of the 18th International AAAI Conference on Web and Social Media. Balci, U., Patel, J., Balci, B., &amp; Blackburn, J. (2024b). Podcast Outcasts: Understanding Rumble’s Podcast Dynamics. arXiv Preprint arXiv:2406.14460. Mell-Taylor, Alex. (2021). Rumble Is Still Where the Right Goes to Play. Medium. Retrieved April 09, 2025, from https://aninjusticemag.com/rumble-is-still-where-the-right-goes-to-play-d3fe7df98875. Shaughnessy, B., DuBosar, E., Hutchens, M. J., &amp; Mann, I. (2024). An attack on free speech? Examining content moderation, (de-), and (re-) platforming on American right-wing alternative social media. New Media &amp; Society, 14614448241228850. https://doi.org/10.1177/14614448241228850. Thompson, S. (2024). I Traded My News Apps for Rumble, the Right-Wing YouTube. Here’s What I Saw. The New York Times. Retrieved April 09, 2025, from https://www.nytimes.com/interactive/2024/12/13/business/rumble-trump-bongino-kirk.html. ","version":"Next","tagName":"h2"},{"title":"How to stream, store and retrieve data on X (Twitter)","type":0,"sectionRef":"#","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/","content":"How to stream, store and retrieve data on X (Twitter) Level: Beginner Platform: X Language: Python AN Andreas Neumeier SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky JR Jasmin Riedl SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky WD Wiebke Drews SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn Bluesky Original post on 13.09.2024 by Neumeier et al. Social media platforms have gained significant importance in recent years due to their extensive reach and high usage. They provide researchers in various fields with valuable insights into public, social, and political areas, making them an essential data source. When it comes to X (formerly Twitter) and its data gathering, specific features exist, which we discuss in the following documents. The provided documents outline the management of X (Twitter) data and guide users through data streaming, storage, and retrieval. The streaming-database document recommends methods for working with X (Twitter) streaming data and covers storage in both relational and non-relational databases. The twitter-api page introduces the Python implementation of Twitter API v2 through the sparta-twitterapi package. The guide outlines the prerequisites for X (Twitter), including required software, tools, and developer credentials, and provides clear instructions on installing the Python package and completing initial authentication. Additionally, the document describes the available endpoints through the Python package. The twitter-rules guide demonstrates how to utilize X’s API tools for searching and filtering. It assists users in conducting efficient searches and explains the operators and their functions.","keywords":"","version":"Next"},{"title":"Example: X (Twitter) API","type":0,"sectionRef":"#","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api","content":"","keywords":"","version":"Next"},{"title":"How to access data via the Twitter API v2​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#how-to-access-data-via-the-twitter-api-v2","content":" Developers can access a wide range of X (Twitter) data, including Tweets, users, and more features with the Twitter API v2. The API provides vast opportunities, whether to gather insights, build a new application, or improve an existing one. Version 2 of the Twitter API offers a more flexible and scalable approach to accessing Twitter data, and the Python implementation aims to simplify this process for Python developers. Documentation for the Python package is available at https://unibwsparta.github.io/twitterapi/index.html.  ","version":"Next","tagName":"h2"},{"title":"1. Prerequisites​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#1-prerequisites","content":" ","version":"Next","tagName":"h2"},{"title":"1.1. Software & Tools​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#11-software--tools","content":" Python (3.8 or higher)Pip (Python package installer)  ","version":"Next","tagName":"h3"},{"title":"1.2 Twitter Credentials​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#12-twitter-credentials","content":" To use the Twitter API, you must have a developer account on Twitter, where you'll obtain the Bearer Token.  ","version":"Next","tagName":"h3"},{"title":"2. Installation​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#2-installation","content":" Install the required Python package using pip:  pip3 install sparta-twitterapi   ","version":"Next","tagName":"h2"},{"title":"3. Basic Usage​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#3-basic-usage","content":" ","version":"Next","tagName":"h2"},{"title":"3.1 Authentication​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#31-authentication","content":" Authentication is required before making any requests to the API. The library reads the Bearer Token from an environment variable named BEARER_TOKEN. You can set the environment variable directly or through the os library in the Python code before the imports.  Set the environment variable in bash:  export BEARER_TOKEN=&quot;xxxxxxxxxxx&quot;   Set the environment variable in python:  import os os.environ[&quot;BEARER_TOKEN&quot;] = &quot;xxxxxxxxxxx&quot;   ","version":"Next","tagName":"h3"},{"title":"3.2 First example​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#32-first-example","content":" This script retrieves two tweets defined by the tweet ID and outputs their content on the console.  import os os.environ[&quot;BEARER_TOKEN&quot;] = &quot;XXXXXXXXXXXXXX&quot; from sparta.twitterapi.tweets.tweets import get_tweets_by_id async for tweet_response in get_tweets_by_id(['1511275800758300675', '1546866845180887040']): print(tweet_response.tweet)   ","version":"Next","tagName":"h3"},{"title":"4. Available Endpoints​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#4-available-endpoints","content":" The sparta-twitterapi package provides several endpoints organized into three core categories: These consist of Compliance, Users, and Tweets. It also offers a data model for obtaining tweets that is resistant to API changes. Here’s a comprehensive overview. For further details, refer to the official documentation.  ","version":"Next","tagName":"h2"},{"title":"4.1 Data Model​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#41-data-model","content":" To account for differences between X’s (Twitter’s) API description and its actual responses, we devised a Pydantic model to represent tweet responses as the original classes provided by X (Twitter) couldn’t be used. Every tweet includes two attributes. tweet_response.tweet comprises the original tweet object, including details about the tweet, such as its id, creation date, text, entities, and more. The extension objects linked with the tweet, such as retweeted tweets, quoted tweets, users, media, polls, and places, are included in tweets_response.includes. Find all tweet attributes and their corresponding descriptions here.  ","version":"Next","tagName":"h3"},{"title":"4.2 Tweets Endpoints​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#42-tweets-endpoints","content":" The twitterapi package provides various endpoints in the &quot;Tweets&quot; category that serve different functions and requirements associated with Twitter posts. Developers can utilize these endpoints related to Tweets to obtain comprehensive tools for harnessing, analyzing, and managing the vast amount of information flowing through Twitter every moment.  In the endpoints provided by the Python library, all available fields and extensions are retrieved by default. For comprehensive details, parameter explanations, and best practices, you should refer to the official sparta-twitterapi package documentation. Here is a thorough outline of the endpoints that are available in this category:  Filtered Stream Endpoint​  The filtered stream endpoint group is designed for developers who wish to modify the public tweets' real-time stream based on specific criteria. This endpoint group allows users to follow real-time conversations on specific topics or events and observe trend development. This group includes several endpoints allowing:  Creating and managing rules. Apply a set of rules to filter real-time tweets and return only the tweets that fulfill the criteria.Connect to the filtered stream.  Full-archive Search Endpoint​  The full-archive search endpoint is available exclusively to projects with Academic Research or Enterprise access. It provides the capability to:  Access all public Tweets from the archive, starting with the very first Tweet in March 2006.Retrieve them based on specific search queries.  Quote Tweets Lookup Endpoint​  Retrieving Quote Tweets that are linked to a specific Tweet ID. This allows developers and researchers to quickly extract the entire conversation related to a certain Tweet, along with all its Quote Tweets, which promotes a thorough comprehension of the discussions.  Recent Search Endpoint​  The recent search endpoint provides access to:  Filtered public tweets posted in the last week.  Retweeted By Endpoint​  Users can utilize the Retweets lookup endpoint to:  Obtain a list of accounts that retweeted a specific Tweet.  Tweets Endpoint​  This set of endpoints is designed to be simple, offering methods to:  Retrieve either a single Tweet or a group of Tweets identified by a Tweet ID. Despite the simplicity of this endpoint, it's essential for various tasks, such as retrieving updated Tweet details, checking a Tweet's availability, reviewing its edit history, and managing compliance events.  ","version":"Next","tagName":"h3"},{"title":"4.3 Users Endpoints​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#43-users-endpoints","content":" The twitterapi package provides various endpoints under the 'Users' category. These endpoints are designed to offer insights and data about Twitter users. Below is a detailed breakdown of the available endpoints in this category:  Followers Lookup Endpoint​  The Followers Lookup Endpoint is designed to unravel and analyze the relationships between Twitter users. This process is commonly referred to as network analysis. There are two primary endpoints in this category:  get_following_by_id: This endpoint returns user objects that represent the users that a specific user is following. It provides a list of users whom the queried user has chosen to follow. get_followers_by_id: On the other hand, this endpoint returns user objects that represent the users who follow the specified user. It gives a list of users who follow the queried user.  Both of these endpoints provide valuable insights into the connection dynamics of specific accounts by shedding light on user relationships.  User Lookup Endpoint​  The User Lookup Endpoint is a service that retrieves details of one or more users based on either a user ID (get_users_by_ids) or username (get_users_by_username). This endpoint uses the GET method to return the following:  User Details: The response typically includes user objects that contain fields such as follower count, location, pinned Tweet ID, and profile bio, among others. These fields provide a comprehensive view of the user's public profile and preferences. Find all user attributes and their corresponding descriptions here.  Developers, who seek to gain insights into user behavior, relationships, and characteristics on Twitter, will find these Users endpoints particularly useful. Consult the official documentation of the twitterapi package to gain a more detailed comprehension and explore other preferences and criteria.  ","version":"Next","tagName":"h3"},{"title":"4.4 Compliance Endpoints​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-api#44-compliance-endpoints","content":" For ensuring compliance with events, developers, who store X (Twitter) data offline, must reflect the real-time state and user intent on X (Twitter). This involves updating the data whenever users delete or edit their tweets, protect their accounts, or make other changes. There are two methods to perform this task:  Compliance Event Stream​  The Compliance Event Stream provides real-time updates on any compliance-related events, ensuring that any stored data is kept in sync with the live state on X (Twitter).  Batch Compliance Mode​  Batch compliance endpoints are valuable for developers managing extensive datasets. By uploading sizable collections of Tweet or user IDs, you can quickly determine the compliance status of each entry. Identifying data that requires action ensures the compliance of your datasets. It is important to note that the batch compliance endpoints are specifically designed for their intended purposes. Any use of these endpoints outside of their intended purposes is strictly prohibited and could result in enforcement actions taken by X (Twitter). ","version":"Next","tagName":"h3"},{"title":"Best Practices to Stream and Store Data on X","type":0,"sectionRef":"#","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/streaming-database","content":"","keywords":"","version":"Next"},{"title":"How to Deal with Varying Amounts of X Streaming Data​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/streaming-database#how-to-deal-with-varying-amounts-of-x-streaming-data","content":" Expect the number of tweets coming through the filtered stream to vary based on factors like time of day or current events. This fluctuation means that processing capabilities must be adaptable to handle increases or decreases in volume. Additionally, it’s important to note that X’s (Twitter’s) filtered stream endpoint does not include a built-in buffer. Therefore, if incoming data is not processed promptly, the stream may stop working.  If you experience a disconnection, you may retrieve data up to five minutes after the interruption. However, it is recommended to minimize interruptions to maintain a consistent and efficient data flow. One effective approach is establishing a local buffer to temporarily store incoming tweets for later processing.  Moreover, incorporating a publish-subscribe system like Kafka is worth considering. With this system in place, incoming tweets are stored instantly, allowing subsequent processes to access and analyze them at their own pace. This approach effectively manages peak loads and balances the data processing workload.  Always monitor your analysis pipeline’s capacity. If you notice that it is struggling to handle the regular tweet volume, it is time to consider scaling up your resources. This step ensures that your system stays reliable and responsive, especially during times of high demand.  ","version":"Next","tagName":"h2"},{"title":"How to Store your X Data​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/streaming-database#how-to-store-your-x-data","content":" To ensure that tweets can be processed later, they need to be stored in a structured format.  The method in which the data is stored varies depending on the specific application. It is feasible to store them in both relational and non-relational databases.  For effective storage and retrieval of tweets, selecting the appropriate database structure is crucial. Your chosen structure depends heavily on your application’s nature and how you intend to interact with stored data. Here’s a quick overview of how you can store and/or organise tweets in both relational and non-relational databases:  ","version":"Next","tagName":"h2"},{"title":"1. Relational Databases​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/streaming-database#1-relational-databases","content":" In a relational database (e.g., PostgreSQL, MySQL), information is arranged into tables with predetermined columns, and connections between these tables are clearly defined through primary and foreign keys.  Tip Remember that the decision to use a relational database such as PostgreSQL should depend on your application’s requirements. If your application relies heavily on structured queries and data relationships, a relational database like PostgreSQL can be very beneficial.  PostgreSQL-Schema example for tweets:  id (text)​  Description: A unique identifier for each tweet.Constraints: Must be unique for each tweet.Example: &quot;1234567890&quot;  project (text)​  Description: The name of a project. This allows you to store tweets from multiple projects in the same table.Example: &quot;project1&quot;  created_at (datetime)​  Description: The date and time when the tweet was published.Constraints: Must be a valid date and time.Example: 2023-08-24 12:34:56+00  author_id (text)​  Description: The unique identifier of the user who authored the tweet.Example: &quot;9876543210&quot;  text (text)​  Description: The actual content of the tweet, limited to a specific number of characters.Example: &quot;This is a sample tweet!&quot;  retweeted_id (text)​  Description: If the tweet is a retweet, this field stores the original tweet's unique identifier.Constraints: Should map to a valid tweet id or be NULL if it's not a retweet.Example: &quot;987654321&quot;  conversation_id (text)​  Description: An identifier to group tweets from the same conversation/thread.Constraints: Can be same as the tweet's id if it's the start of a conversation or can map to another tweet's id if it's part of an ongoing conversation.Example: &quot;1122334455&quot;  media (jsonb)​  Description: A JSON object containing information about any media (like images, videos) attached to the tweet.Example: [ { &quot;url&quot;: &quot;https://pbs.twimg.com/media/test.jpg&quot;, &quot;type&quot;: &quot;photo&quot;, &quot;width&quot;: 4086, &quot;height&quot;: 3065, &quot;media_key&quot;: &quot;3_123456&quot;, &quot;public_metrics&quot;: {} } ]   polls (jsonb)​  Description: A JSON object containing data about any polls included in the tweet.Example: [ { &quot;id&quot;: &quot;123456789&quot;, &quot;question&quot;: &quot;Favorite color?&quot;, &quot;options&quot;: [ { &quot;label&quot;: &quot;Red&quot;, &quot;votes&quot;: 18, &quot;position&quot;: 1 }, { &quot;label&quot;: &quot;Blue&quot;, &quot;votes&quot;: 28, &quot;position&quot;: 2 }, { &quot;label&quot;: &quot;Green&quot;, &quot;votes&quot;: 18, &quot;position&quot;: 3 } ], &quot;end_datetime&quot;: &quot;2023-08-24 12:34:56.000Z&quot;, &quot;voting_status&quot;: &quot;closed&quot;, &quot;duration_minutes&quot;: 10080 } ]   places (jsonb)​  Description: A JSON object containing geolocation or place information associated with the tweet.Constraints: Should adhere to a pre-defined structure for storing place data.Example: [ { &quot;id&quot;: &quot;37439688c6302728&quot;, &quot;geo&quot;: { &quot;bbox&quot;: [11.360589, 48.061634, 11.722918, 48.248124], &quot;type&quot;: &quot;Feature&quot;, &quot;properties&quot;: {} }, &quot;name&quot;: &quot;Munich&quot;, &quot;country&quot;: &quot;Germany&quot;, &quot;full_name&quot;: &quot;Munich, Germany&quot;, &quot;place_type&quot;: &quot;city&quot;, &quot;country_code&quot;: &quot;DE&quot; } ]   entities (jsonb)​  Description: A JSON object that captures specific entities within the tweet like hashtags, user mentions, URLs, etc.Constraints: Should adhere to a pre-defined structure for capturing these entities.Example: { &quot;hashtags&quot;: [ { &quot;end&quot;: 15, &quot;start&quot;: 0, &quot;tag&quot;: &quot;#example&quot; }, { &quot;end&quot;: 30, &quot;start&quot;: 15, &quot;tag&quot;: &quot;#database&quot; } ], &quot;mentions&quot;: [ { &quot;id&quot;: &quot;123456&quot;, &quot;end&quot;: 15, &quot;start&quot;: 0, &quot;username&quot;: &quot;@username&quot; } ] }   tweet (jsonb)​  Description: A JSON object that captures the entire raw tweet data, useful in case something changes in the Twitter API, for backup, or for applications that need the full payload.Example: {&quot;id&quot;: &quot;1234567890&quot;, &quot;text&quot;: &quot;This is a sample tweet!&quot;, &quot;lang&quot;: &quot;de&quot;, &quot;public_metrics&quot;: {&quot;like_count&quot;: 0, &quot;quote_count&quot;: 0, &quot;reply_count&quot;: 0, &quot;retweet_count&quot;: 0, &quot;impression_count&quot;: 0}, ...}   compliance (text)​  Description: Displays the compliance status of the tweet in terms of compliance events such as deleted, edited, witheld. This can be used to indicate whether a compliance event exists for a tweet.Example: deleted  ","version":"Next","tagName":"h3"},{"title":"2. Non-Relational Databases​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/streaming-database#2-non-relational-databases","content":" Non-relational databases (e.g., MongoDB, Cassandra) offer greater storage flexibility since they don’t need a fixed schema.  Document-based storage (like MongoDB) example:  Each tweet can be a document:  { &quot;_id&quot;: &quot;1234567890&quot;, &quot;author_id&quot;: &quot;9876543210&quot;, &quot;text&quot;: &quot;This is a sample tweet!&quot;, &quot;created_at&quot;: &quot;2023-08-24 12:34:56+00&quot;, &quot;entities&quot;: { &quot;hashtags&quot;: [ {&quot;end&quot;: 15, &quot;start&quot;: 0, &quot;tag&quot;: &quot;#example&quot;}, {&quot;end&quot;: 30, &quot;start&quot;: 15, &quot;tag&quot;: &quot;#database&quot;} ], &quot;mentions&quot;: [ {&quot;id&quot;: &quot;123456&quot;, &quot;end&quot;: 15, &quot;start&quot;: 0, &quot;username&quot;: &quot;@username&quot;} ] }, ... }   Non-relational databases excel when:  The data structure is flexible and might change over time.Horizontal scalability is required.High write speeds are necessary, and data consistency can be slightly relaxed.  ","version":"Next","tagName":"h3"},{"title":"In Short: How to choose your database​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/streaming-database#in-short-how-to-choose-your-database","content":" The choice between relational and non-relational databases should be made based on your application’s specific requirements. If the application heavily relies on structured relationships, complex queries, and data integrity, a relational database is preferable. On the other hand, if your application needs scalability, flexibility in schema design, and can compromise a bit on consistency, a non-relational database might be the better choice. ","version":"Next","tagName":"h2"},{"title":"Data Collection on YouTube","type":0,"sectionRef":"#","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube","content":"","keywords":"","version":"Next"},{"title":"Disinformation on video platforms​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#disinformation-on-video-platforms","content":" With the visual turn on social media and the growing importance of audio-visual platforms as information spaces, researchers have long acknowledged YouTube’s central role as a conduit of disinformation, conspiracy and extremist discourse (Allgaier, 2019; Knüpfer et al., 2023).  In the rapidly developing nexus of disinformation and Artificial Intelligence (AI), YouTube already hosts a variety of manipulative synthetic content, exemplified by recent discoveries of Spanish-speaking, anti-European content disseminated massively by disinformation networks (Maldida.es, 2025).  This underlines the need for researchers to closely monitor activities on the platform and collect large-scale data for analyses. Fortunately, as YouTube has been the dominant video platform for over a decade now and has long provided access to many features through different APIs, there are lots of brilliant resources out there to help collecting different types of data from YouTube (Richardson &amp; Flannery O’Connor, 2023).  ","version":"Next","tagName":"h2"},{"title":"What you will learn in this chapter​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#what-you-will-learn-in-this-chapter","content":" This tutorial focuses on three main data collection methods, equipping you to monitor:  the prevalence of topic specific videos through search queries;Comment sections under specific videos;YouTube channels, their statistics and video output.  ","version":"Next","tagName":"h3"},{"title":"Authentication​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#authentication","content":" Before getting started, make sure to have all necessary authentication requirements. Obtaining an API key or OAuth 2.0 token is the central requirement to making any valid request. However, in contrast to other platforms, there are no huge obstacles to gain access (like vetting processes for researchers). All you need is a Google account with permission to create projects on the Google Cloud Console. Step-by-step guides to get an API key are provided in written form here and in video form here. For all the data collection performed in this tutorial, the OAuth 2.0 token is not required.  With a key in hand, it is best to follow along using a Jupyter Notebook either in your browser or any common programming environments (IDEs) like Visual Studio Code.  The tutorial guides you through as follows:  Basic setupConstruction of a single queryData collection methods: SearchVideo informationCommentsChannels  Note If you want to expand your data collection beyond what is shown here, you can find the extensive documentation for this API provided by Google here. There is a quota for the API, with standard projects being limited to 10,000 request units per day. Google provides an overview of quota unit calculation here.  ","version":"Next","tagName":"h3"},{"title":"Basic Setup​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#basic-setup","content":" In our python environment, we will use the following packages, all easily installable via pip (PyPI). First, install them using the command below:  pip install jsonlines tqdm pandas google-api-python-client #The exclamation point is used to signal to your machine that this shell command (“pip install something”) should be run externally   Then, import them into your environment:  import jsonlines import json import pandas as pd from datetime import datetime from tqdm import tqdm import os import googleapiclient.discovery from googleapiclient.discovery import build import googleapiclient.errors   Here, you insert the necessary information for the authentication:   # You can disable OAuthlib's HTTPS verification when running locally. # Please *DO NOT* leave this option enabled in production. os.environ[&quot;OAUTHLIB_INSECURE_TRANSPORT&quot;] = &quot;1&quot; API_key = &quot;YOUR_API_KEY_HERE&quot; ## Replace this string with your actual API key API_service_name = &quot;youtube&quot; ## Specify what Google API you want to use API_version = &quot;v3&quot; ## Specify the version   Note Storing credentials like API keys or other sensitive information in plain sight is fine when running your scripts locally. However, a more secure approach is to set them up as environment variables outside your script or to use a configuration file. An easy-to-follow tutorial on the different ways to store sensitive information securely can be accessed here.  Now you can construct your API client. This object youtube is how you interact with and make calls to retrieve YouTube data.  youtube = build(API_service_name, API_version, developerKey=API_key)   ","version":"Next","tagName":"h2"},{"title":"Construction of a single query​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#construction-of-a-single-query","content":" In essence, all YouTube data is categorized into different resources (channels, videos, playlists, thumbnails, etc.). Each resource affords different methods to retrieve data of interest but, luckily, the query structure is largely the same.  Let us look at a single query example. We want to query the YouTube Data API for the top 50 most viewed videos related to election integrity in the weeks leading up to the US election. We use our client object to access the search() resource. The list() function allows us to retrieve a collection of results that match the query – this can be videos but also channels or playlists. Inside the list() function, we specify some necessary and optional parameters.  ## Initial API request request = youtube.search().list( part=&quot;snippet&quot;, #neccessary parameter, where snippet contains more detailed information maxResults=50, #default value is 5, max value is 50 publishedAfter=&quot;2024-10-01T00:00:00Z&quot;, publishedBefore=&quot;2024-11-06T00:00:00Z&quot;, #timeframe of interest order=&quot;viewCount&quot;, #alternative would be by “date” in reverse chronological order q=&quot;election fraud | stolen election | election lie&quot;, #Use this “|” OR separator or the NOT (-) operator to further specify your keywords of interest relevanceLanguage=&quot;en&quot;, #returns videos most relevant to the specified language type=&quot;video&quot; #we only want videos as results ) response = request.execute() #Use this command to execute our API call   If the request was successful, the response contains a dictionary with the following keys:  response.keys()   dict_keys(['kind','etag','nextPageToken','regionCode','pageInfo','items'])   The video results are stored inside items, you can see the exemplary information we retrieved for the first video here:  response[&quot;items&quot;][0]    {'kind': 'youtube#searchResult', 'etag': '2PoKoFaYrW3QLMB7vec-RZEr_rM', 'id': {'kind': 'youtube#video', 'videoId': 'IPUhRjAMCTo'}, #videoId is important for later! 'snippet': {'publishedAt': '2024-10-08T03:00:17Z', 'channelId': 'UCwWhs_6x42TyRM4Wstoq8HA', 'title': 'Jon Stewart on Elon Musk, Free Speech &amp; Trump's Election Interference Claims | The Daily Show', 'description': 'With less than a month until Election Day, Jon Stewart unpacks how Trump and his newest &quot;dark MAGA&quot; henchman, Elon Musk, ...', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/IPUhRjAMCTo/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/IPUhRjAMCTo/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/IPUhRjAMCTo/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'The Daily Show', 'liveBroadcastContent': 'none', 'publishTime': '2024-10-08T03:00:17Z'}}   ","version":"Next","tagName":"h2"},{"title":"Data collection methods: Search​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#data-collection-methods-search","content":" While we have already successfully run a YouTube search query, in any research effort, 50 results are hardly enough to obtain meaningful insights. You can retrieve more data through the nextPageToken. This is insofar important, as most APIs rely on pagination to control the amount of data access to their servers.  If there are more results to your query, the response will include a nextPageToken, which you can include in your next query to get the 50 next results – a process we can iterate as long as we want to. Let us generalize our previous code to collect the N first pages of results for our query:  ## &gt; Loop to retrieve videos related to search query from multiple pages &lt; N = 2 # We set N to 2 to define that we want the top 2 pages of results. # The “nextPageToken” variable will be used in the for loop to store the ID of the next page. next_page_token = None search_results = list() # An empty list to store the query results for i in tqdm(range(N)): # The &quot;tqdm&quot; wrapper around the &quot;ids_list&quot; variable allows us to see a progress bar # Retrieve a page of results if next_page_token is None: # i.e. if this is the request for the first page, we do not use it as a parameter request = youtube.search().list( part=&quot;snippet&quot;, maxResults=50, publishedAfter=&quot;2024-10-01T00:00:00Z&quot;, publishedBefore=&quot;2024-11-06T00:00:00Z&quot;, order=&quot;viewCount&quot;, q=&quot;election fraud | stolen election | election lie&quot;, relevanceLanguage=&quot;en&quot;, type=&quot;video&quot; ) page_response = request.execute() search_results.append(page_response) else: # If it not None however, we use &quot;nextPageToken&quot; to specify the &quot;pageToken&quot; as a query parameter request = youtube.search().list( part=&quot;snippet&quot;, maxResults=50, publishedAfter=&quot;2024-10-01T00:00:00Z&quot;, publishedBefore=&quot;2024-11-06T00:00:00Z&quot;, order=&quot;viewCount&quot;, q=&quot;election fraud | stolen election | election lie&quot;, relevanceLanguage=&quot;en&quot;, type=&quot;video&quot;, pageToken=next_page_token # here goes our token ) page_response = request.execute() search_results.append(page_response) # Try to retrieve the &quot;nextPageToken&quot; if there is one. try: next_page_token = page_response[&quot;nextPageToken&quot;] # If the response does not have a &quot;nextPageToken&quot; field, we simply break out of the loop except KeyError: break   Note If you are only interested in the video results, you can also extract the respective data inside this loop by extending the search_results list with page_response[&quot;items&quot;].  ","version":"Next","tagName":"h2"},{"title":"Data collection methods: Video information​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#data-collection-methods-video-information","content":" As seen above, the information for each video we can collect directly from the search query is limited. To obtain more detailed data like the views or comment count, we turn to the videos() resource and again use the list() method. For instance, the same video shown above offers the following statistics:  {'viewCount': '5325494', 'likeCount': '143592', 'favoriteCount': '0', 'commentCount': '8695'}   Let’s write a function that takes a video_id as input und calls the API to retrieve more detailed information. Crucially, the youtube.videos().list() request can take multiple id’s as input, so we can speed up our data collection with batches.  # Our function to get video details def get_video_details(video_ids): if not video_ids: return [] # define batch size (limit is 50 again) batch_size = 50 videos = [] # process video id's in batches for i in range(0, len(video_ids), batch_size): batch = video_ids[i:i + batch_size] details_request = youtube.videos().list( part=&quot;snippet,statistics&quot;, id=&quot;,&quot;.join(batch) ) details_response = details_request.execute() videos.extend([ { &quot;title&quot;: video[&quot;snippet&quot;][&quot;title&quot;], &quot;published_at&quot;: video[&quot;snippet&quot;][&quot;publishedAt&quot;], &quot;channel_title&quot;: video[&quot;snippet&quot;][&quot;channelTitle&quot;], &quot;view_count&quot;: video[&quot;statistics&quot;].get(&quot;viewCount&quot;, 0), &quot;like_count&quot;: video[&quot;statistics&quot;].get(&quot;likeCount&quot;, 0), &quot;dislike_count&quot;: video[&quot;statistics&quot;].get(&quot;dislikeCount&quot;, 0), &quot;comment_count&quot;: video[&quot;statistics&quot;].get(&quot;commentCount&quot;, 0) } for video in details_response.get(&quot;items&quot;, []) ]) return videos   Now we extract the ID’s of our 100 most viewed videos related to election integrity and use the function above to retrieve more interesting information about the first 5 of them:  # Extract unique video ID’s from search results video_ids = list(set(video[&quot;id&quot;][&quot;videoId&quot;] for page in search_results for video in page.get(&quot;items&quot;, []))) # limit to first 5 videos N = 5 video_ids = video_ids[:N] # use the function latest_videos = get_video_details(video_ids) # print (some) info for video in latest_videos: print(f&quot;Title: {video['title']}, Published: {video['published_at']}, &quot; f&quot;Channel: {video['channel_title']}, Views: {video['view_count']}, &quot; f&quot;Likes: {video['like_count']}, Dislikes: {video['dislike_count']}, &quot; f&quot;Comments: {video['comment_count']}&quot;)   Title: DEBUNKING The Latest Election Lies From MAGA Senator | Bulwark Takes, Published: 2024-10-07T02:19:12Z, Channel: The Bulwark, Views: 332857, Likes: 20952, Dislikes: 0, Comments: 2460 Title: Voter Registration Fraud Discovered in Pennsylvania, Published: 2024-10-25T18:55:56Z, Channel: The Michael Lofton Show, Views: 567625, Likes: 9243, Dislikes: 0, Comments: 3285 Title: Will Trump’s baseless stolen election claims spark another Capitol attack? | ABC News, Published: 2024-11-03T22:56:07Z, Channel: ABC News (Australia), Views: 3994, Likes: 47, Dislikes: 0, Comments: 0 Title: Can Kamala Harris defeat Trump’s election lies in battleground Georgia? | Anywhere but Washington, Published: 2024-10-03T11:54:15Z, Channel: The Guardian, Views: 99329, Likes: 1718, Dislikes: 0, Comments: 510 Title: &quot;Trump's 2024 Election Strategy: Lies and Controversy!&quot;, Published: 2024-11-02T11:09:09Z, Channel: MJ News, Views: 6, Likes: 0, Dislikes: 0, Comments: 1   As of now, this data is stored in latest_videos as a list of dictionaries. To make it more manageable, we simply convert it to a pandas DataFrame object (a table, basically). This way, we can also easily export it to a CSV or Excel file.  data = pd.DataFrame(latest_videos) print(data)   Table 1: Results for video detail collection of the first five videos  index\ttitle\tpublished_at\tchannel_title\tview_count\tlike_count\tdislike_count\tcomment_count0\tDEBUNKING The Latest Election Lies From MAGA S...\t2024-10-07T02:19:12Z\tThe Bulwark\t332857\t20952\t0\t2460 1\tVoter Registration Fraud Discovered in Pennsyl...\t2024-10-25T18:55:56Z\tThe Michael Lofton Show\t567625\t9243\t0\t3285 2\tWill Trump’s baseless stolen election claims s...\t2024-11-03T22:56:07Z\tABC News (Australia)\t3994\t47\t0\t0 3\tCan Kamala Harris defeat Trump’s election lies...\t2024-10-03T11:54:15Z\tThe Guardian\t99329\t1718\t0\t510 4\t&quot;Trump's 2024 Election Strategy: Lies and Cont...\t2024-11-02T11:09:09Z\tMJ News\t6\t0\t0\t1  ","version":"Next","tagName":"h2"},{"title":"Data collection methods: Comments​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#data-collection-methods-comments","content":" Comment sections can be collected via the comments() resource and list() method. A single query for the example video looks like this:  request = youtube.commentThreads().list( part=&quot;snippet,id,replies&quot;, maxResults=100, #For this resource, the max amount of results is 100 order=&quot;time&quot;, videoId=&quot;IPUhRjAMCTo&quot; ) comment_response = request.execute()   With the output data for a single comment in the thread:  {'kind': 'youtube#commentThread', 'etag': 'FjHUXM2rssJNyLjk0GUPrIP1AeY', 'id': 'Ugy8fFs_mIdgiaBW7wF4AaABAg', 'snippet': {'channelId': 'UCwWhs_6x42TyRM4Wstoq8HA', 'videoId': 'IPUhRjAMCTo', 'topLevelComment': {'kind': 'youtube#comment', 'etag': 'Kp3rpMeuZ1_egtsYT6K_GX9-rrU', 'id': 'Ugy8fFs_mIdgiaBW7wF4AaABAg', 'snippet': {'channelId': 'UCwWhs_6x42TyRM4Wstoq8HA', 'videoId': 'IPUhRjAMCTo', 'textDisplay': 'No do the same for Kamala and Biden. Much more material.', 'textOriginal': 'No do the same for Kamala and Biden. Much more material.', 'authorDisplayName': '@stevenberry3294', 'authorProfileImageUrl': 'https://yt3.ggpht.com/ytc/AIdro_mljzddy7jo9d1eT87Vxkf-wgEsl_KEIealLasN5hw=s48-c-k-c0x00ffffff-no-rj', 'authorChannelUrl': 'http://www.youtube.com/@stevenberry3294', 'authorChannelId': {'value': 'UCiJyUwZOM8CL07N7RjjmWdA'}, 'canRate': True, 'viewerRating': 'none', 'likeCount': 0, 'publishedAt': '2025-03-04T02:12:13Z', 'updatedAt': '2025-03-04T02:12:13Z'}}, 'canReply': True, 'totalReplyCount': 0, 'isPublic': True}}   Having constructed the comment collection query for a single video, we can write a loop to retrieve all comments for the same 100 most viewed videos related to election integrity.  This loop does two things:  It iterates over each video IDIt iterates through all comment results for each video id with pagination  comment_results = dict() # This time, we create an empty dictionary to store the comment query results # iterate over the video IDs for id in tqdm(video_ids): # this initialises the comment results for this particular video ID to be an empty list comment_results[id] = list() # Try to retrieve the first page of comments for the video try: request = youtube.commentThreads().list( part=&quot;snippet,id,replies&quot;, maxResults=100, order=&quot;time&quot;, videoId=id ) comment_response = request.execute() comment_results[id].append(comment_response) # Some videos might have disable comments. # If so, these lines of code will catch the error and simply move on to the next video. except Exception as e: print(id, e) continue # Try to retrieve the &quot;nextPageToken&quot; if there is one. try: nextPageToken = comment_response[&quot;nextPageToken&quot;] # If the response does not have a &quot;nextPageToken&quot; field, the loop moves on to the next video except KeyError: continue # Given a value was found, this retrieves the comments until a &quot;nextPageToken&quot; can’t be found while True: request = youtube.commentThreads().list( part=&quot;snippet,id,replies&quot;, maxResults=100, order=&quot;time&quot;, videoId=id, pageToken=nextPageToken ) comment_response = request.execute() comment_results[id].append(comment_response) try: nextPageToken = comment_response[&quot;nextPageToken&quot;] except KeyError: break   Now we retrieve the number of comment threads for each of the first three videos and the respective total number of comments we were able to collect.  stats_list = list() for i, id in enumerate(comment_results): nb_threads = 0 nb_comments = 0 for result in comment_results[id]: nb_threads += len(result[&quot;items&quot;]) for item in result[&quot;items&quot;]: nb_comments += 1 if &quot;replies&quot; in item: nb_comments += len(item[&quot;replies&quot;][&quot;comments&quot;]) stats_list.append({&quot;video_id&quot;: id, &quot;nb_threads&quot;: nb_threads, &quot;nb_comments&quot;: nb_comments}) stats_df = pd.DataFrame(stats_list)   Table 2: Results for comment collection of the first three videos  video_id\tnb_threads\tnb_commentsIPUhRjAMCTo\t5317\t6337 chIsUyT5mVg\t4676\t5566 yiowo1L58rg\t3874\t4379  Note We can use the comment data collected to analyze the networks that develop in these comment sections. This can be found in the chapter &quot;Social Network Analysis&quot;. For this, use the unique ID’s we gathered from the comment authors and their replies and convert them to a simple edge list.  ","version":"Next","tagName":"h2"},{"title":"Data collection methods: Channels​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#data-collection-methods-channels","content":" Lastly, if we have a set of channels we want to monitor in terms of their impact and the contents they disseminate, we can retrieve this data with channels() and list() method and subsequently utilize the methods we already learned to collect all the information we need.  We first write a function to retrieve some channel’s general information and statistics. Then, we write a function that displays the unique ID of that channel. Lastly, we write a function to get the latest videos this channel has published. In this example, we retrieve information about the channel “@AntiSpiegel” that is associated with the media outlet “Anti-Spiegel” run by the prominent Russian propagandist Thomas Röper.    Screenshot of the AntiSpiegel YouTube channel  # define function to get a channel's information def get_channel_info(user_handle:str): request = youtube.channels().list( part=&quot;snippet,statistics&quot;, forHandle= user_handle ) response = request.execute() info = response['items'][0]['snippet'] statistics = response['items'][0]['statistics'] return info,statistics # define function to get channel id def get_channel_id(user_handle): request = youtube.search().list( part=&quot;snippet&quot;, q=user_handle, type=&quot;channel&quot;, maxResults=1 ) response = request.execute() if response[&quot;items&quot;]: print(f&quot;Channels found: {len(response[&quot;items&quot;])}&quot;) return response[&quot;items&quot;][0][&quot;id&quot;][&quot;channelId&quot;] else: print(&quot;No channel found with that username&quot;) return None # define function to get latest video id's def get_latest_videos(channel_id, max_results:int=5,after:str= '2025-01-01',before:str='2025-02-23'): request = youtube.search().list( part=&quot;id&quot;, channelId=channel_id, order=&quot;date&quot;, publishedAfter=f&quot;{after}T00:00:00Z&quot;, publishedBefore=f&quot;{before}T00:00:00Z&quot;, maxResults=max_results, type=&quot;video&quot; ) response = request.execute() video_ids = [video[&quot;id&quot;][&quot;videoId&quot;] for video in response.get(&quot;items&quot;, [])] video_ids = video_ids[:max_results] return video_ids   Applying these functions to the &quot;@AntiSpiegel&quot; YouTube channel looks like this:  channel_info,statistics = get_channel_info(&quot;@AntiSpiegel&quot;) print(&quot;Channel info:&quot;,channel_info['title'],&quot;\\n\\n&quot;,&quot;Channel statistics:&quot;,statistics) # retrieve channel id for any account channel_id = get_channel_id(&quot;@AntiSpiegel&quot;) print(f&quot;\\n Channel ID: {channel_id}&quot;) # retrieve id's of latest videos on XXXX account video_ids = get_latest_videos(channel_id) print(video_ids)   Channel info: Anti Spiegel Channel statistics: {'viewCount': '14460636', 'subscriberCount': '144000', 'hiddenSubscriberCount': False, 'videoCount': '113'} Channel ID: UC93mqUPbNmHZhl4fAVvZWpQ ['IJyCAsBJJEo', 'm1jAhFRq3YI', 'T37-ST2kkiI', 'RyxFcVMDJts', 'O9P1eAZ9Sc0']   To sum up, the combination of functions and methods provided in this tutorial equip you to closely monitor and retrieve a comprehensive set of datapoints from YouTube. You can now construct, edit and execute queries for any resource the API provides. Wrapping these queries with some Python code allows you to store and analyse data on channels, videos about topics of interest as well as discourses in the comment sections. Crucially, the steps in this tutorial prepare you to explore the vast landscape of content on YouTube and gain insights into the production and dissemination of disinformation across different geographical or societal contexts.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Data Collection on YouTube","url":"/docs/data-collection/03_00_platform-specific guidelines/03_00_data-collection_youtube#references","content":" Allgaier, J. (2019). Science and environmental communication on YouTube: Strategically distorted communications in online videos on climate change and climate engineering. Frontiers in communication, 4, 36. Knüpfer, C., Schwemmer, C., &amp; Heft, A. (2023). Politicization and Right-Wing Normalization on YouTube: A Topic-Based Analysis of the “Alternative Influence Network”. International Journal Of Communication, 17, 23. Retrieved from https://ijoc.org/index.php/ijoc/article/view/20369. Maldida.es. (2025). „European politician crushes Spanish politician in the European Parliament“: A network of disinformation channels on YouTube. Maldita.es. Retrieved April 09, 2025, from https://maldita.es/malditobulo/20250313/network-channels-youtube-disinformation-spanish-politics-eu/. Richardson, L., &amp; Flannery O’Connor, J. (2023, August 24). Complying with the Digital Services Act. The Keyword. Retrieved April 09, 2025, from https://blog.google/around-the-globe/google-europe/complying-with-the-digital-services-act/ ","version":"Next","tagName":"h2"},{"title":"What types of data can be collected for research on social media?","type":0,"sectionRef":"#","url":"/docs/data-collection/03_01_data-types","content":"","keywords":"","version":"Next"},{"title":"Introduction: A structured overview of the four different types of social media data​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#introduction-a-structured-overview-of-the-four-different-types-of-social-media-data","content":"   In today's digital world, social media platforms are overflowing with data, making them a goldmine for researchers in many fields. As of 2023, more than 4.9 billion people around the globe are using social media, which is over 60% of the world’s population [Source].  A comprehensive understanding of these platforms and the types of data they generate can significantly enhance the ability to analyse user behaviour, societal trends and communication patterns. This chapter aims to provide a structured overview of four different types of data available from social media:  Content Data: This includes all kinds of media created and shared by users, like text, images, videos, and audio. It gives us insights into how people express themselves and communicate.Interaction Data: This involves metrics that capture how users engage with content, including reactions, shares, and overall reach, revealing patterns of engagement and influence.Metadata: This consists of contextual information about the content, such as timestamps, geolocation data, and technical specifications, which help us better understand the context and usage of the content.User Data: This category covers information about the users themselves, including demographic details, profile information, and account characteristics, which help to analyse user behaviour and segmentation.  By organising these data types, we hope to highlight potential research applications and encourage researchers to make the most of social media data in their work. If you want to learn more about potential sensibilities, legal and ethical considerations for various types of data, check the corresponding chapters in the section on How to get started. If you want to learn more on how to access the available data then feel free to check out the corresponding section How to access data on platforms.  ","version":"Next","tagName":"h2"},{"title":"Content Data​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#content-data","content":" Content data is the main type of information generated on social media platforms. It includes the actual material that users create and share, and can be broadly grouped into three main types: textual content, visual content, and audio content. Each type provides unique insights into user preferences, cultural trends, and engagement patterns.  ","version":"Next","tagName":"h2"},{"title":"Textual Content​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#textual-content","content":" Textual content covers all written communication, including status updates, tweets, captions, comments, and hashtags. This type of data is essential for understanding user opinions, sentiment, and trending topics. Natural language processing (NLP) techniques are often used to derive meaning from large amounts of textual data, allowing for sentiment analysis, topic modelling, and tracking language trends. Hashtags and keywords also play an important role in monitoring how specific themes or movements gain traction across networks.  ","version":"Next","tagName":"h3"},{"title":"Visual Content​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#visual-content","content":" Visual content includes images, graphics, memes, and videos shared on platforms like Instagram, TikTok, or Facebook. It serves as a rich source of insight into cultural trends, identity representation, and the visual language of social media. Visuals can express emotions and social statements in ways that text alone cannot. To analyse visual content effectively, specialised tools like image recognition software and machine learning algorithms are utilised to identify patterns in imagery, colour usage, and facial expressions.  ","version":"Next","tagName":"h3"},{"title":"Audio Content​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#audio-content","content":" Audio data, such as voice messages, podcasts, or music clips, are becoming more prominent with the rise of platforms like Clubhouse and TikTok. Analysing audio allows us to explore speech patterns, voice tones, and the popularity of music and sound trends across networks. The growth of voice-driven platforms opens up new avenues to examine conversational dynamics, cultural expressions through sound, and the effects of audio content on social engagement.  ","version":"Next","tagName":"h3"},{"title":"Interaction Data​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#interaction-data","content":" Interaction data captures the various ways users engage with content and each other on social media platforms. Examining this data allows for insights into user behaviour, how information spreads, and community dynamics. Interaction data can be divided into three main categories: Content Engagement, User-to-User Engagement, and Content Redistribution.  ","version":"Next","tagName":"h2"},{"title":"Content Engagement​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#content-engagement","content":" Content Engagement looks at how users interact with posts, such as through views, comments, and reactions. Views indicate passive consumption and provide metrics for content visibility and reach. By looking at view counts, it’s possible to understand audience size and exposure to content. Comments let users share feedback or join discussions, providing qualitative data that reveals public sentiment and the nature of conversations around certain topics. Reactions - in the form of likes or emojis - offer quick emotional responses to content and serve as indicators of popularity and immediate impact. By examining reaction patterns, insights into user sentiment and emotional engagement can be gathered.  ","version":"Next","tagName":"h3"},{"title":"User-to-User Engagement​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#user-to-user-engagement","content":" User-to-User Engagement emphasises the interactions between individuals, reflecting the relationships formed on social media. Following shows sustained interest and connection, with follower counts revealing potential influence and network size. Analysing follower dynamics provides insight into audience-building strategies and influencer networks. Mentions, or tagging other users, foster discussions and boost content visibility, and can be analysed to understand social interactions and user connectivity. Direct Messages (DMs) facilitate private conversations, which are crucial for understanding personal relationships, although they pose privacy challenges for research. Group participation allows users to connect in communities based on shared interests, shedding light on collective behaviour and group dynamics.  ","version":"Next","tagName":"h3"},{"title":"Content Redistribution​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#content-redistribution","content":" Content Redistribution looks at how users help spread content across networks. Shares enable users to pass on posts to their followers, significantly increasing visibility. Analysing share patterns can reveal the viral potential of content and how information diffuses across networks. Retweets and reposts are specific forms of sharing that extend the reach of content without changing it. In TikTok reaction videos, creatures react to an existing TikTok video that is shown in parallel. This provides a new opportunity for instant reaction and sharing. Studying the frequency of these actions provides insights into how content spreads organically and its overall impact.  ","version":"Next","tagName":"h3"},{"title":"Metadata​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#metadata","content":" While content data focuses on the actual substance of social media posts, metadata offers contextual information about that content, including when, where, and how it was created or interacted with.  ","version":"Next","tagName":"h2"},{"title":"Temporal Data​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#temporal-data","content":" Temporal metadata captures time-related information, such as when posts are created, liked, or shared. This data is useful for analysing trends over time, like peak activity hours, content lifespan, and the timing of viral trends.  ","version":"Next","tagName":"h3"},{"title":"Spatial Data​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#spatial-data","content":" Spatial metadata involves location information linked to posts or user interactions, obtained through geotags or inferred from user activity. For researchers, spatial data is vital for exploring geographic trends, regional sentiments, and localised behaviour patterns.  ","version":"Next","tagName":"h3"},{"title":"Technical Data​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#technical-data","content":" Technical metadata provides insights into the devices, operating systems, and platforms used to access social media. This information helps understand access patterns, such as whether mobile or desktop devices dominate usage, and how different browsers might influence user behaviour. It also helps in identifying bots or automated accounts based on patterns of access and engagement.  ","version":"Next","tagName":"h3"},{"title":"User Data​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#user-data","content":" User data encompasses the information that social media platforms collect about individual users. This data helps create personalised user experiences and allows for analysing demographic trends, user behaviours, and network structures. Unlike content or interaction data, user data is specifically focused on the characteristics of individuals who are engaging with the platforms.  ","version":"Next","tagName":"h2"},{"title":"Demographic Data​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#demographic-data","content":" Demographic data consists of basic information like age, gender, language, and ethnicity. Social media platforms often gather this data during account creation or through user interactions. For researchers, demographic data is crucial for understanding how different groups engage with content, their preferences, and how specific demographics can influence trends or discussions.  ","version":"Next","tagName":"h3"},{"title":"Profile Information​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#profile-information","content":" Profile information covers the details users opt to share publicly, including bios, interests, locations, and affiliations. Platforms like LinkedIn provide detailed professional profiles, while others like X/Twitter offer short bios. This data helps analyse how users present their identities, their social connections, and the types of networks formed based on shared interests or occupations.  ","version":"Next","tagName":"h3"},{"title":"Account Details​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#account-details","content":" Account details refer to the technical and behavioural attributes tied to user accounts, such as account creation date, activity frequency, and follower count. This data helps differentiate between new and established users and active and passive users, identify influencers or high-impact accounts, and track the evolution of user behaviour over time.  ","version":"Next","tagName":"h3"},{"title":"Conclusion: These categories create a robust framework that facilitates the exploration of online human behaviour​","type":1,"pageTitle":"What types of data can be collected for research on social media?","url":"/docs/data-collection/03_01_data-types#conclusion-these-categories-create-a-robust-framework-that-facilitates-the-exploration-of-online-human-behaviour","content":" In summary, social media data encompasses a wide range of categories that provide valuable insights into user behaviour, interactions, and content dynamics.  Content data, including textual, visual, and audio elements, forms the basis for understanding user-generated contributions and trends. Each content type offers unique opportunities for analysis, helping to explore themes, sentiments, and user engagement.  Interaction data reflects the various ways users engage with content and with one another on social media platforms. By examining this data, valuable insights can be gained into patterns of engagement, the effectiveness of content, and the dynamics of user relationships within digital communities.  Metadata adds further context to the analysis with temporal, spatial, and technical information. This additional layer is essential for uncovering patterns over time, understanding geographic influences, and addressing the technical aspects of content creation and sharing.  User data, which includes demographic data, profile information, and account details, offers insights into the characteristics and preferences of users, allowing for a more nuanced understanding of audience engagement.  Together, these categories create a robust framework that facilitates the exploration of behaviour patterns, sentiment, and influence across social media platforms. As new analytical techniques and technologies continue to emerge, there is great potential for valuable insights into social behaviour and communication in the digital age. ","version":"Next","tagName":"h2"},{"title":"Data Collection of Facebook and Instagram Ads","type":0,"sectionRef":"#","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#introduction","content":" Meta’s Ad Library provides a public record of ads that run on Facebook and Instagram. Researchers, journalists, and civic watchdogs can use this data to analyze advertising trends, for example, tracking political campaign ads, spending, and the reached demographics. The Meta Ad Library API offers programmatic access to these ads, enabling retrieval of detailed ad content and performance information. Each ad entry includes metadata such as the ad’s text, the advertiser’s page, the time period it ran, the amount spent (as a range), impressions delivered (also as a range), and breakdowns of the audience by age, gender, and region. Importantly, many metrics are given as ranges (min–max) rather than precise values.  This tutorial will demonstrate how to use R (with the tidyverse ecosystem) and the Radlibrary R package to access the Meta Ad Library via its official API. We will walk through obtaining API access, constructing queries to find ads (by keyword or page id), retrieving ad data, and performing analyses such as ad volume and spend over time, top advertisers, and demographic targeting patterns.  ","version":"Next","tagName":"h2"},{"title":"Step 1: Setting Up API Access (Verification & Developer Account)​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#step-1-setting-up-api-access-verification--developer-account","content":" Before writing any code, you need to secure access to the Ad Library API. Meta requires a few one-time setup steps:  Confirm your identity and location: Facebook mandates an ID verification process for anyone accessing political ad data (the same process required to run political ads). You will need to provide a government ID and proof of your country. This can take 1–2 days for approval.    Create a Facebook Developer account: Go to the Facebook for Developers portal and sign up with your Facebook account (if you haven’t already). Agree to any platform policies as needed. Once you have a developer account, create a new “App” in the dashboard (choose Business or Custom app type for this purpose). This app is just a container to obtain API credentials. Generate an access token: The Ad Library API is accessed via Meta’s Graph API. The simplest way to get a token is by using theGraph API Explorertool. Once you are on the Graph API Explorer page, generate a user access token. You need to add the permission ads_read in the token generation dialog so that the token is authorized to query the ads archive. Once generated, copy this token for use in R. Keep it confidential and treat it like a password – anyone with this token could potentially query the API on your behalf until it expires.  Note Token expiration: By default, tokens from the Explorer are short-lived(usually ~1-2 hours). For short analysis sessions that might be sufficient, but in most cases you will likely need longer access. You can exchange the short-lived token for a 60-day token using your App’s App ID and App Secret. In this tutorial, we will proceed with a short-lived token for simplicity, but it is strongly encouraged to get a long-term token for your analysis (holds for 60 days). For instructions on how to do this, refer to the official Meta documentation on access tokens.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Installing and Loading R Packages​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#step-2-installing-and-loading-r-packages","content":" We will use an R package called Radlibrary (by Meta’s Facebook Research team) to interact with the Ad Library API. Radlibrary is a convenient wrapper that handles authentication and query construction, saving us from crafting raw Graph API calls (which we could also do if we feel fancy like that). It also helps format results into tidy data frames. In addition, we will use the tidyverse for data manipulation (dplyr, tidyr) and ggplot2 for visualization. Finally, we will use my very own package,metatargetr to retrieve some ad spending data. If you haven’t installed these packages, do so first:  # Install Radlibrary from GitHub (it’s not on CRAN as of writing) if(!(&quot;pak&quot; %in% installed.packages())){ install.packages(&quot;pak&quot;) # if devtools not already installed }   # Install Radlibrary pak::pak(&quot;facebookresearch/Radlibrary&quot;) # Install metatargetr pak::pak(&quot;favstats/metatargetr&quot;) # Install lubridate pak::pak(&quot;lubridate&quot;) # Install tidyverse if not already (includes dplyr, ggplot2, etc.) pak::pak(&quot;tidyverse&quot;)   # Load the libraries in your R session library(Radlibrary) library(metatargetr) library(lubridate) # for convenient date functions library(tidyverse)   Make sure Radlibrary installed successfully (you might need to update Rtools on Windows or install additional library on Linux distributions). Now you are ready to use the Meta Ad Library API in R!  ","version":"Next","tagName":"h3"},{"title":"Step 3: Authenticating with your Access Token​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#step-3-authenticating-with-your-access-token","content":" With your user access token in hand (from Step 1), you need to provide it to Radlibrary so it can authenticate API requests. As a general rule, never hard-code the token directly in scripts. One safe approach is to use R’s readline() function to paste the token interactively (this avoids storing it in your R command history):  # Prompt for the token (paste your token string at the prompt that appears) token &lt;- readline(prompt = &quot;Enter your Facebook API access token: &quot;)   When you run this, R will pause and let you paste the token. Hit Enter and it will be stored in the token variable for use. This method ensures the token is not visible in your script or R history.  Optionally:  You can also save the token as an environment variable like this:  # Set the token as an environment variable Sys.setenv(META_API_TOKEN = token)   Now, in the rest of your script, you can retrieve your token like thisbut only after you have restarted your R session:  token &lt;- Sys.getenv(&quot;META_API_TOKEN&quot;)   ","version":"Next","tagName":"h3"},{"title":"Querying the Ad Library API​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#querying-the-ad-library-api","content":" ","version":"Next","tagName":"h2"},{"title":"Step 4: Building a Query to the Ad Library API (adlib_build_query)​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#step-4-building-a-query-to-the-ad-library-api-adlib_build_query","content":" Now we get to the moment we have been waiting for – how to get the data! The Ad Library API requires specifying what ads you want to retrieve. This is done by constructing a query with various parameters. TheRadlibrary function adlib_build_query() helps create this query object.  We will start with a simple example scenario: Suppose we want to find ads related to the climate in the runup to the 2025 German parliamentary elections. We are interested in all such ads (whether currently active or inactive) that were shown three weeks before election day but only those that were classified or self-identified as political or issue ads.  First, we specifiy a list of all the variables that we would like to retrieve, here I use all available variables as of June 2025.  ## First we ad_fields &lt;- c( ## some meta info and unique identifier &quot;page_id&quot;, &quot;page_name&quot;,# &quot;id&quot;, # id is added automatically ## general info, text, description, run times &quot;ad_creation_time&quot;, &quot;ad_delivery_start_time&quot;, &quot;ad_delivery_stop_time&quot;, &quot;ad_creative_bodies&quot;, &quot;ad_creative_link_captions&quot;, &quot;ad_creative_link_descriptions&quot;, &quot;ad_creative_link_titles&quot;, &quot;ad_snapshot_url&quot;, &quot;languages&quot;, &quot;publisher_platforms&quot;, ## spending info &quot;currency&quot;, &quot;spend&quot;, &quot;bylines&quot;, &quot;beneficiary_payers&quot;, ## delivery and reach &quot;delivery_by_region&quot;, &quot;demographic_distribution&quot;, &quot;estimated_audience_size&quot;, &quot;impressions&quot;, # &quot;br_total_reach&quot;, # unique reach (only available for Brazil) ## EU only &quot;eu_total_reach&quot;, &quot;age_country_gender_reach_breakdown&quot;, &quot;target_ages&quot;, &quot;target_gender&quot;, &quot;target_locations&quot; )   Now we are ready to build the query step by step:  # Build an Ad Library API query for ads related to &quot;climate&quot; in Germany during 2025 election query &lt;- adlib_build_query( ad_reached_countries = &quot;DE&quot;, # country where ads were delivered ad_delivery_date_min = &quot;2025-02-03&quot;, # specify minimum date: 21 days before election day ad_delivery_date_max = &quot;2025-02-23&quot;, # specify maximum date: election day ad_active_status = &quot;ALL&quot;, # include both active and inactive ads search_terms = &quot;klima&quot;, # keywords to search in ad text or metadata ad_type = &quot;POLITICAL_AND_ISSUE_ADS&quot;, # restrict to political/issue ads fields = ad_fields, # data fields we want limit = 200 # number of results per page (max 1000) )   Note You might encounter the following warning: Warning: Unsupported fields supplied: followed by a list of parameters. This warning can be safely ignored. The Radlibrary package, despite being developed by the Facebook team, may not be up to date with the newest parameters.    Parameter Breakdown​  Let us unpack the parameters used in the API query (and some additional ones):  ad_reached_countries Specifies the countries where the ads were delivered. For example, setting this to &quot;DE&quot; retrieves ads delivered in Germany. At least one country code must be specified. Multiple countries can be provided as a vector, e.g., c(&quot;US&quot;, &quot;CA&quot;).  ad_delivery_date_min and ad_delivery_date_max Define the date range for when the ads were delivered. The format should be &quot;YYYY-MM-DD&quot;. For instance, setting ad_delivery_date_min = &quot;2025-02-22&quot; and ad_delivery_date_max = &quot;2025-02-23&quot; retrieves ads delivered between February 22 and February 23, 2025.   ad_active_status determines the delivery status of the ads to retrieve. If not specified, the default is &quot;ACTIVE&quot;, which returns only currently active ads. For historical analysis, setting this to &quot;ALL&quot; retrieves both active and inactive ads. Valid values: &quot;ALL&quot;: all ads, past and present&quot;ACTIVE&quot;: only currently running ads&quot;INACTIVE&quot;: only ads that have stopped running   search_terms is a keyword or phrase to search within the ad’s content, title, or disclaimer text. The API treats a blank space as a logical AND and searches for both terms without other operators. For example, &quot;climate change&quot; is interpreted as &quot;climate&quot; AND &quot;change&quot;. To search for an exact phrase, use the search_type parameter set to &quot;KEYWORD_EXACT_PHRASE&quot;.   search_page_ids is an optional alternative to search_terms and retrieves ads from a specific Facebook Page. Provide the numeric Page ID (e.g., &quot;1234567890&quot;). This is ideal when focusing on a particular advertiser’s metadata and content. You can find page IDs via: The Ad Library API (just query it by search_terms as we show below and take note of a page id of interest).Download spending reports in the Ad Library Report which includes spending by page id.In the URL of an Ad Library page, i.e. after the view_all_page_id URL parameter. For example: https://www.facebook.com/ads/library/?view_all_page_id=179587888720522 is the Ad Library Page for the U.S. Department of Homeland Security and 179587888720522 is the page id.   ad_type specifies the category of ads to retrieve. Valid values include: &quot;ALL&quot;: Retrieves all ads, regardless of category.&quot;POLITICAL_AND_ISSUE_ADS&quot;&quot;EMPLOYMENT_ADS&quot;&quot;HOUSING_ADS&quot;&quot;FINANCIAL_PRODUCTS_AND_SERVICES_ADS&quot;   fields determines what information about each ad will be returned. In our example, we request specific fields defined in the ad_fields variable. The fields are categorized as follows: Meta Information and Identifiers: &quot;page_id&quot;: Unique identifier for the Facebook Page.&quot;page_name&quot;: Name of the Facebook Page. General Information: &quot;ad_creation_time&quot;: Time when the ad was created.&quot;ad_delivery_start_time&quot;: Start time of the ad delivery.&quot;ad_delivery_stop_time&quot;: Stop time of the ad delivery.&quot;ad_creative_bodies&quot;: Main text content of the ad.&quot;ad_creative_link_captions&quot;: Captions in the call-to-action section.&quot;ad_creative_link_descriptions&quot;: Descriptions in the call-to-action section.&quot;ad_creative_link_titles&quot;: Titles in the call-to-action section.&quot;ad_snapshot_url&quot;: URL to a snapshot of the ad.&quot;languages&quot;: Languages used in the ad.&quot;publisher_platforms&quot;: Platforms where the ad was published (e.g., Facebook, Instagram). Spending Information: &quot;currency&quot;: Currency used for the ad spend.&quot;spend&quot;: Amount spent on the ad.&quot;bylines&quot;: Bylines associated with the ad.&quot;beneficiary_payers&quot;: Entities that paid for the ad. Delivery and Reach: &quot;delivery_by_region&quot;: Regional delivery information.&quot;demographic_distribution&quot;: Demographic breakdown of the ad’s audience.&quot;estimated_audience_size&quot;: Estimated size of the audience.&quot;impressions&quot;: Number of times the ad was displayed. EU-Specific Fields: &quot;eu_total_reach&quot;: Total reach within the European Union.&quot;age_country_gender_reach_breakdown&quot;: Breakdown of reach by age, country, and gender.&quot;target_ages&quot;: Targeted age groups.&quot;target_gender&quot;: Targeted genders.&quot;target_locations&quot;: Targeted locations. These fields provide comprehensive information about each ad, including its content, delivery, and audience targeting. For more info, you can check the Meta Ad Library API documentation.   limit limits the number of results per API call. The default value is 25, and the maximum is 1,000. If your query could return more, you will need to paginate (more about that later). For now, we assume 100 is sufficient for demonstration purposes.    Next Step​  At this point, we have only created a query object, a structured list containing all parameters. The query has not yet been sent to Meta.  The function adlib_build_query() only constructs the query. You can inspect it by printing query, which will show its components and the exact URL to be called.  Let us now proceed to fetch the data.  ","version":"Next","tagName":"h3"},{"title":"Step 5: Retrieving Ad Data from the API (adlib_get)​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#step-5-retrieving-ad-data-from-the-api-adlib_get","content":" To execute the query and get results, we use Radlibrary’s functionadlib_get(). This function takes our query and the access token, sends the request to Meta’s Graph API, and returns the response. Let’s call it:  # Execute the query and retrieve data result &lt;- adlib_get(query, token = token)   Under the hood, this hits the Graph API’s /ads_archive endpoint with all the parameters we specified. The result we get back is an object of class adlib_data_response. It contains the data and some metadata.  glimpse(result$data[[1]], max.level = 1)    $ id : chr &quot;547343151714655&quot; $ page_id : chr &quot;530858850114749&quot; $ page_name : chr &quot;Undone Work GmbH&quot; $ ad_creation_time : chr &quot;2025-02-23&quot; $ ad_delivery_start_time : chr &quot;2025-02-23&quot; $ ad_delivery_stop_time : chr &quot;2025-02-28&quot; $ ad_creative_bodies :List of 1 $ ad_creative_link_captions :List of 1 $ ad_snapshot_url : chr &quot;https://www.facebook.com/ads/archive/render_ad/?id=547343151714655&amp;access_token=XXXX&quot;| __truncated__ $ languages :List of 1 $ publisher_platforms :List of 1 $ currency : chr &quot;EUR&quot; $ spend :List of 2 $ bylines : chr &quot;Undone Work GmbH&quot; $ beneficiary_payers :List of 1 $ delivery_by_region :List of 16 $ demographic_distribution :List of 15 $ estimated_audience_size :List of 1 $ impressions :List of 2 $ eu_total_reach : int 616 $ age_country_gender_reach_breakdown:List of 1 $ target_ages :List of 2 $ target_gender : chr &quot;All&quot; $ target_locations :List of 1   Now that result is in hand, let’s convert it into a more analysis-friendly format.  ","version":"Next","tagName":"h3"},{"title":"Step 6: Converting to a Tidy Data Frame​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#step-6-converting-to-a-tidy-data-frame","content":" Radlibrary provides an S3 method to turn the result into a tibble (a tidyverse-friendly data frame). We simply use as_tibble():  ads_df &lt;- as_tibble(result, censor_access_token = TRUE)   By default, we include censor_access_token = TRUE to strip out the token from any embedded URLs in the data (this is a safety measure so we don’t accidentally reveal our token when inspecting data). Now ads_dfis a tibble where each row is one ad and each column is a variable returned by the API.  If you want to check the columns, try glimpse(ads_df) ornames(ads_df) to inspect the structure of the ads_df data frame. Some of the key columns include: - impressions_lower,impressions_upper: The estimated range of impressions delivered. -spend_lower, spend_upper: The estimated range of ad spend in the currency used - demographic_distribution: A so-called list-column containing, for each ad, a data frame of demographic percentages (since we asked for it). We will explore how to work with this column in Step 9.  ","version":"Next","tagName":"h3"},{"title":"Step 7: Handling Pagination for Larger Datasets (paginate_meta_api)​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#step-7-handling-pagination-for-larger-datasets-paginate_meta_api","content":" The Meta Ad Library API returns only a limited number of ads per request. To retrieve more than the default amount, you need to handle pagination by following the next_page links provided in the API response.  While the Radlibrary package offers the adlib_get_paginated()function to assist with pagination, it unfortunately does NOT handle rate limiting or delays between requests. To address this, I have implemented a custom function, paginate_meta_api(), which automates pagination and includes logic to manage API rate limits by introducing appropriate delays between requests. Specify max_pages, i.e. how many iterations you want to go through and also whether it should print update while retrieving data verbose = TRUE, and API usage limitsapi_health = TRUE (by default FALSE).  Here is how you can use the custom function:  # Load the custom pagination function source(&quot;https://gist.githubusercontent.com/favstats/ac37f6a7c881dddfa1c156bfb3e2dbdf/raw/b49e3f73881a4595309480e418658e018fbd0980/paginate_meta_api.R&quot;) # Retrieve all pages with delay logic climate_ads &lt;- paginate_meta_api(query, token, max_pages = 100, verbose = FALSE, api_health = FALSE)   At this stage, we have a data frame climate_ads of all retrieved ads and their metadata. We can now perform analysis on this data. Let us tackle a few common analysis tasks one by one.  ","version":"Next","tagName":"h3"},{"title":"Analzing the Data​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#analzing-the-data","content":" ","version":"Next","tagName":"h2"},{"title":"Step 8: Analyzing Ad Volume and Top Advertisers​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#step-8-analyzing-ad-volume-and-top-advertisers","content":" A basic question is how the number of ads changes over time. For example, did advertising surge closer to election day? We can visualize the number of ads in our dataset by date by using the ad delivery start date as the date an ad “entered” the library (since if an ad is active for multiple days, it is counted on the first day it ran). Let us create a time series of ad count by day:  climate_ads %&gt;% mutate(start_date = as.Date(ad_delivery_start_time)) %&gt;% # extract date portion count(start_date) %&gt;% ggplot(aes(x = start_date, y = n)) + geom_line(color = &quot;steelblue&quot;) + labs(x = &quot;Date&quot;, y = &quot;Number of Ads Started&quot;, title = &quot;Daily Count of New Ads in Ad Library (\\&quot;climate\\&quot; query in Germany)&quot;) + theme_minimal()     This code groups ads by their start date and counts them, then plots a line graph. The result shows that we retrieved much more data than we had specified. This sometimes happens – the API is not perfect. We filter to include only data within the specified timeframe.  climate_ads %&gt;% mutate(start_date = as.Date(ad_delivery_start_time)) %&gt;% # extract date portion count(start_date) %&gt;% filter(start_date &gt;= as.Date(&quot;2025-02-03&quot;)) %&gt;% ggplot(aes(x = start_date, y = n)) + geom_line(color = &quot;darkgreen&quot;) + labs(x = &quot;Date&quot;, y = &quot;Number of Ads Started&quot;, title = &quot;Daily Count of New Ads in Ad Library (\\&quot;climate\\&quot; query in Germany)&quot;) + theme_minimal()     Another limitation with counting ads is that the ads listed in the ad library do not represent unique ads, but rather ad runs. If an advertiser runs the same ad again with some changes in settings, it will be counted as a separate ad. This may overinflate the number of unique ads. One possible way to address this is to filter for unique texts (e.g. ad_creative_bodies).  Another approach is to aggregate by spending, which gives a sense of where the advertiser’s focus lies.  climate_ads %&gt;% mutate(keyword = &quot;climate&quot;) %&gt;% mutate(start_date = as.Date(ad_delivery_start_time)) %&gt;% # extract date portion group_by(start_date,keyword) %&gt;% summarize(spend_lower = sum(spend_lower), spend_upper = sum(spend_upper)) %&gt;% ungroup() %&gt;% rowwise() %&gt;% mutate(spend_mid = median(c(spend_lower, spend_upper))) %&gt;% filter(start_date &gt;= as.Date(&quot;2025-02-03&quot;)) %&gt;% ggplot(aes(x = start_date, y = spend_mid, color = keyword)) + geom_ribbon(aes(ymin = spend_lower, ymax = spend_upper), alpha = 0.1, linetype = &quot;blank&quot;) + geom_line() + labs(x = &quot;Date&quot;, y = &quot;Daily Ad Spending in Euro&quot;, title = &quot;Daily Spending on Ads in Ad Library (\\&quot;climate\\&quot; query in Germany)&quot;) + theme_minimal() + scale_color_manual(values = c( &quot;darkgreen&quot;)) + theme(legend.position = &quot;bottom&quot;)     Who is advertising on the climate topic?​  Another common analysis is to identify which organizations or pages are running the most ads in your data. We can easily rank advertisers by the number of ads:  climate_ads %&gt;% group_by(page_name) %&gt;% dplyr::summarize(spend_upper = sum(spend_upper)) %&gt;% ungroup() %&gt;% arrange(desc(spend_upper)) %&gt;% slice(1:10) %&gt;% mutate(page_name =fct_reorder(page_name, spend_upper)) %&gt;% ggplot(aes(x = page_name, y = spend_upper)) + geom_col(fill=&quot;darkgray&quot;) + coord_flip() + # flip for horizontal bars (easier to read names) labs(x = &quot;Page Name&quot;, y = &quot;Upper Spending Boundary&quot;, title = &quot;Top 20 Advertisers in \\&quot;Climate\\&quot; Ad Dataset&quot;) + theme_minimal()     Given that search terms sometimes are a bit unpredictable and don’t always work as expected, we can also query the top 10 advertisers based on spending. We can do so by retrieving the spending reports from Meta, conveniently archived by themetatargetr package. For a full tutorial on metatargetr and its capabilities see this tutorial.  spending_report &lt;- get_report_db(&quot;DE&quot;, timeframe = 30, ds = &quot;2025-02-23&quot;) national_parties &lt;- spending_report %&gt;% filter(page_name %in% c(&quot;Die Linke&quot;, &quot;SPD&quot;, &quot;BÜNDNIS 90/DIE GRÜNEN&quot;, &quot;FDP&quot;, &quot;CDU&quot;, &quot;AfD&quot;)) # Build an Ad Library API query for ads related to &quot;climate&quot; in Germany during 2025 election query &lt;- adlib_build_query( ad_reached_countries = &quot;DE&quot;, # country where ads were delivered ad_delivery_date_min = &quot;2025-02-03&quot;, # specify minimum date: 21 days before election day ad_delivery_date_max = &quot;2025-02-23&quot;, # specify maximum date: election day ad_active_status = &quot;ALL&quot;, # include both active and inactive ads search_page_ids = national_parties$page_id, # search page IDs, up to 10 at once ad_type = &quot;POLITICAL_AND_ISSUE_ADS&quot;, # restrict to political/issue ads fields = ad_fields, # data fields we want limit = 200 # number of results per page (max 1000) ) top_ads &lt;- paginate_meta_api(query, token, max_pages = 100, verbose = TRUE, api_health = TRUE)   We are going to visualize some of the text included inside the ad data by creating a chatter plot. For that, we also need some additional packages listed below.  pak::pak(&quot;tidytext&quot;) pak::pak(&quot;stopwords&quot;) pak::pak(&quot;ggrepel&quot;)   # Define party colors party_colors &lt;- c( &quot;Die Linke&quot; = &quot;#BE3075&quot;, &quot;SPD&quot; = &quot;#E3000F&quot;, &quot;BÜNDNIS 90/DIE GRÜNEN&quot; = &quot;#64A12D&quot;, &quot;FDP&quot; = &quot;#FFED00&quot;, &quot;CDU&quot; = &quot;#000000&quot;, &quot;AfD&quot; = &quot;#009EE0&quot; ) # Tokenize ad texts and count word frequencies top_ads %&gt;% unnest(ad_creative_bodies) %&gt;% tidytext::unnest_tokens(word, ad_creative_bodies) %&gt;% filter(!is.na(word)) %&gt;% anti_join(tibble(word = stopwords::stopwords(&quot;de&quot;)), by = &quot;word&quot;) %&gt;% # Select top 30 words per party count(page_name, word, sort = TRUE) %&gt;% group_by(page_name) %&gt;% top_n(30, n) %&gt;% ungroup() %&gt;% mutate(page_name = fct_relevel(page_name, c(&quot;Die Linke&quot;, &quot;SPD&quot;, &quot;BÜNDNIS 90/DIE GRÜNEN&quot;, &quot;FDP&quot;, &quot;CDU&quot;, &quot;AfD&quot;))) %&gt;% # Create the chatter plot ggplot(aes(x = page_name, y = n, label = word, color = page_name)) + # geom_point(alpha = 0.7) + ggrepel::geom_text_repel( force = 5, box.padding = 0.1, max.overlaps = Inf, segment.color = NA, # This removes the lines size = 3 ) + labs( x = &quot;Political Party (Left to Right)&quot;, y = &quot;Word Frequency&quot;, title = &quot;Common Words in Political Ads by Party&quot; ) + scale_color_manual(values = party_colors) + theme_minimal() + scale_y_log10() + theme(legend.position = &quot;none&quot;)     This chatter plot visualizes the most frequent words found in the ads of Germany’s major political parties. The parties are arranged along the x-axis according to their position on the political spectrum, from left to right. The y-axis represents the frequency of each word on a logarithmic scale, which helps visualize words with a wide range of frequencies. This type of analysis allows us to quickly grasp the key themes and messaging priorities for each party. For instance, we can observe which topics are unique to certain parties and which are shared across the political landscape, providing insights into their campaign strategies and focus areas.  ","version":"Next","tagName":"h3"},{"title":"Step 9: Examining Demographic Distributions​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#step-9-examining-demographic-distributions","content":" One aspect of the Ad Library data is the audience distribution for each ad. We requested demographic_distribution in our query, which for each ad includes the percentage of impressions by age bracket and gender. This data is returned as a nested list-column in our queried dataset. To analyze it, we need to unnest that list into a usable table.  We can use tidyr::unnest() to expand the demographic distribution:  # Unnest demographic distribution into a long format data frame demo_df &lt;- top_ads %&gt;% select(id, page_name, demographic_distribution, page_name) %&gt;% # focus on relevant columns unnest(demographic_distribution) head(demo_df)    ## # A tibble: 6 × 5 ## id page_name percentage age gender ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2124558957977402 SPD 0.00029 18-24 female ## 2 2124558957977402 SPD 0.00159 18-24 male ## 3 2124558957977402 SPD 0.00391 25-34 female ## 4 2124558957977402 SPD 0.00985 25-34 male ## 5 2124558957977402 SPD 0.0136 35-44 female ## 6 2124558957977402 SPD 0.0265 35-44 male   After unnesting, demo_df will have one row per demographic category per ad. It should include the columns: id (ad id), page_name, age, gender, and percentage. Each row might say, for example, ad X – age 18-24 – female – 0.2 (meaning 20% of ad X’s impressions were shown to women aged 18-24). The percentages for a given ad across all age/gender categories sum up to 100%.  Important If an ad did not reach a particular demographic group, it may not have an entry for that group.  Now, what can we learn from this? Here are a couple of insights we might extract:  Which age groups are ads reaching most frequently? We can count how many ads reached each age group. For instance, how many ads reached any people in the 65+ category versus 18-24? If few ads have impressions in older age groups, that suggests advertisers either target younger users or simply fail to engage older audiences. Similarly, we could examine how many ads target women vs. men, or the average percentage of impressions to women vs. men.  Note of caution A more robust analysis would weight the data by impressions or spending. Averaging percentages across ads without doing so treats a low-reach ad the same as a high-reach one. For simplicity, however, we proceed with the unweighted approach.  For a quick view, we can calculate the overall gender split in relative impressions, assuming equal weight per ad (again, caution advised):  demo_df %&gt;% filter(age != &quot;Unknown&quot;, gender != &quot;unknown&quot;) %&gt;% group_by(page_name, age, gender) %&gt;% summarise(percentage = mean(percentage), .groups = &quot;drop&quot;) %&gt;% mutate(percentage = ifelse(gender == &quot;male&quot;, -percentage, percentage)) %&gt;% ggplot(aes(x = age, y = percentage, fill = gender)) + geom_col(width = 0.8) + coord_flip() + facet_wrap(~page_name, ncol = 3) + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + labs( x = &quot;Age Group&quot;, y = &quot;Average Percentage of Impressions&quot;, title = &quot;Ad Audience Demographics by Party, Age, and Gender&quot;, fill = &quot;Gender&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;)     In the illustrative chart above, each bar shows how many ads had at least some impressions in that age group. We observe a trend where AfD reaches more younger men on average, whereas Die Linke is more likely to reach younger women. Keep in mind, this does not directly tell us the volume of impressions, just the distribution of reach. An ad with only a tiny fraction of impressions in 65+ would still count here. To truly measure impression share, one would need to aggregate the percentages weighted by each ad’s total impressions. Because the data only provides ranges for impressions, a rough approach could be to use the midpoint of each ad’s impression range as a weight. That level of detail is beyond our scope here, but it is something to consider for a more rigorous analysis.  In summary, the demographic data allows us to see who is being reached by these ads. Advertisers’ choices (or the outcome of the delivery algorithm) become visible: Are they reaching young adults more than seniors? Are they targeting predominantly one gender? These insights are valuable for understanding the focus and targeting strategies of political campaigns.  ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Data Collection of Facebook and Instagram Ads","url":"/docs/data-collection/03_00_platform-specific guidelines/03_04_data-collection_meta_ads#conclusion","content":" In this tutorial, we demonstrated a full workflow for accessing and analyzing Facebook and Instagram advertising data using R and the Meta Ad Library API. We covered everything from setting up access credentials and verifying identity, to using the Radlibrary package to query the API, and finally exploring the data with tidyverse tools and visualizations. We learned how to retrieve ads by keyword or advertiser, how to handle pagination and nested demographic data, and how to create basic insights like time trends and top advertisers.  The Meta Ad Library API provides researchers to study political advertising and how public discourse is shaped through paid messages. As a next step, you might refine these examples: try querying a different issue or country, dive deeper into ad content with text analysis, fetch regional distributions to map out where ads are being seen, or correlate spending with specific topics. You may also check out my other tutorial on metatargetr which adds additional features not present in the Ad Library API such as retrieval of ad library reports and exact spending on specific target audiences (including detailed and custom audiences).  Happy researching – and may your analyses shed light on the world of online (political) ads! ","version":"Next","tagName":"h2"},{"title":"Common data collection methods on social media platforms illustrated with TikTok","type":0,"sectionRef":"#","url":"/docs/data-collection/03_02_data-collection-methods","content":"","keywords":"","version":"Next"},{"title":"Open Source Research (aka Document Audit)​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#open-source-research-aka-document-audit","content":" Open-Source Research can cover a range of sources. From public documentation shared by platforms themselves, to reports or information they are legally required to publish to internal documents that become available due to leaks or unintended publications.  ","version":"Next","tagName":"h2"},{"title":"Press Releases​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#press-releases","content":" Online platforms and services often share information and updates for marketing purposes, highlighting positive things like user growth or examples of how they support the individual or public good. TikTok for example regularly publishes updates on their own “newsroom” about new products as well as analyses of the content on the platform (e.g. Toplists). While press releases are commonly favorable to the platform, they also intend to shape public discourse about the platform (or disperse criticism). For example, in the TikTok Community News section you will find that TikTok specifically mentions several communities from Entertainment (Gaming, Sports, Music), but also stories with which TikTok wants to emphasize diversity after - presumably to counter narratives that certain voices are being silenced or that negative content is spreading on the platform. This includes articles hightlighting #BlackTikTok, #WomenOfTikTok, #TransVisibility or Medical #MentalHealthAwarness #ItsTimeForChange (Eating Disorder Awareness Week).  I asked ChatGPT to “identify 5 recurring topics in the following text”, which was the 33 headlines of 2023 I copy pasted from the website. chatGPT classfied the headlines into the following topics, that could be intrepreted as what the TikTok PR Department thinks is most important to the brand:  Gaming on TikTokCommunity and CreativityPartnerships and CollaborationsMusic and EntertainmentSocial and Cultural Awareness  ","version":"Next","tagName":"h3"},{"title":"Documentation and Reports​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#documentation-and-reports","content":" Another source of information are official reports or (legal) documentaion, including Terms of Services, support documents, transparency reports, audits or similar. TikTok for example publishes transparency reports on their website. They often provide data in a machine-readable format so that it is possible to track changes over time or conduct analyses beyond what the platform shares proactively.  ","version":"Next","tagName":"h3"},{"title":"Research​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#research","content":" Large online platforms often have dedicated research and development teams that publish some of their findings in journals or share results during scientific conferences. Google for instance has published some very impactful research, starting with the original page rank algroithm on which the platform was based in its early years, as well as various contributions to research in the AI field, most notably the transformer models that enabled the creation of large language models. Though academic research papers often focus on a rather narrow question, they sometimes explain a key element of a platform in depth (like the page rank example).  Not all platforms are open about the inventions at the core of their systems, but even ByteDance, the parent company of TikTok, allows their employees to publish academic papers from time to time. In 2022, TikTok researchers published a study Monolith: Real Time Recommendation System With Collisionless Embedding Table, which in which they describe how they tackled the challenges of real-time recommendations in large item space. While it is unclear, whether this is the indeed the underlying technology that drives TikTok, many assume it is.  ","version":"Next","tagName":"h3"},{"title":"Internal Documents and Journalistic Sources​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#internal-documents-and-journalistic-sources","content":" Investigative journalists sometimes have access to internal information from whistleblowers and informants that reveal organizational misconduct or systemic problems within platforms.  TikTok has been subject of whistleblowing and leaking, too. In June 2022 audio recordings from meetings were leaked to reporters that disclosed that data from US users was being accessed from China. Early in 2023 the same journalists revealed that TikToks For You Page is not entirely driven by the algorithm, but that instead TikTok employees can use a mechanism coined 'heating' to push certain content.  ","version":"Next","tagName":"h3"},{"title":"Document Leaks​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#document-leaks","content":" Sometimes parts of the material used for journalistic reporting become public, too. Platform researchers can use these documents to obtain first-hand knowledge about a platform and its inner workings.  In 2021, someone discovered a leaked document from TikTok breaking down the algorithm. The New York Times reported about the incident but did not give access to their translation. We optained the orgiginal document from chinese document sharing platforms and translated it to get a first hand look at the document.  During research for our auditing project, we discovered the original Chinese-language document and used google translate to translate it for research purposes. It containes a detailed description of the idea of the algorithms used for ranking including the purposes and underlying ‘values’ that drive the development at TikTok.  ","version":"Next","tagName":"h3"},{"title":"Code/data audit:​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#codedata-audit","content":" You can, of course, also apply more technical approaches to platforms and run analyses based on code, such asopen source code or reverse engineering specific applications or platform architectures.  ","version":"Next","tagName":"h2"},{"title":"Open Source Code​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#open-source-code","content":" While there are few open sources packages by TikTok itself, you can find a selection on Github, for example in the ByteDance repositories. Besides data related to research (see above), you can find documentation for Software Development Kit (SDK) that other apps can use to share data with TikTok as well as libraries hinting at the architecture used at ByteDance in general or TikTok (or is chinese equivalent Douyin) specifically.  Other platforms are more open about the code they develop and the products they use. For example, X (Twitter) has published a version of the code of their recommendation engine, although responses about the transparency it offers are mixed  On a more technical level, Facebook with react and zstd and Google with a variety of contributions have also openly contributed and fostered open source projects and research which also, in part, power their own platforms.  ","version":"Next","tagName":"h3"},{"title":"Reverse Engineering​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#reverse-engineering","content":" Reverse Engineering is a process through which researchers try to understand the logic of a program, service or app that is in some way obfuscated. For instance, this could mean that the code is compiled and the high-level logic is already translated in low-level code – which can make it harder for humans to understand. In other cases, developers intentionally obfuscate code to prevent others from replicating their work, notably when no compiling is involved, as is often the case on the web where JavaScript Code and HTML are normally provided in a form that are reproducible.  Reverse Engineering is helpful when analyzing apps like TikTok. For Android Apps, MobSF provides reverse engineering and other security tools. For example, these tools automatically scan the code for known trackers and libraries. In addition, MobSF can help provide the results of common Android decompiling tools.    ","version":"Next","tagName":"h3"},{"title":"API Access​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#api-access","content":" Many platforms allow researchers to access their platforms, or specific subsystems of their platforms, through APIs. While data access through APIs is subject to changes at the platform’s digression, very large online platforms operating in the European Union are legally required to provide APIs for public content. While X (Twitter) and Reddit are currently lacking in compliance, other platforms like Telegram or YouTube are still accessible through APIs. Moreover, there are third party platforms like RapidAPI that provide “unofficial APIs” for several networks – please do be mindful how and for what research purposes you access these.  ","version":"Next","tagName":"h2"},{"title":"Hidden APIs​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#hidden-apis","content":" Web based platforms offer a simple way to peak into their inner workings. Familiarize yourself with the respective developer tools to understand how a platform's API and REST works. You can use the network panel in the developer tools to see the &quot;[hidden APIs]&quot;(We https://ianlondon.github.io/blog/web-scraping-discovering-hidden-apis/), the messages exchanged between your browser and the platform that contain the data shown to you, mostly in a structured format.  The Markup has a great walkthrough on how they used APIs to uncover a story. If you want to dig a bit deeper and don't want to check all APIs manually you can use tools like mitmproxy2swagger to systematically analyse a website. To do so, you browse a website with the developer tools, open the network tab and surf the website. Afterwards you can download the HAR file as described in the mitmproxy2swagger documentation and use the following command to create a systematic description of the API.  mitmproxy2swagger -i ~/Downloads/www.tiktok.com.har -o ~/Downloads/tt_web_api.yml -p &quot;https://www.tiktok.com/api/&quot;  The YAML file you will receive is an API definition in a style calld &quot;swagger&quot;. If you paste the data to an appropriate editor you will get an overview of the data, that the tiktok web application sends and receives:  ","version":"Next","tagName":"h3"},{"title":"Data Donations​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#data-donations","content":" To understand the experience of actual users of the platforms, first-hand data often provides you with the richest and most illustrative insight.  To collect and analyze user data, you can revert to data donations. Data donations are sensitive and therefore require researchers to be rigorous, transparent, and accountable for how they use the data that they ask users to hand over. Today, users can often access data that a specific platform has stored about them – in the European Union by filing a data access request based on the General Data Protection Regulation (GDPR) . For TikTok, a data donation approach has been used by the DataSkope project as well as by academic researchers, who recruited participants via Facebook and then asked them to share their TikTok data, results are captured in Likes and Fragments: Examining Perceptions of Time Spent on TikTok.  This method comes with certain drawbacks: you need to consider cost as well as size of your needed data set. Moreover, for TikTok specifically, researchers found that the data that users can download only presents a selection of the data the platform does in fact collect.  ","version":"Next","tagName":"h2"},{"title":"Scraping​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/docs/data-collection/03_02_data-collection-methods#scraping","content":" Another common method for gathering information about a platform and content that is published on it is scraping. Scraping is a way of extracting data from websites or apps in a structured form to replicate the content available on a platform. This allows researchers to gain insights into different aspects such as: Networks of actors, content published on specific topic, filtering or prioritization mechanisms to present content on the platform and more. For example, we have scraped the public TikTok website to better understand the topics or pieces of content that are going “viral” in Germany in a week-by-week analysis. Other researchers have leveraged scraping to understand the development in different countries or learn more about TikTok’s approach to blocking and deleting content. An in-depth how to guide for web scraping can be found on this Data Knowledge Hub. ","version":"Next","tagName":"h2"},{"title":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","type":0,"sectionRef":"#","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules","content":"","keywords":"","version":"Next"},{"title":"Best Practices for Creating Queries and Rules​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#best-practices-for-creating-queries-and-rules","content":" Here are some tips and best practices for creating effective rules for querying the X (Twitter) API:  Define clear goals: Before you start querying, know what information you need. Do you want to collect tweets about a specific topic, from a specific location, or at a specific time? The clearer your goals, the more efficient your query will be. Keyword research: Take the time to research relevant keywords, phrases, and hashtags. This will help you focus your data request. Use Boolean Operators: You can use Boolean operators such as AND, OR, and NOT to make your search criteria more specific. For example, climate change AND research returns tweets that contain both terms. Consider alternate spellings: When focusing on keywords or hashtags, consider alternate spellings, abbreviations, or misspellings. Use exclusions: To filter unwanted results from your query, you can exclude certain words or phrases. Use geolocation features: If your research interest is geographically limited, use the X’s (Twitter’s) API geolocation feature to collect tweets from specific regions. (Note: Only a small number of tweets contain geo-information.) Set time limits: If relevant, you can limit your query to a specific time period. Test and tweak: Start with a broad query and narrow it down incrementally. Review the results and adjust your query as needed. Consider usage limits: The X (Twitter) API has usage limits. Make sure you understand them and plan your queries accordingly.  ","version":"Next","tagName":"h2"},{"title":"Building a query or rule​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#building-a-query-or-rule","content":" If you are using the Search Tweets endpoint (Recent Search and Full Search), the filter is called a query. And if you are using the Filtered Stream endpoint, it is called a rule.  ","version":"Next","tagName":"h2"},{"title":"Query limitations​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#query-limitations","content":" Depending on your access level, the length of queries or the number of rules may be limited.  ","version":"Next","tagName":"h3"},{"title":"Operator types: standalone and conjunction-required​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#operator-types-standalone-and-conjunction-required","content":" Operators are distinguished in two types: standalone operators and conjunction-required operators. Standalone operators need not necessarily be used in conjunction with other operators, but they can be.  The following query utilizes the #hashtag operator, which is a standalone operator.  #science   Conjunction-required operators presuppose the use of at least one standalone operator in the query. Otherwise, they would be too broad in scope and the query would generate an excessive number of Tweets. The following examples do not contain standalone operators and thus are not legitimate queries.  has:mentions has:media OR is:verified   If we add in a standalone operator, such as the phrase “twitter data”, the query will work properly. The above example can be made valid by adding a standalone operator. For example:  &quot;research results&quot; has:mentions (has:media OR has:links)   ","version":"Next","tagName":"h3"},{"title":"Boolean operators and grouping​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#boolean-operators-and-grouping","content":" Multiple keywords can be combined by using boolean operators. Those are short words (e.g. AND, OR) that can be used to either expand the scope of the query or to specify the query.  Operator\tDescriptionAND logic\tQueries containing keywords combined by the boolean AND logic will yield only Tweets containing all the keywords. Spaces between keywords are implicitly interpreted as AND logic. For example, research results #ScientificBreakthrough will only match Tweets containing the words research and results as well as the hashtag #ScientificBreakthrough. OR logic\tCombining keywords with the OR-operator expands the scope of the query. It will find every tweet containing at least one of the given keywords. For example, human OR resources OR #meme will retrieve all tweets that include at least one of the terms human, resources or the hashtag #meme. NOT logic, negation\tTo apply negation (NOT) in logic, add a hyphen (-) before a keyword or operator. For instance, in the query science #meme -informatics, the search will identify posts with both #meme and science, excluding those that also have the term informatics. A frequently used example is -is:retweet, excluding Retweets and allowing matches for original Tweets, Quote Tweets, and replies. While all operators can be negated, standalone negated operators are not functional. Grouping\tGrouping operators is possible with parentheses. For instance, (research results) OR (#meme has:images) will yield Tweets that have either the research and results terms, or images tagged with the #meme hashtag. The sequence of evaluation involves ANDs first, then ORs.  When using both AND and OR functionalities together, the order of operations is as follows:  Operators that are linked by AND logic are combined as the first step.Afterward, operators connected through OR logic are applied.  For example:  dog OR cat mouse will be interpreted as dog OR (cat mouse)horse cow OR sheep will be interpreted as (horse cow) OR sheep  To remove any vagueness and ensure accurate evaluation of your rule, use parentheses to group terms together when necessary.  For example:  (dog OR cat) mousehorse (cow OR sheep)  ","version":"Next","tagName":"h2"},{"title":"Punctuation, diacritics, and case sensitivity​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#punctuation-diacritics-and-case-sensitivity","content":" Characters that include accents or diacritics are handled just like regular characters, without being regarded as word separators. For instance, a rule with the keyword jalapeños would solely identify Tweets containing the exact term jalapeños, without considering matches like jalape, jalapen, or os.  All operators are treated case-insensitive. For example, the query science will provide the same results as science, SCIENCE, Science.  Rules containing accents or diacritics lead to distinct behaviors between the Filtered Stream and Search endpoints.  ","version":"Next","tagName":"h2"},{"title":"Filtered Stream​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#filtered-stream","content":" When you define a keyword or hashtag rule that includes character accents or diacritics, it will identify Tweets containing the precise word with the correct accents or diacritics. It won’t include Tweets with accurate letters but lacking accents or diacritic marks.  For example, rules with the keyword Résumé or hashtag #jalapeños will match Tweets that contain Résumé or #jalapeños because they include the accents or diacritic. These rules will not match Tweets that contain Resume or #jalapenos.  ","version":"Next","tagName":"h3"},{"title":"Search Tweets​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#search-tweets","content":" Unlike Filtered Stream endpoints, Search endpoints are not insensitive to accents and diacritics. This means that queries containing them will return both terms with and without accents or diacritics. For example, the querys Résumé or hashtag #jalapeños will yield Tweets containing Résumé or #jalapeños as well as those containing Resume or #jalapenos.  For example, the queries Résumé or hashtag #jalapeños will yield Tweets containing Résumé or #jalapeños as well as those containing Resume or #jalapenos.  ","version":"Next","tagName":"h3"},{"title":"Quote Tweet matching behavior​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#quote-tweet-matching-behavior","content":" ","version":"Next","tagName":"h2"},{"title":"Filtered Stream​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#filtered-stream-1","content":" Operators will apply to both the content present in the initial Tweet that was quoted and the content present within the Quote Tweet.  ","version":"Next","tagName":"h3"},{"title":"Search Tweets​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#search-tweets-1","content":" Operators will not find matches in the content of the original Tweet that was quoted, but they will match content present in the Quote Tweet.  ","version":"Next","tagName":"h3"},{"title":"Iteratively building a rule​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#iteratively-building-a-rule","content":" ","version":"Next","tagName":"h2"},{"title":"Test your rule early and often​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#test-your-rule-early-and-often","content":" Achieving accurate results with a rule on the first attempt is uncommon. Due to the sheer volume and variety of Tweets it is rarely evident which exact Tweets the search will return.  In the process of creating a rule, it is therefore essential to frequently test it using the stream endpoint to observe the data it retrieves. You should also consider testing it using one of the Search Tweet endpoints, provided that the operators you use are also compatible with that endpoint.  In the following we will start with this simple rule and develop it in dependence on the output it generates:  create OR creation   ","version":"Next","tagName":"h3"},{"title":"Use results to narrow the rule​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#use-results-to-narrow-the-rule","content":" While testing the rule, it is essential to review the returned Tweets to verify if they contain the anticipated and desired data. It is recommended to start with a broad rule that typically generates a large set of Tweets. Afterwards this rule can be refined to exclude unwanted results.  Since with the existing rule we have obtained tweets in multiple languages the following specification limits the results to Tweets in English.  (create OR creation) lang:en   The test resulted in several Tweets praising divine creation. We are going to remove them from the results by adding the negated keyword operator -divine. Furthermore, we do not want to include retweets. We can reach that goal by adding the negated -is:retweet operator.  (create OR creation) lang:en -divine -is:retweet   ","version":"Next","tagName":"h3"},{"title":"Adjust for inclusion where needed​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#adjust-for-inclusion-where-needed","content":" If the query does not return certain Tweets that you know exist, you might need to widen your rule by eliminating operators that could potentially lead to the exclusion of the desired data.  We noticed that there are Tweets treating the same topic that are not included in our search results. That is because they use different terms similar or equal meaning. To cover those Tweets, we can add those terms to the rule:  (create OR creation OR making OR founding) lang:en -divine -is:retweet   ","version":"Next","tagName":"h3"},{"title":"Adjust for popular trends/bursts over the time period​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#adjust-for-popular-trendsbursts-over-the-time-period","content":" Since X (Twitter) is a highly dynamic platform, your rules may need to be adapted to upcoming and outdated trends. We therefore recommend updating and adjusting your rules periodically.  ","version":"Next","tagName":"h3"},{"title":"Operators​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#operators","content":" Essential: Available with all access levels.Elevated: Available when using a Project with Elevated, Academic Research, or Enterprise access.Certain operators have an alternate name or alias that can be used.  ","version":"Next","tagName":"h2"},{"title":"Available for Search Tweets and Filtered Stream​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#available-for-search-tweets-and-filtered-stream","content":" Operator\tType\tAvailability\tDescriptionkeyword\tStandalone\tEssential\tMatches a keyword within the body of a Tweet. This is a tokenized match, meaning that your keyword string will be matched against the tokenized text of the Tweet body. Tokenization splits words based on punctuation, symbols, and Unicode basic plane separator characters. For example, a Tweet with the text “I like coca-cola” would be split into the following tokens: I, like, coca, cola. These tokens would then be compared to the keyword string used in your query. To match strings containing punctuation (for example coca-cola), symbol, or separator characters, you must wrap your keyword in double-quotes. Example: pepsi OR cola OR &quot;coca cola&quot; emoji\tStandalone\tEssential\tMatches an emoji within the body of a Tweet. Similar to a keyword, emojis are a tokenized match, meaning that your emoji will be matched against the tokenized text of the Tweet body. Note that if an emoji has a variant, you must wrap it in double quotes to add to a query. Example: (😃 OR 😡) 😬 &quot;exact phrase match&quot;\tStandalone\tEssential\tMatches the exact phrase within the body of a Tweet. Example: (&quot;Twitter API&quot; OR #v2) -&quot;recent search&quot; #\tStandalone\tEssential\tMatches any Tweet containing a recognized hashtag, if the hashtag is a recognized entity in a Tweet. This operator performs an exact match, NOT a tokenized match, meaning the rule #thanku will match posts with the exact hashtag #thanku, but not those with the hashtag #thankunext. Example: #thankunext #fanart OR @arianagrande @\tStandalone\tEssential\tMatches any Tweet that mentions the given username, if the username is a recognized entity (including the @ character). Example: (@twitterdev OR @twitterapi) -@twitter $\tStandalone\tElevated\tMatches any Tweet that contains the specified ‘cashtag’ (where the leading character of the token is the ‘$’ character). Note that the cashtag operator relies on Twitter’s ‘symbols’ entity extraction to match cashtags, rather than trying to extract the cashtag from the body itself. Example: $twtr OR @twitterdev -$fb from:\tStandalone\tEssential\tMatches any Tweet from a specific user. The value can be either the username (excluding the @ character) or the user’s numeric user ID. You can only pass a single username/ID per from: operator. Example: from:twitterdev OR from:twitterapi -from:twitter to:\tStandalone\tEssential\tMatches any Tweet that is in reply to a particular user. The value can be either the username (excluding the @ character) or the user’s numeric user ID. You can only pass a single username/ID per to: operator. Example: to:twitterdev OR to:twitterapi -to:twitter url:\tStandalone\tEssential\tPerforms a tokenized match on any validly-formatted URL of a Tweet. This operator can matches on the contents of both the url or expanded_url fields. For example, a Tweet containing &quot;You should check out Twitter Developer Labs: https://t.co/c0A36SWil4&quot; (with the short URL redirecting to https://developer.twitter.com) will match both the following rules: from:TwitterDev url:&quot;https://developer.twitter.com&quot; (because it will match the contents of entities.urls.expanded_url) from:TwitterDev url:&quot;https://t.co&quot; (because it will match the contents of entities.urls.url) Tokens and phrases containing punctuation or special characters should be double-quoted (for example, url:&quot;/developer&quot;). Similarly, to match on a specific protocol, enclose in double-quotes (for example, url:&quot;https://developer.twitter.com&quot;). retweets_of:\tStandalone\tEssential\tMatches Tweets that are Retweets of the specified user. The value can be either the username (excluding the @ character) or the user’s numeric user ID. You can only pass a single username/ID per retweets_of: operator. Example: retweets_of:twitterdev OR retweets_of:twitterapi in_reply_to_tweet_id:\tStandalone\tEssential\tAvailable alias: in_reply_to_status_id: Matches on replies to the specified Tweet. Example: in_reply_to_tweet_id:1539382664746020864 retweets_of_tweet_id:\tStandalone\tEssential\tAvailable alias: retweets_of_status_id: Matches on explicit (or native) Retweets of the specified Tweet. Note that the Tweet ID used should be the ID of an original Tweet and not a Retweet. Example: retweets_of_tweet_id:1539382664746020864 quotes_of_tweet_id:\tStandalone\tEssential\tAvailable alias: quotes_of_status_id: Matches on Quote Tweets of the specified Tweet. Note that the Tweet ID used should be the ID of an original Tweet and not a Quote Tweet. Example: quotes_of_tweet_id:1539382664746020864 context:\tStandalone\tEssential\tMatches Tweets with a specific domain id/entity id pair. To learn more about this operator, please visit our page on annotations. You can only pass a single domain/entity per context: operator. context:domain_id.entity_id However, you can combine multiple domain/entities using the OR operator: (context:47.1139229372198469633 OR context:11.1088514520308342784) Examples: context:10.799022225751871488 (domain_id.entity_id returns Tweets matching that specific domain-entity pair) entity:\tStandalone\tEssential\tMatches Tweets with a specific entity string value. To learn more about this operator, please visit our page on annotations. Please note that this is only available with recent search. You can only pass a single entity: operator. entity:&quot;string declaration of entity/place&quot; Examples: entity:&quot;Michael Jordan&quot; OR entity:&quot;Barcelona&quot; conversation_id:\tStandalone\tEssential\tMatches Tweets that share a common conversation ID. A conversation ID is set to the Tweet ID of a Tweet that started a conversation. As Replies to a Tweet are posted, even Replies to Replies, the conversation_id is added to its JSON payload. You can only pass a single conversation ID per conversation_id: operator. Example: conversation_id:1334987486343299072 (from:twitterdev OR from:twitterapi) list:\tStandalone\tElevated\tNEW Matches Tweets posted by users who are members of a specified list. For example, if @twitterdev and @twitterapi were members of List 123, and you included list:123 in your query, your response will only contain Tweets that have been published by those accounts. You can find List IDs by using the List lookup endpoint. Please note that you can only use a single list: operator per query, and you can only specify a single List per list: operator. Example: list:123 place:\tStandalone\tElevated\tMatches Tweets tagged with the specified location or Twitter place ID. Multi-word place names (“New York City”, “Palo Alto”) should be enclosed in quotes. You can only pass a single place per place: operator. Note: See the GET geo/search standard v1.1 endpoint for how to obtain Twitter place IDs. Note: This operator will not match on Retweets, since Retweet's places are attached to the original Tweet. It will also not match on places attached to the original Tweet of a Quote Tweet. Example: place:&quot;new york city&quot; OR place:seattle OR place:fd70c22040963ac7 place_country:\tStandalone\tElevated\tMatches Tweets where the country code associated with a tagged place/location matches the given ISO alpha-2 character code. You can find a list of valid ISO codes on Wikipedia. You can only pass a single ISO code per place_country: operator. Note: This operator will not match on Retweets, since Retweet's places are attached to the original Tweet. It will also not match on places attached to the original Tweet of a Quote Tweet. Example: place_country:US OR place_country:MX OR place_country:CA point_radius:\tStandalone\tElevated\tMatches against the place.geo.coordinates object of the Tweet when present, and in X (Twitter), against a place geo polygon, where the Place polygon is fully contained within the defined region. point_radius:[longitude latitude radius] Units of radius supported are miles (mi) and kilometers (km) Radius must be less than 25mi Longitude is in the range of ±180 Latitude is in the range of ±90 All coordinates are in decimal degrees Rule arguments are contained within brackets, space delimited You can only pass a single geo polygon per point_radius: operator. Note: This operator will not match on Retweets, since Retweet's places are attached to the original Tweet. It will also not match on places attached to the original Tweet of a Quote Tweet. Example: point_radius:[2.355128 48.861118 16km] OR point_radius:[-41.287336 174.761070 20mi]&lt;br /&gt; bounding_box:\tStandalone\tElevated\tAvailable alias: geo_bounding_box: Matches against the place.geo.coordinates object of the Tweet when present, and in X (Twitter), against a place geo polygon, where the place polygon is fully contained within the defined region. bounding_box:[west_long south_lat east_long north_lat] west_long south_lat represent the southwest corner of the bounding box where west_long is the longitude of that point, and south_lat is the latitude. east_long north_lat represent the northeast corner of the bounding box, where east_long is the longitude of that point, and north_lat is the latitude. Width and height of the bounding box must be less than 25mi Longitude is in the range of ±180 Latitude is in the range of ±90 All coordinates are in decimal degrees. Rule arguments are contained within brackets, space delimited. You can only pass a single geo polygons per bounding_box: operator. Note: This operator will not match on Retweets, since Retweet's places are attached to the original Tweet. It will also not match on places attached to the original Tweet of a Quote Tweet. Example: bounding_box:[-105.301758 39.964069 -105.178505 40.09455]&lt;br /&gt; is:retweet\tConjunction required\tEssential\tMatches on Retweets that match the rest of the specified rule. This operator looks only for true Retweets (for example, those generated using the Retweet button). Quote Tweets will not be matched by this operator. Example: data @twitterdev -is:retweet is:reply\tConjunction required\tEssential\tDeliver only explicit replies that match a rule. Can also be negated to exclude replies that match a query from delivery. Note: This operator is also available with the filtered stream endpoint. When used with filtered stream, this operator matches on replies to an original Tweet, replies in quoted Tweets, and replies in Retweets. Example: from:twitterdev is:reply is:quote\tConjunction required\tEssential\tReturns all Quote Tweets, also known as Tweets with comments. Example: &quot;sentiment analysis&quot; is:quote is:verified\tConjunction required\tEssential\tDeliver only Tweets whose authors are verified by Twitter. Example: #nowplaying is:verified -is:nullcast\tConjunction required\tElevated\tRemoves Tweets created for promotion only on ads.twitter.com that have a &quot;source&quot;:&quot;Twitter for Advertisers (legacy)&quot; or &quot;source&quot;:&quot;Twitter for Advertisers&quot;. This operator must be negated. For more info on Nullcasted Tweets, see our page on Tweet availability. Example: &quot;mobile games&quot; -is:nullcast has:hashtags\tConjunction required\tEssential\tMatches Tweets that contain at least one hashtag. Example: from:twitterdev -has:hashtags has:cashtags\tConjunction required\tElevated\tMatches Tweets that contain a cashtag symbol (with a leading ‘$’ character. For example, $tag). Example: #stonks has:cashtags has:links\tConjunction required\tEssential\tThis operator matches Tweets which contain links and media in the Tweet body. Example: from:twitterdev announcement has:links has:mentions\tConjunction required\tEssential\tMatches Tweets that mention another Twitter user. Example: #nowplaying has:mentions has:media\tConjunction required\tEssential\tAvailable alias: has:media_link Matches Tweets that contain a media object, such as a photo, GIF, or video, as determined by Twitter. This will not match on media created with Periscope, or Tweets with links to other media hosting sites. Example: (kittens OR puppies) has:media has:images\tConjunction required\tEssential\tMatches Tweets that contain a recognized URL to an image. Example: #meme has:images has:video_link\tConjunction required\tEssential\tAvailable alias: has:videos Matches Tweets that contain native Twitter videos, uploaded directly to Twitter. This will not match on videos created with Periscope, or Tweets with links to other video hosting sites. Example: #icebucketchallenge has:video_link has:geo\tConjunction required\tElevated\tMatches Tweets that have Tweet-specific geolocation data provided by the Twitter user. This can be either a location in the form of a Twitter place, with the corresponding display name, geo polygon, and other fields, or in rare cases, a geo lat-long coordinate. Note: Operators matching on place (Tweet geo) will only include matches from original tweets. Retweets do not contain any place data. Example: recommend #paris has:geo -bakery lang:\tConjunction required\tEssential\tMatches Tweets that have been classified by Twitter as being of a particular language (if, and only if, the tweet has been classified). It is important to note that each Tweet is currently only classified as being of one language, so AND’ing together multiple languages will yield no results. You can only pass a single BCP 47 language identifier per lang: operator. Note: if no language classification can be made the provided result is ‘und’ (for undefined). Example: recommend #paris lang:en The list below represents the currently supported languages and their corresponding BCP 47 language identifier: Amharic: am German: de Malayalam: ml Slovak: sk Arabic: ar Greek: el Maldivian: dv Slovenian: sl Armenian: hy Gujarati: gu Marathi: mr Sorani Kurdish: ckb Basque: eu Haitian Creole: ht Nepali: ne Spanish: es Bengali: bn Hebrew: iw Norwegian: no Swedish: sv Bosnian: bs Hindi: hi Oriya: or Tagalog: tl Bulgarian: bg Latinized Hindi: hi-Latn Panjabi: pa Tamil: ta Burmese: my Hungarian: hu Pashto: ps Telugu: te Croatian: hr Icelandic: is Persian: fa Thai: th Catalan: ca Indonesian: in Polish: pl Tibetan: bo Czech: cs Italian: it Portuguese: pt Traditional Chinese: zh-TW Danish: da Japanese: ja Romanian: ro Turkish: tr Dutch: nl Kannada: kn Russian: ru Ukrainian: uk English: en Khmer: km Serbian: sr Urdu: ur Estonian: et Korean: ko Simplified Chinese: zh-CN Uyghur: ug Finnish: fi Lao: lo Sindhi: sd Vietnamese: vi French: fr Latvian: lv Sinhala: si Welsh: cy Georgian: ka Lithuanian: lt   ","version":"Next","tagName":"h3"},{"title":"Only available for Filtered Stream​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#only-available-for-filtered-stream","content":" Operator\tType\tAvailability\tDescriptionfollowers_count: Essential\tMatches Tweets when the author has a followers count within the given range. If a single number is specified, any number equal to or higher will match. Example: followers_count:500 Additionally, a range can be specified to match any number in the given range. Example: followers_count:1000..10000 tweets_count: Essential\tAvailable alias: statuses_count: Matches Tweets when the author has posted a number of Tweets that falls within the given range. If a single number is specified, any number equal to or higher will match. Example: tweets_count:1000 Additionally, a range can be specified to match any number in the given range. Example: tweets_count:1000..10000 following_count: Essential\tAvailable alias: friends_count: Matches Tweets when the author has a friends count (the number of users they follow) that falls within the given range. If a single number is specified, any number equal to or higher will match. Example: following_count:500 Additionally, a range can be specified to match any number in the given range. Example: following_count:1000..10000 listed_count: Essential\tAvailable alias: user_in_lists_count: Matches Tweets when the author is included in the specified number of Lists. If a single number is specified, any number equal to or higher will match. Example: listed_count:10 Additionally, a range can be specified to match any number in the given range. Example: listed_count:10..100 url_title: Essential\tAvailable alias: within_url_title: Performs a keyword/phrase match on the expanded URL HTML title metadata. Example: url_title:snow url_description: Essential\tAvailable alias: within_url_description: Performs a keyword/phrase match on the expanded page description metadata. Example: url_description:weather url_contains: Essential\tMatches Tweets with URLs that literally contain the given phrase or keyword. To search for patterns with punctuation in them (i.e. google.com) enclose the search term in quotes. NOTE: This will match against the expanded URL as well. Example: url_contains:photos source: Essential\tMatches any Tweet generated by the given source application. The value must be either the name of the application or the application’s URL. Cannot be used alone. Example: source:&quot;Twitter for iPhone&quot; Note: As a Twitter app developer, Tweets created programmatically by your application will have the source of your application Name and Website URL set in your app settings. in_reply_to_tweet_id: Essential\tAvailable alias: in_reply_to_status_id: Deliver only explicit Replies to the specified Tweet. Example: in_reply_to_tweet_id:1539382664746020864 retweets_of_tweet_id: Essential\tAvailable alias: retweets_of_status_id: Deliver only explicit (or native) Retweets of the specified Tweet. Note that the status ID used should be the ID of an original Tweet and not a Retweet. Example: retweets_of_tweet_id:1539382664746020864  ","version":"Next","tagName":"h3"},{"title":"Effects of operators​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#effects-of-operators","content":" This table provides examples of how different operators behave when tweets are retweeted, replied to, or users are mentioned.  \tTweet from user1\tRetweet from user1 of a tweet from user2\tRetweet from user2 of a tweet from user1\tReply from user1 to a tweet from user2\tReply from user2 to a tweet from user1\tReply from user3 to a reply of user2 to a tweet of user1\tReply from user3 to a reply from user1 to a tweet of user2\tRetweet of user3 to the reply of user2 to the tweet of user1\tMention of user1 in a tweet of user2\tReply from user 3 to a tweet of user2 where user1 is mentioned\tRetweets of a tweet where user1 is mentionedOriginal tweet author\tuser1\tuser2\tuser1\tuser2\tuser1\tuser1\tuser2\tuser1\tuser2\tuser2\tuser2 conversation_id\tuser1 tweet\tuser2 tweet\tuser2 retweet\tuser2 tweet\tuser1 tweet\tuser1 tweet\tuser2 tweet\tuser1 tweet\tuser2 tweet\tuser2 tweet\tuser3 retweet from:user1\t✅\t✅\t❌\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌ from:user1 -is:retweet\t✅\t❌\t❌\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌ from:user1 -is:reply\t✅\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌\t❌\t❌ from:user1 -is:quote\t✅\t✅\t❌\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌ to:user1\t❌\t❌\t❌\t❌\t✅\t❌\t✅\t✅\t❌\t❌\t❌ @user1\t❌\t❌\t✅\t❌\t✅\t✅\t✅\t✅\t✅\t✅\t✅ @user1 -is:retweet\t❌\t❌\t❌\t❌\t✅\t✅\t✅\t❌\t✅\t✅\t❌ retweets_of:user1\t❌\t❌\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌\t❌ midterms user1 is candidate\t✅\t❌\t❌\t✅\t✅\t✅\t✅\t❌\t✅\t✅\t❌  ","version":"Next","tagName":"h3"},{"title":"Sources​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/docs/data-collection/03_00_platform-specific guidelines/twitter/twitter-rules#sources","content":" https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rulehttps://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-queryhttps://twitterdev.github.io/do_more_with_twitter_data/finding_the_right_data.htmlhttps://developer.twitter.com/en/docs/tutorials/building-high-quality-filters ","version":"Next","tagName":"h2"},{"title":"Getting started: Introduction to researching and monitoring online discourse","type":0,"sectionRef":"#","url":"/docs/get-started/","content":"Getting started: Introduction to researching and monitoring online discourse To help you get started and better navigate the research landscape of social media monitoring (SMM) and researching online discourse more broadly, we’ve collected a first set of chapters that aim to provide orientation and guidance to researchers and practitioners exploring various options and dimensions of independently researching social media. We welcome additional content and have added a selection of chapters below that would make for great contributions. Overview: Tools for working with digital trace data The purpose of this entry is to provide an overview of tools that can be used across the research data cycle when working with digital trace data: From data collection to preprocessing, analysis and visualization, reporting, and archiving and sharing data. Learn more Ethical considerations This chapter critically examines the ethical challenges inherent in independently researching social media and data sharing practices. Drawing on the Open Knowledge Foundation’s (OKFN) extensive experience in data ethics, it outlines a set of standards rooted in principles such as privacy, transparency, and accountability. Emphasis is placed on the significance of a privacy-centric design approach to promote ethical data management. Learn more Legal considerations This chapter provides an overview of the legal parameters that govern independent research of social media platforms. By presenting a clear outline of the legal context and incorporating relevant IT considerations, it provides a starting point for researchers and practitioners to ensure lawful engagement with these platforms. Learn more Open for contributions We welcome contributions on a rolling basis. At the moment, we particularly welcome chapters dealing with the following questions A one-page checklist for legal considerations in the EUData access rights beyond the European Union and the U.S.Guidelines for how to apply for API access and establish ethical oversightA checklist for civil society researchers to ensure organisational readiness to conduct ethical monitoring","keywords":"","version":"Next"},{"title":"Legal considerations","type":0,"sectionRef":"#","url":"/docs/get-started/01_03_legal-considerations","content":"","keywords":"","version":"Next"},{"title":"1. Useful legal concepts when analysing social media platforms​","type":1,"pageTitle":"Legal considerations","url":"/docs/get-started/01_03_legal-considerations#1-useful-legal-concepts-when-analysing-social-media-platforms","content":" Social media platforms are pervasive in our lives and deeply entangled with the exercise of some of our rights and many of the aspects of our public life. Yet, most of these platforms were not created with this in mind. On the one hand, we can use them to exercise our freedom of expression, our right of association or our freedom to conduct a business. On the other, we must not forget that by doing so we are implicitly trusting that their legitimate owners will be collaborative and supportive to everyone’s self-determination even though, legally speaking, they are not required to do so.  Problems connected to this circumstance can be found in contested practices such as “deplatforming” (1) users, which consists of removing and banning an individual or a group from a mass communication medium such as a social networking or blogging website. The case is similar for so called “shadow banning” (2), which is the practice of blocking or partially blocking a user or a user's content from some areas of an online community in such a way that the ban is not readily apparent to the user.  The decision on whether and how to enforce a ban from a social media platform can be taken unilaterally and with a lack of transparency because these spaces constitute private property, and it is up to the platform providers to decide who can access them and who cannot.  This very same logic applies when we look at the analysis of platforms for research purposes.  While law makers and regulators are still in the process of working towards a common set of rules for online platforms that would apply internationally, it is essential for you to be aware that different countries have their own approaches to regulating these platforms. This means that the way online platforms are managed can vary from one country to another. However, this variation should not overwhelm you. There are some fundamental principles that you can keep in mind when considering how these platforms are controlled.  It's important to recognize that the internet does not have strict, universal rules that every country follows. Instead, each nation has the authority to make its own regulations regarding online platforms. These rules can cover a wide range of topics, including how data is handled, what content is allowed and how businesses compete.  As a researcher of online platforms, it's a good idea to stay informed about the specific rules that apply in the country or countries where you are operating. This knowledge will help you navigate the digital landscape more effectively and responsibly. Additionally, understanding that while rules may differ, some core principles remain constant can serve as a compass for your online exploration.  What is the legal concept of a digital platform?  One of the key principles that underpin all legislative efforts in this field is the legal notion of “digital asset”, which is anything that exists only in digital form and comes with a distinct usage right or distinct permission for use. Digital assets encompass a wide range of items, such as software, photographs, logos, illustrations, animations, audiovisual content, presentations, spreadsheets, digital artwork, word documents, email messages, websites, and numerous other digital formats, along with their associated metadata.  From a legal standpoint, these kinds of assets represent an extension in the digital world of one's physical assets and virtual space being part of an individual's personal sphere that, as such, deserves legal protection.  Following this logic, online platforms are considered digital assets owned by private subjects to which a very ancient legal concept applies that since Latin time was known as ius excludendi alios, literally “the right to exclude others” (3). This concept refers to the fundamental property right that allows a property owner to exclude others from using or entering their property and to create a set of rules to be followed while staying inside of it. In essence, it is the right to control access to and use of one's property and is a key element of property ownership in many legal systems. It means that the property owner has the authority to determine who and under which conditions can access their property.  If you want to monitor and analyse social media platforms, notably without their consent, you need to be prepared that this could represent a violation of this “right to exclude others” and consequentially translate into civil or even criminal liability, if not both. Platform owners could decide to carry legal actions against people performing social media analysis or, at the very least, permanently ban them from the platform.  For the scope of this chapter, we will not delve into the ramifications of criminal liability, as matters of criminal law significantly vary from nation to nation within their specific legal systems. It is just worth specifying that often this form of legal responsibly involves the actual violation of security measures put in place on a given platform and, in most cases, the actual intentionality to temper with or damage the digital infrastructure (even if this is still highly debated among scholars).  What kind of civil liability can derive from platform analysis?  Another key principle that must be considered relates to the risk of civil liability, which is usually distinguished in contractual liability or tort liability. In brief, contractual liability arises from the breach of an agreement stipulated between parties, while tort liability is the responsibility to compensate for harm or injury caused to another person or their property through wrongful conduct outside of a contract.  In the field of social media platform analysis, you are most likely to be liable at a civil level if you breach the terms of services and/or user agreements, infringe on copyright, intellectual property, or trade secrets. In addition, you be held liable if you violate data protection law or the privacy of other users.  Examples of tort liability can cover a wide range of civil wrongs, such as the disruption of the service provided by a platform or the violation of its trade secrets. This can result in compensation or remedies for the damaged party.  The interesting part, however, is that there is much to learn about the ethics and the behaviour of big platforms also from the way they position themselves inside this vast and often convoluted legal context. In a certain sense, analysing their legal practices can be already revealing of the way they run their business.  The Digital Services Act allows so-called “vetted researchers” (4) to file for data access to conduct independent audits, which significantly increases the possibilities for public scrutiny of the digital activities on social media platforms. This research access is promising and important though the specifics of this procedure are yet to be decided. It will be interesting to see how platforms comply with and implement these new rules.  ","version":"Next","tagName":"h2"},{"title":"1.1 Terms of Services and User Agreements: What do they mean for the analysis of social media platforms?​","type":1,"pageTitle":"Legal considerations","url":"/docs/get-started/01_03_legal-considerations#11-terms-of-services-and-user-agreements-what-do-they-mean-for-the-analysis-of-social-media-platforms","content":" Every user who navigates the internet, whether using social media platforms, e-commerce marketplaces, streaming services, and others, will at some point be confronted with the famous “wall of text” of the online world: the Terms of Services (ToS).  These ToS govern our relationship with platforms, delineate our rights and impose restrictions that bear profound legal implications. They represent a significant portion of the conditions under which the owners of a certain platform are willing to allow other people to interact with their digital assets and, most importantly, constitute part of an agreement (a contract, in many ways) between user and platform. The work of demystifying these documents can help lawyers, scholars, and researchers in having a better understanding of the actions they are allowed to perform and sometimes also offer a reference point between what is supposed to happen on a platform and what is possible to observe.  At first glance, Terms of Services and user agreements may appear verbose, intricate, and dense. However, beyond the intricate legal jargon lies an interplay of restrictions and allowances that determine how platforms can be analysed, studied, and critiqued. In an age where digital platforms influence economies, democracies, and personal lives, understanding these limitations represents a precious strategic advantage.  One primary concern revolves around data access and the possibility of monitoring and understanding how the platform works. Platforms, through their ToS, often restrict how users can collect data, analyse it, or share it. For researchers aiming to study platform algorithms, behaviours or biases, these restrictions can severely impede transparency and understanding of the concrete underlying mechanisms that they are trying to observe. There is a delicate balance to strike between data ownership and platform integrity and the actual ability to study these platforms and hold them accountable in case of potential malpractices.  Can you use automated tools to analyse social media platforms?  Although it is always necessary to make important distinctions case by case, one general tendency that often characterises the prescriptions of terms of service imposed by social media platforms, is the prohibition for users to utilise automated tools when interacting with the digital infrastructure. Techniques like “web scraping” (5) or the usage of a “bot net” (6) are almost always forbidden by the ToS of the major social media platforms (More on webscraping in the chapter here). Researchers must therefore carefully weigh whether they resort to other methodologies to carry on their work.  One alternative may be the use of Application Programming Interface (API) (More on APIs in the chapters on Twitter and on TikTok) that many websites and online services provide directly to allow developers to access and retrieve data in a structured and standardised way. APIs are a more reliable and less ethically challenging method of gathering data compared to web scraping because they are designed for data exchange. Researchers can use programming languages like Python to interact with APIs and retrieve data.  Another solution can be to rely on public datasets (More on public datasets in the chapter here), which many organisations and institutions make publicly available for research and analyses so that these datasets can be used for various purposes for free. When possible, researchers may also purchase datasets from data providers and vendors, which provide datasets that are often clean, structured, and ready for analysis. Alternatively, there is always the solution of manual data entry; while it is the most time-consuming method, manual data entry involves copying and pasting information from web pages into a spreadsheet or database and can be suitable for small-scale data extraction tasks.  However, it is worth specifying that the use of automation in platform monitoring is not per se legally forbidden, notably in cases in which it is used to speed up the process of gathering publicly available data from a website and does not interfere with the regular functioning of a platform’s servers.  The legitimacy of techniques like scraping is contested. The research community may point to norms such as the Digital Services Act as a legal foundation, yet the legal argument is challenging and there are always ethical considerations that must be weighed carefully.  This goes to say that the use of automation aimed to analyse social media platforms can surely give ground, in those cases when the ToS forbids such practice, to the issuance of a ban from said platform. However, this will not necessarily translate into the insurgency of civil liability. After all, preserving the agreement between the user and the owner of a platform is something significantly different than interpreting and enforcing the law in a way that solely the jurisdictional power can do.  What to pay attention to when reading ToS?  It does not come as a surprise then, that these agreements often contain clauses pertaining to dispute resolution, often pushing disagreements into arbitration, and preventing class-action lawsuits. Such stipulations can limit the tools available for users and researchers to challenge or question platform actions, thereby creating potential power imbalances between individual users and tech-giants.  Yet, while these agreements impose limitations, they also provide insights. A meticulous examination of a platform's ToS can reveal much about its priorities, ethics, and business model. Is the platform user-centric or does it prioritise its economic objectives? Do its legal stipulations push for transparency or opacity? The answers to these questions may appear in the lines of user agreements and are essential for any comprehensive platform analysis.  The road ahead for lawyers and researchers is both challenging and fascinating. As we explore these agreements, we must develop methodologies to analyse platforms within their legal bounds while advocating for transparency, accountability, and fairness. Strategies may range from seeking amendments in these agreements to leveraging technology for compliant data collection and analysis.  ","version":"Next","tagName":"h3"},{"title":"1.2 Reflecting on Copyright, Intellectual Property and Trade Secret: Implications for Platform Analysis​","type":1,"pageTitle":"Legal considerations","url":"/docs/get-started/01_03_legal-considerations#12-reflecting-on-copyright-intellectual-property-and-trade-secret-implications-for-platform-analysis","content":" In the digital age, where ownership over immaterial goods grows in its importance and tireless innovation is driving progress, the rules about copyright, intellectual property (IP), and trade secrets become indicators of great significance. These are the rules upon which digital platforms are built and for anyone studying these platforms, it is crucial to understand how these legal instruments work together.  Why does copyright law matter in social media analysis?  The field of copyright law has undergone a profound transformation in the past decades. In this era of rapid information exchange, content is disseminated, reimagined, and redistributed at an unprecedented pace, presenting a myriad of challenges for platforms when it comes to ascertaining content ownership and navigating usage rights. This evolving landscape has given rise to intriguing questions, such as how one can attribute originality in a meme culture (7) and how to determine the bounds of fair use in an environment teeming with remixes and mashups.  A clear example of this comes from the many legal battles started against the practice of training generative artificial intelligence models using copyrighted material of renowned writers. As of today, for instance, there are three different lawsuits for copyright infringement lodged against the renowned company OpenAI (8).  For researchers, the intricate layers of copyright law wield a profound influence on how content on digital platforms can be utilised, analysed, and critiqued. The shifting landscape demands a nuanced understanding of copyright principles, such as the doctrine of fair use, which varies from jurisdiction to jurisdiction. This requires a careful navigation of the legal landscape to avoid the potential pitfalls of copyright infringement. Researchers must be vigilant in respecting the rights of content creators while pursuing their academic or investigative endeavours.  What can we learn about a platform from its intellectual property portfolio?  In the broad spectrum of legal rights pertaining intellectual property, it is possible to observe yet another dynamic aspect of the strategies implemented by big platforms.  IP rights provide a safeguard to platform's innovations, fortifying their position and guaranteeing the inviolability of their distinctive value propositions. Through this defensive role, platforms can enjoy a sense of security, knowing that their creative outputs and innovative solutions are shielded from unauthorised duplication, thereby preserving their exclusivity in the market and sustaining customer loyalty.  At the same time, intellectual property can also provide platforms with the means to assert dominance within market landscapes. With this offensive capability, platforms can strategically employ their IP rights to assert their presence and challenge competitors. By aggressively leveraging their patents, trademarks and copyrights, platforms can establish a formidable presence in their respective domains, effectively carving out their niches and dissuading rivals from encroaching on their territory. Moreover, IP rights grant platforms the authority to influence industry standards, regulatory frameworks and even the direction of the market, thereby solidifying their position as industry leaders.  For analysts dedicated to deconstructing platforms and their strategies, a comprehensive understanding of the platform's intellectual property portfolio proves indispensable. Such insight serves as a valuable observation point to the platform's inner workings, strategic priorities, and competitive positioning. By scrutinising the breadth and depth of a platform's IP holdings, researchers can discern the platform's focal points, for instance whether it is heavily invested in technological innovation or brand recognition. Additionally, the assessment of a platform's intellectual property portfolio unveils potential vulnerabilities, helping analysts identify areas where the platform might be exposed to competitive threats or imitation.  How relevant are platform's trade secrets?  Things are significantly different with trade secrets. Unlike patents, which require public disclosure and grant exclusive rights for a limited period, trade secrets have the role to protect a platform's core functionalities and innovations.  In the current technology-driven market, trade secrets serve as safeguard for the proprietary knowledge that allows big platforms to maintain their competitive advantage. These legal instruments protect a diverse set of elements that are vital for social media platforms, including their algorithms responsible for making their user experience as engaging and irresistible as possible; their proprietary data processing techniques that underpin automated decision-making and their distinctive business strategies.  Delving into the inner workings of a platform without violating its trade secrets is indeed a challenging mission. It requires a delicate balance between technical skills and legal prudence. The technical finesse required involves a deep understanding of complex algorithms and data processing methods, enabling analysts to infer a platform's functionality without direct access to its proprietary code. This often entails reverse engineering, complex data analysis and other advanced techniques that enable a comprehensive assessment of the platform's capabilities. Simultaneously, legal knowledge is paramount in ensuring that this exploration remains within the boundaries of the law. As trade secrets are legally protected, analysts must be equipped with a keen awareness of the limits imposed by trade secret laws and the need for ethical and lawful analysis.  Beyond the individual intricacies of copyright, IP and trade secrets, their convergence brings forth unique challenges and considerations. Platforms often operate in ecosystems where these legal constructs intertwine, creating an overlap of rights and restrictions that researchers must adequately navigate. Additionally, with platforms often operating across borders, the international dimensions of these laws add yet another layer of complexity.  ","version":"Next","tagName":"h3"},{"title":"1.3 Privacy and data protection law compliance in analysing social media platforms​","type":1,"pageTitle":"Legal considerations","url":"/docs/get-started/01_03_legal-considerations#13-privacy-and-data-protection-law-compliance-in-analysing-social-media-platforms","content":" At the very core of what represents the economic strength of large online platforms there is beyond any doubt the incredible quantity of personal data that they are capable to collect and process daily. This unprecedented power has been able to transform consolidated market dynamics on a global scale.  One pressing challenge faced by researchers and analysts, who delve into the task of understanding and examining these platforms is to be able to investigate this data-rich landscape while ensuring strict adherence to privacy and data protection laws.  Is data-protection law a tool or a limit?  A foundational understanding begins with recognising the importance of individual privacy rights in the digital context. In essence, every user's interaction with a social media platform is, in many jurisdictions, considered a piece of personal data and a fragment of one's digital identity. As such, the law meticulously guards against any unauthorised or non-compliant access, use, or dissemination of this data.  The General Data Protection Regulation (GDPR) (9) of the European Union and the California Consumer Privacy Act (CCPA) (10) in the United States stand as prominent examples of legal regimes that set rigorous standards for data protection. While the specifics may vary, the core principles remain consistent: personal data must be processed lawfully, transparently and for a specific purpose; it must be minimal, accurate and stored only as long as necessary; individuals have the right to access, correct or delete their data.  For platform analysts, these laws carve out both a roadmap and a minefield. A roadmap in the sense of providing structured guidelines on lawful data processing. A minefield because inadvertent non-compliance can lead to severe legal consequences, both in terms of penalties and reputational harm.  How can we use people's personal data in a safe and ethical way?  A particular issue pertains the topic of “informed consent”. When users sign up for social media platforms, they often provide consent for data collection regulated by terms of service or privacy policies. However, this consent may not extend to third-party analyses or research and analysts must, therefore, be careful in ensuring that their data access and processing methods either fall within the coverage of existing consents or have separate, explicit permissions.  In this regard, a very inspiring practice is represented by the “data donation” (11), which is the voluntary act of individuals or organisations contributing their data for research, social good or humanitarian purposes. This concept involves people or entities choosing to share their data in various ways to support different objectives such as the research on climate, healthcare, or social media platforms.  Additionally, the rise of 'anonymised' or 'de-identified' data offers both opportunities and challenges. While such data sets, stripped of personal identifiers, can potentially be used without infringing on individual privacy rights, questions arise about the true effectiveness of anonymisation techniques, especially given the sophistication of modern re-identification methods. Moreover, it is crucial for the anonymisation procedure to take place directly on the device of the interested data subject, as it would otherwise represent very likely an actual processing of personal data.  ","version":"Next","tagName":"h3"},{"title":"2. Considerations on lawful and crucial support from IT experts​","type":1,"pageTitle":"Legal considerations","url":"/docs/get-started/01_03_legal-considerations#2-considerations-on-lawful-and-crucial-support-from-it-experts","content":"   Once we have delineated the main legal profiles that shape and constrain the landscape of social media analysis, we can transition from the juridical outlines to the technical aspects connected with the previous considerations. In the digital era, law and technology influence each other at a growing pace and the same is true for legal experts and information technology specialists.  IT experts, in this context, emerge as both decipherers and architects that can unravel these intricate patterns, offering insights that surpass simple interactions with a platform. Their contributions can, for instance, manifest itself in the creation of customised tools specifically engineered to enhance the precision and efficiency of social media monitoring. These instruments often may consist in specific customised analysis tools capable of performing small tasks tailored for the needs of each project.  In this section of the chapter, we will discuss some key aspects of how to document and describe the analysis operations performed on a digital platform through a technical report. Moreover, we will ponder on how a strong technical expertise in the field of IT can be leveraged to even make the platforms want to be analysed it the first place.  ","version":"Next","tagName":"h2"},{"title":"2.1 Best practices in evidence gathering and drafting of technical reports​","type":1,"pageTitle":"Legal considerations","url":"/docs/get-started/01_03_legal-considerations#21-best-practices-in-evidence-gathering-and-drafting-of-technical-reports","content":" To sustain any form of argumentation, whether for a legal case, a journalistic inquiry or academic research, it is always of the utmost importance to be able to count on solid and consistent evidence. The same applies when analysing social media platforms, where you may need to master the art of digital forensics to gather evidence in legal and ethical manner.  Technical reports are integral components of the narratives necessary to be built around a meaningful inquiry and they must be meticulously crafted, ensuring clarity, credibility, and compliance with both the relevant legal framework and the best practices of the field.  There is a remarkable diversity of digital evidence in today's digital landscape, depending on the concrete scope of one’s research. From emails to server logs, social media interactions to encrypted messages, the digital footprint left by individuals and entities is vast and varied. Navigating this expansive domain necessitates rigorous protocols to ensure the authenticity, integrity, and admissibility of evidence.  The first and most important concept for a solid acquisition of digital evidence is the so called “chain of custody” (12), which is the order and way evidence has been handled in an uninterrupted process that can guarantee that is has not been altered. Establishing a clear chain of custody from the moment of data acquisition to its presentation is paramount and every transfer, storage or access point must be meticulously logged, ensuring that the evidence remains untainted and its origins traceable.  Preservation of the original state is key to insure the authenticity of the evidence. Tools like forensic disk imaging can capture exact replicas of digital media, preserving metadata and ensuring that the original source remains unaltered. For the selection of the best evidence gathering tools is convenient to pay special attention to the level of accuracy that they are capable of, as this will have a great impact on the credibility of our claims and their ability to stand against counterarguments.  The same level of care shall be given to the actual draft of the technical report that will describe the evidence and how the acquisition procedure looked like with a consistent level of clarity, comprehensibility, and precision. A technical report must be easily navigable, with a clear structure and layout in which there must be the description of (at least) the methodology used, findings obtained and conclusions derivable from them. The language used should ensure that even the readers unfamiliar with technical jargon can be able to follow the narrative.  Given that technical reports often find their way into legal settings where technical expertise may be limited, including a plain language summary in the introduction can bridge the gap and offer a concise and comprehensible overview of the report's key findings.  Detailing every step of the methodology used for the evidence gathering process, the characteristics of the tools used and the rationale behind each decision help in making the report reliable and allows for its reproducibility. Visual aids like graphs, charts and tables can simplify complex data sets, providing clear visual representations that enhance understanding. However, each visual aid must be accompanied by comprehensive captions and sourced appropriately to avoid the risks of misinterpretation.  Before finalising a technical report, it is advisable to subject it to peer review by external experts that can validate the methodology, highlight potential omissions, and ensure that the report stands up to scrutiny.  ","version":"Next","tagName":"h3"},{"title":"2.2 Ethical hacking and security research exemptions: Balancing vulnerability disclosure and platform integrity​","type":1,"pageTitle":"Legal considerations","url":"/docs/get-started/01_03_legal-considerations#22-ethical-hacking-and-security-research-exemptions-balancing-vulnerability-disclosure-and-platform-integrity","content":" There is one more way to go very deep into the analysis of large online platforms, or any digital infrastructure, and benefit from a sort of exemption that facilitate the research and reduces the risks of repercussions. It is called “ethical hacking” (13) and it consists in investigating a platform to identify vulnerabilities, bugs, and other defects without a malicious intent, but to improve the security and performance of a certain place in the cyberspace.  Guiding principle of an ethical hacker, also called “white-hat hacker” is to uncover weaknesses of a platform so they can be rectified. This is the core difference with the so called “black-hat hackers”, who seek vulnerabilities to exploit them or for other malevolent reasons.  Numerous jurisdictions have begun recognising the invaluable contributions of ethical hackers. Laws have evolved, creating exemptions for security research, ensuring that well-intentioned hackers do not find themselves inadvertently on the wrong side of the law. However, these exemptions come with conditions, ensuring the research does not compromise user data, platform stability or infringe on intellectual property.  In a collaborative stride, many platforms now host Vulnerability Disclosure Programs, inviting ethical hackers to identify and report vulnerabilities and these programs provide a structured framework, often with guidelines on safe testing, responsible disclosure, and rewards for researchers. These guidelines are very important because ethical hackers, for example when probing social media platforms, risk accessing user data or exceeding the scope of the research they were supposed to conduct. Striking a balance between research comprehensiveness and user privacy is critical and ethical hackers must employ techniques to test vulnerabilities without compromising or accessing real user data.  One of the pivotal dilemmas in ethical hacking is the timing of vulnerability disclosure. While immediate disclosure seems the most transparent approach, it might leave platforms exposed until a fix is implemented. Yet, delayed disclosure might provide ample remediation time but could risk undisclosed vulnerabilities being discovered and exploited by malicious actors.  It is worth noticing that different national jurisdictions can often have differing stances on ethical hacking and security research and, for this reason, understanding and navigating these legal intricacies is imperative to ensure this form of research remains compliant even when conducted across borders. As technology advances, the realm of ethical hacking will inevitably evolve and understanding the new trends can prepare platforms and researchers for forthcoming cybersecurity landscapes.  ","version":"Next","tagName":"h3"},{"title":"3. Navigating the legal terrain of social media monitoring​","type":1,"pageTitle":"Legal considerations","url":"/docs/get-started/01_03_legal-considerations#3-navigating-the-legal-terrain-of-social-media-monitoring","content":"   As we conclude this chapter, it is important to emphasise few steps that you may follow to ensure that your efforts in monitoring online discourse remain legally sound.  Before embarking on any social media monitoring endeavour, it is essential to acquaint yourself with the relevant laws and regulations governing online activities. Each jurisdiction may have its own set of rules governing data privacy, intellectual property, defamation and more. Additionally, meticulously review the Terms of Service (ToS) of the social media platforms you intend to monitor. These agreements often contain provisions addressing data usage, scraping, and content sharing, which can significantly impact your monitoring activities. Seek the assistance of a lawyer or a legal expert that you trust, as you start detailing the framework of your research.  Respect for user privacy is a cornerstone of responsible social media monitoring. When collecting and analysing data that involves individuals, seek informed consent whenever possible. This not only aligns with ethical research practices but also mitigates potential legal concerns. Furthermore, when sharing or presenting your findings, take diligent steps to anonymize data and protect the identities of users, unless explicit consent has been granted for their inclusion.  Important note It is always advisable to seek guidance from a lawyer or a legal expert knowledgeable of IT law and the specific characteristics of the legal system applicable in the jurisdiction in which you will operate. This can be paired with the invaluable contribution of technology experts.  As researchers exploring the expansive realm of social media monitoring, you have the fundamental role to uncover insights, influence discussions and contribute to the collective understanding of online discourse. This is a power that comes with great responsibility, and your commitment to adhering to legal and ethical principles is paramount. By being aware of the current context described in this chapter, we can navigate the legal terrain of social media monitoring with confidence ensuring that our research remains both legally sound and ethically responsible.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Legal considerations","url":"/docs/get-started/01_03_legal-considerations#references","content":" (1) A. Mekacher1, M. Falkenberg and A. Baronchelli, The Systemic Impact of Deplatforming on Social Media, 2023  (2) https://www.nytimes.com/interactive/2023/01/13/business/what-is-shadow-banning.html  (3) https://www.oxfordreference.com/display/10.1093/acref/9780195369380.001.0001/acref-9780195369380-e-1127  (4) https://digitalservicesact.cc/dsa/art31.html  (5) V. Singrodia, A. Mitra, S. Paul, A Review on Web Scrapping and its Applications, 2019  (6) N. Kaur, M. Singh, A Review on Web Scrapping and its Applications, 2016  (7) S. Blackmore, The Meme Machine, published by Oxford University, 2000  (8) https://www.reuters.com/technology/more-writers-sue-openai-copyright-infringement-over-ai-training-2023-09-11/  (9) https://eur-lex.europa.eu/eli/reg/2016/679/oj  (10) https://oag.ca.gov/privacy/ccpa  (11) J. Ohme, T. Araujo, Digital data donations: A quest for best practices, 2022  (12) M.N.O. Sadiku, A.E. Shadare,S.M. Musa, Digital Chain of Custody, 2017  (13) Hafele, Three Different Shades of Ethical Hacking: Black, White and Gray, 2021 ","version":"Next","tagName":"h2"},{"title":"Overview: Tools for working with digital trace data","type":0,"sectionRef":"#","url":"/docs/get-started/01_01_overview","content":"","keywords":"","version":"Next"},{"title":"What is digital trace data?​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#what-is-digital-trace-data","content":" Online discourse can be researched with a variety of methods and types of data. A widely used data type in this domain is digital trace data. These can be defined as &quot;'records of activity (trace data) undertaken through an online information system (thus, digital)' (Howison et al., 2011, p. 769) and can be collected from a multitude of technical systems, such as websites, social media platforms, smartphone apps, or sensors&quot; (Stier et al., 2020, p. 504). Social media data is, in large part, digital trace data but, as this definition shows, the category is broader. From this definition, it is also apparent that there are different ways in which digital trace data can be further categorized. One way is distinguishing between sources of digital trace data, such as social media platforms, websites, or sensors. Another way is to categorize the data based on the type of information they contain. For example, social media data can be categorized based on their modality (text, image, audio, video) or their structure (e.g., network or metadata) as well.  On a more abstract level, Menchen-Trevino (2013) makes a distinction between participation traces (e.g., comments or posts) and transactional data (e.g., login data or logs more generally). Similarly, Hox (2017) differentiates between intentional and unintentional traces. Regarding the scope of the data, Menchen-Trevino (2013) further distinguishes between horizontal (e.g., all posts from a social media platform containing a specific hashtag) and vertical trace data (comprehensive usage data - potentially from different sources - for a limited group of users). The Data Knowledge Hub section on &quot;Social media data types&quot; combines some of these categorizations by dividing social media data into content data, interaction data, user data, and metadata (plus three subtypes for each of those).  ","version":"Next","tagName":"h2"},{"title":"A variety of available tools​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#a-variety-of-available-tools","content":" Regardless of the specific type, when working with digital trace data we make use of different tools. Typically, we work with combinations of tools or tool stacks. The purpose of this entry is to provide an overview of tools that can be used across the research data cycle when working with digital trace data: From data collection to preprocessing, analysis and visualization, reporting, and archiving and sharing data. In addition to tools that can be used specifically for one of those steps, the next section also discusses general-purpose tools that can be used for different phases of the research data cycle. After introducing the different tool options, this entry ends with an outlook on the new world of AI(-based) tools and suggestions for some criteria for choosing the &quot;right&quot; tools.  A few things should be noted when reading this entry:  This entry is meant to provide a broad overview of tools. It does not provide any in-depth introductions. There are other entries here in the Data Knowledge Hub that provide more in-depth introductions to specific methods and/or tools, and there are hundreds of introductions and tutorials on the tools mentioned in this entry. For working with digital trace data in general, the GESIS Guides to Digital Behavioral Data can offer some further guidance, covering various aspects, e.g., related to data collection or data management and ethics.Most of the tools discussed in the following are not specifically created for working with digital trace data or studying online discourse. They can be used for a variety of purposes. However, they are suitable options for being included in a tool stack for research (on online discourse) with digital trace data.The tools are not listed in any particular order, and the list is not exhaustive.The focus is on open-source tools, but some commercial tools are also included.  ","version":"Next","tagName":"h2"},{"title":"General-purpose tools​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#general-purpose-tools","content":" Research using digital trace data (on online discourse as well as other topics) typically involves writing code for one or more of the steps in the research data cycle, most commonly for data preprocessing, analysis, and visualization. The most widely used programming languages for this are Python and R. Both languages have many packages and libraries that can be used for a wide range of tasks across the research data cycle. This makes them highly extensible and versatile.  When working with R, Python, or any other programming language, it makes sense to use an integrated development environment (IDE) that provides an interface for writing and executing code. There are many different IDEs available. The most widely used one for R is RStudio. Its potential successor Positron also offers full support for Python. A popular IDE for Python is PyCharm, and another widely used options that works with and offers specific extensions for many programming languages (including R, and Python) is Visual Studio Code.  Another general-purpose tool that is especially helpful with regard to transparency and reproducibility is the version control system git. As for R and Python, there are plenty of introductions to git available online, but a good starting point is the official documentation. git is particularly useful for sharing research output (especially code) and collaborating with others if used in combination with a public git hosting service. The most popular one is GitHub and another popular option is GitLab. A very comprehensive resource focusing on the use of git and GitHub in combination with R and RStudio is the online guide &quot;Happy Git and GitHub for the useR&quot;.  Moving from the general to the more specific, in the following, I will provide an overview of tools that can be used for the different steps in the research data cycle. Notably, many tools can be used for several steps. In such cases, the respective tools will be discussed in the section that is most relevant for their main purpose. Another thing to take into account is that the distinction between the different steps is not always clear-cut. For example, data collection and data preprocessing are often closely intertwined. It also depends on the specific research question or use case whether a tool is used for data collection, preprocessing, or analysis and visualization.  ","version":"Next","tagName":"h2"},{"title":"Data collection​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#data-collection","content":" There are different options for collecting digital trace data and all of those have their own pros and cons (Breuer et al., 2020; Ohme et al., 2023). The most widely used options in research with digital trace data in general and research on online discourse in particular are APIs (application programming interfaces) and web scraping. However, there are also other options, such as reusing existing data(sets), direct collaborations with platforms, or data donations from users.  There are several entries within the Data Knowledge Hub that provide detailed guidance on some specific methods for collecting digital trace data. API access, (web) scraping, and data donations are discussed in the Data Knowledge Hub entry on &quot;Common data collection methods on social media platforms illustrated with TikTok&quot;. Another entry on &quot;Data Collection on X (Twitter)&quot; is an in-depth guide on how to collect data from X (Twitter) using the APIs offered by the platform, and there also is a section on &quot;Webscraping techniques with R&quot;. Besides that, the aforementioned GESIS Guides to Digital Behavioral Data series also include guides on collecting data via APIs and web scraping in general, as well as for specific platforms, such as YouTube or Google Trends.  Different types of data and different collection methods require different tools. There are various collections and overviews of tools for social media data. While these lists can quickly become outdated because tools are not maintained anymore, new tools are developed, or the platforms that the tools are created for change their APIs, they can still be helpful for finding, comparing, and choosing data collection tools. Typically, these lists also provide some criteria that can be used for selecting tools. The guide by Deubel et al. (2023), e.g., provides information on what platform(s) the tool can be used for, whether it makes use of an API, requires programming skills (and in what language), offers a graphical user interface (GUI) and analysis features, and whether it is free and open source (FOS) or not. Other tool overviews are provided in the Wiki of the Social Media Observatory at the Leibniz Institute for Media Research, the list of Social Media Research Tools by Brandon Silverman and Chris Miles, or the Social Media Research Toolkit by the Social Media Lab at Ted Rogers School of Management, Toronto Metropolitan University.  Besides choosing the appropriate tool(s) for collecting digital trace data, it is also important to consider legal aspects as well as the ethical implications of data collection. The relevant legal and ethical considerations for working with social media data are discussed in detail in the respective sections within the Data Knowledge Hub. For digital trace data in general, the guide by Breuer et al. (2025) gives an overview and some recommendations for ethical questions related to doing research with this type of data.  ","version":"Next","tagName":"h2"},{"title":"Data preprocessing​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#data-preprocessing","content":" What preprocessing steps are required strongly depends on the type(s) of data being used. This, of course, also determines which tools are appropriate for the task. For textual data, the Data Knowledge Hub entry on &quot;How to analyse social media data&quot; provides a good overview and also points to some further resources. Sticking with the example of text data, there are many packages and libraries available for text mining and natural language processing (NLP) in R and Python (as well as other programming languages). A very powerful and widely used NLP library for Python is spaCy. There also is an R implementation of spaCy called spacyr. spacyr is part of the Quanteda ecosystem, which is a very popular family of packages for the processing and quantitative analysis of textual data in R. Another widely used R package for text mining is tidytext.  The analysis of audio, image, or video data requires other or additional preprocessing steps. A common approach is to transform such data into textual data via speech recognition or computer vision tools. A popular speech recognition model is Whisper by OpenAI that can be used in Python, via a command line tool, or an independently developed R package. There are also many tools and libraries for image and video analysis. For Python, OpenCV is a widely used library. For R, an interesting new package for image analysis that makes use of large language models (LLMs) is kuzco. While the landscape of tools and libraries for image and video analysis is quickly changing, the book on images as data by Webb Williams et al. (2020) still provides a good introduction.  ","version":"Next","tagName":"h2"},{"title":"Data analysis & visualization​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#data-analysis--visualization","content":" As for preprocessing, what types of analysis and visualization are possible and make sense heavily depends on a) the specific type(s) of data being used and b) the respective research question(s). For textual data, common analysis methods, e.g., include supervised machine learning or topic modeling (see the &quot;Overview: How to analyse social media data&quot; for an introduction to these as well as other methods for text analysis). The other data analysis projects included in the Data Knowledge Hub provide further examples of analysis methods for social media data, such as social network analysis and hashtag analysis.  Notably, many of the tools and libraries/packages mentioned in the previous two sections can also be used for analyzing and visualizing digital trace data, but there are also many other options, especially for Python and R. Regarding data visualization in general, two very popular and versatile options are matplotlib for Python and ggplot2 for R. A very helpful resource for choosing the right data visualization that also provides code examples for both Python and Ris the website From Data to Viz. For analyzing and visualizing network data, which is also widely used in research on online discourse, two widely used options are Gephi and igraph, which also offers libraries for Python and R.  ","version":"Next","tagName":"h2"},{"title":"Reporting​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#reporting","content":" There are many digital tools and pieces of software that can be used for reporting research (on online discourse) with digital trace data. Given the complexity of the data and methods commonly used in this field, two particularly suitable formats are notebooks and dashboards. Notebooks represent a popular type of literate programming framework that combines text, code, and its output (e.g., figures) in one document. Dashboards are interactive web applications that allow users to explore data and results in a more dynamic way. The following table provides an overview of some of the most popular tools for creating notebooks and dashboards.  \tFormat(s)\tSupported languages\tCostsQuarto\tNotebook, Dashboard\tPython, R, Julia, Observable JavaScript\tFOS1 Jupyter\tNotebook\tJulia, Python, R\tFOS2,3 RMarkdown\tNotebook, Report\tR\tFOS1 marimo\tNotebook\tPython, SQL\tFOS1 Shiny\tDashboards, Web Apps\tR, Python\tFOS1 Google Colab\tNotebook\tFocus on Python, but several others work as well (including R)\tFreemium Observable\tNotebook, Dashboard\tObservable JavaScript\tPaid hosting via ObservableHQ Tableau\tDashboard\tGUI and proprietary visual query language (VizQL) but offers integration with SQL, Python, R\tCommercial  Notably, as is the case also for all other sections, this list is not exhaustive. There are many more tools and services for creating (and publishing) notebooks, dashboards or 'reproducible reports', especially commercial ones (Curvenote is one example). Some of the tools (including a few from the table above) have also started integrating AI-based features (e.g., Deepnote) (note: AI-based tools will be discussed in more detail in a later section of this entry).  ","version":"Next","tagName":"h2"},{"title":"Archiving & sharing data​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#archiving--sharing-data","content":" One or more reports in the form of an article, blog post, and/or an interactive notebook or dashboard (using one of the tools mentioned in the previous section) is the usual output of any research project. However, there also are other research products that can and should be shared. Apart from the code that can, e.g., be shared via GitHub, another important product is the data that is collected and analyzed. Archiving and sharing them is important to ensure/increase transparency and reproducibility of research.  As for the phase of data collection, ethical and legal questions become especially relevant when it comes to archiving and sharing digital trace data. In addition to the resources mentioned before, for archiving and sharing digital trace data (and social media data in particular) the book chapter by Bishop and Gray (2017) provides some general guidance. It is important to clarify whether or how the data can (legal) and should (ethical) be shared. However, in most cases, even if the full/raw data cannot be shared, it should be possible to at least share a processed or aggregated version of the data that allows for a reproduction of the reported analyses.  There are many options to choose from for archiving and sharing research data. The Registry of Research Data Repositories (re3data) is a dedicated search engine for finding research repositories. There are different criteria that can be used to search for and choose a suitable repository for your research data. There are general ones (e.g., the OSF, Zenodo, or the Harvard Dataverse) as well as discipline-, country- or data-type-specific repositories. Depending on your needs, some criteria to consider could be where the archive is situated, if it focuses on specific types of data, whether it uses persistent identifiers (such as DOIs), or if it allows for access control. Klein et al. (2018) suggest some further criteria for choosing a repository for archiving and sharing research data in general.  Regardless of the chosen repository, the data should ideally be shared in a way that is consistent with the FAIR data principles (Wilkinson et al., 2016), meaning that the data are findable, accessible, interoperable, and reusable. In addition to choosing a suitable repository, this also brings requirements with regard to the format and documentation of the data (see Breuer et al., 2021 for a discussion of this for social media data).  ","version":"Next","tagName":"h2"},{"title":"Outlook: AI(-based) tools​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#outlook-ai-based-tools","content":" The advent of large language models (LLMs) and generative AI (genAI) also has a major impact on tools for research with digital trace data. There are AI(-based) tools that can be used for (almost) every task in the research process, even going beyond the phases included here (e.g., also for searching and summarizing relevant literature). As with other tools, some AI(-based) tools are general-purpose, while others are specifically designed for certain tasks (Breuer, 2023).  Since research with digital trace data typically involves writing code for one or more steps in the research process, coding assistance tools like GitHub Copilot4, Claude Code by Anthropic, or Gemini Code Assist by Google can be helpful. LLMs are also becoming increasingly popular for preprocessing and analyzing textual data (Törnberg, 2024) but also image and video data (Jo et al., 2024). The use of LLMs for text analysis is also discussed in the &quot;Overview: How to analyse social media data&quot;.  Given the speed of the development in this area, it is hard to keep track with new models and tools being released almost daily. However, the still fairly frequently maintained list &quot;AI Tools for Research Workflow in Academia&quot; by Niels Van Quaquebeke can serve as a good overview and starting point.  One thing that should generally be noted when using AI(-based) tools is that while they can be very useful and facilitate steps in the research process, they are also associated with specific challenges. Besides potential costs, the most notable ones are intransparency, (data) privacy, and a possible (increased) dependency on commercial providers.  ","version":"Next","tagName":"h2"},{"title":"Choosing the right tools​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#choosing-the-right-tools","content":" As should be evident from the previous sections, there is a large variety of tools to choose from for research (on online discourse) with digital trace data (and the combination possibilities are practically endless). There are, however, some important criteria to consider when choosing the right tools for your project. In general, there are three main aspects to consider:  The specific task(s) that the tools are meant to accomplish: For example, if you want to collect data from a specific social media platform, you need to choose a tool that is able to do so. Available resources. This relates to the time and money you have available for your project but also skills and expertise. Some tools are free, while others require a subscription or a one-time payment. Some tools are easy to use, while others require more technical knowledge. Requirements. There may be requirements from employers, clients, or collaborators that need to be considered. Some institutions may require the use of specific tools or solutions for data processing, analysis, or storage. In addition, there may be legal requirements that need to be considered when choosing tools, such as data protection regulations.  Since we typically work with combinations of different tools (tool stacks), another relevant aspect to consider is the compatibility of the tools. Some tools are designed to work together, while others may require additional steps to integrate them.  Finally, to increase the trustworthiness of research results, transparency and reproducibility are important aspects to consider when choosing tools. According to the glossary of the Framework for Open and Reproducible Research Training (FORRT), transparency means &quot;[h]aving one’s actions open and accessible for external evaluation&quot;. And The Turing Way defines &quot;reproducible research as work that can be independently recreated from the same data and the same code that the original team used.&quot; In general, FOS tools are better suited for ensuring transparency and reproducibility. Another important aspect in this regard is documentation, both of the tools themselves as well as the ways in which they are used in specific projects.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Overview: Tools for working with digital trace data","url":"/docs/get-started/01_01_overview#references","content":" Bishop, L., &amp; Gray, D. (2017). Chapter 7: Ethical Challenges of Publishing and Sharing Social Media Research Data. In K. Woodfield (Ed.), Advances in Research Ethics and Integrity (Vol. 2, pp. 159–187). Emerald Publishing Limited. https://doi.org/10.1108/S2398-601820180000002007 Breuer, J. (2023). Putting the AI into social science – How artificial intelligence tools are changing and challenging research in the social sciences. In A. Sudmann, A. Echterhölter, M. Ramsauer, F. Retkowski, J. Schröter, &amp; A. Waibel (Eds.), Beyond Quantity. Research with Subsymbolic AI (pp. 255–273). transcript. https://www.transcript-open.de/pdf_chapter/bis%206999/9783839467664/9783839467664-014.pdf Breuer, J., Bishop, L., &amp; Kinder-Kurlanda, K. (2020). The practical and ethical challenges in acquiring and sharing digital trace data: Negotiating public-private partnerships. New Media &amp; Society, 22(11), 2058–2080. https://doi.org/10.1177/1461444820924622 Breuer, J., Borschewski, K., Bishop, L., Vávra, M., Štebe, J., Strapcova, K., &amp; Hegedűs, P. (2021). Archiving Social Media Data: A guide for archivists and researchers. https://doi.org/10.5281/zenodo.6517880 Breuer, J., Stier, S., Lukito, J., Mangold, F., Wieland, M., &amp; Radovanovic, D. (2025). Overview of Ethical Considerations when Working with Digital Behavioral Data (No. 14; GESIS Guides to Digital Behavioral Data). https://www.gesis.org/fileadmin/admin/Dateikatalog/pdf/guides/14_Breuer_et_al._Overview_Ethics_DBD.pdf Deubel, A., Breuer, J., &amp; Weller, K. (2023). Collecting Social Media Data: Tools for Obtaining Data from Social Media Platforms (No. 1; Navigating Research Data and Methods). Center for Advanced Internet Studies. https://www.cais-research.de/wp-content/uploads/Collecting-Social-Media-Data.pdf Howison, J., Wiggins, A., &amp; Crowston, K. (2011). Validity Issues in the Use of Social Network Analysis with Digital Trace Data. Journal of the Association for Information Systems, 12(12), 767–797. https://doi.org/10.17705/1jais.00282 Hox, J. J. (2017). Computational Social Science Methodology, Anyone? Methodology, 13(Supplement 1), 3–12. https://doi.org/10.1027/1614-2241/a000127 Jo, C. W., Wesołowska, M., &amp; Wojcieszak, M. (2024). Harmful YouTube Video Detection: A Taxonomy of Online Harm and MLLMs as Alternative Annotators (Version 1). arXiv. https://doi.org/10.48550/ARXIV.2411.05854 Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., Nilsonne, G., &amp; Frank, M. C. (2018). A practical guide for transparency in psychological science. Collabra: Psychology, 4(1). https://doi.org/10.1525/collabra.158 Menchen-Trevino, E. (2013). Collecting vertical trace data: Big possibilities and big challenges for multi-method research. Policy &amp; Internet, 5(3), 328–339. https://doi.org/10.1002/1944-2866.poi336 Ohme, J., Araujo, T., Boeschoten, L., Freelon, D., Ram, N., Reeves, B. B., &amp; Robinson, T. N. (2023). Digital Trace Data Collection for Social Media Effects Research: APIs, Data Donation, and (Screen) Tracking. Communication Methods and Measures, 18(2), 124–141. https://doi.org/10.1080/19312458.2023.2181319 Stier, S., Breuer, J., Siegers, P., &amp; Thorson, K. (2020). Integrating Survey Data and Digital Trace Data: Key Issues in Developing an Emerging Field. Social Science Computer Review, 38(5), 503–516. https://doi.org/10.1177/0894439319843669 Törnberg, P. (2024). Best Practices for Text Annotation with Large Language Models. Sociologica, 18(2), 67–85. https://doi.org/10.6092/ISSN.1971-8853/19461 Webb Williams, N., Casas, A., &amp; Wilkerson, J. D. (2020). Images as Data for Social Science Research: An Introduction to Convolutional Neural Nets for Image Classification (1st ed.). Cambridge University Press. https://doi.org/10.1017/9781108860741 Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1), 160018. https://doi.org/10.1038/sdata.2016.18    Footnotes​ Depending on what solution you choose for that, there may be costs for publishing/hosting the output (online). ↩ ↩2 ↩3 ↩4 JupyterLab offers the equivalent of an IDE for Jupyter notebooks. ↩ Binder offers an option for the free hosting of reproducible analyses using Jupyter notebooks. ↩ GitHub Copilot can also be easily integrated into RStudio and VS Code. ↩ ","version":"Next","tagName":"h2"},{"title":"Webscraping techniques with R","type":0,"sectionRef":"#","url":"/docs/data-collection/03_03_web-scraping-intro","content":"","keywords":"","version":"Next"},{"title":"Current challenges of webscraping​","type":1,"pageTitle":"Webscraping techniques with R","url":"/docs/data-collection/03_03_web-scraping-intro#current-challenges-of-webscraping","content":" Numerous social networks recently decided to seal off their APIs more strongly and, for example, to offer them only in exchange for payment (or not at all). This increasingly limits the possibility for scientists to systematically capture important parts of digital discourse and to examine its influence on society. The lasting consequences of such decisions are expected to become more evident in the upcoming months and years, yet an immediate effect is already observable: The relevance of webscraping is on the rise again. As a means to replace the sealed-off APIs, many programmers, especially with regard to X (Twitter), resorted to systematically retrieving data via the X website – apparently at such a high frequency that Elon Musk’s X (Twitter) decided to put all of the website’s posts behind a login wall, making it more difficult to capture X’s (Twitter) data via scraping (Binder 2023).  This is a step back: In the past, many social platforms offered an API with (often) free quotas precisely to avoid the burden of webscraping on servers (Khder 2021, p.147). Moreover, webscraping comes with a variety of ethical, technical and legal concerns and questions (see Khder 2021). Even if one is only interested in a fraction of the publicly accessible data, webscraping requires capturing the data of the entire website instead of merely extracting the relevant information: Thus, significantly more data is collected than is necessary, for example, to provide a scientific answer to a hypothesis. Images are loaded, clicks are simulated – allowing the reconstruction of individual user behavior.  While this chapter shows you the ropes of responsible webscraping, it is critical to emphasize that is primarily the providers of social networks and websites who are called upon to offer the interfaces to their data for civil society and science in a suitable manner – after all, their online presence significantly shapes public discourse. This influence must be critically monitored and analyzed, which is why monitoring for research purposes must be supported. Currently many platform operators are resisting this responsibility and the need for transparency – let’s hope the Digital Services Act will improve that situation by forcing platform operators to enable data access (see EDMO 2023; AlgorithmWatch 2023).  ","version":"Next","tagName":"h2"},{"title":"What you will learn in this chapter​","type":1,"pageTitle":"Webscraping techniques with R","url":"/docs/data-collection/03_03_web-scraping-intro#what-you-will-learn-in-this-chapter","content":" This chapter therefore aims to highlight three things:  how to access such content, using concrete examples;how to deal with scraped data within the scope of one’s own responsibility to prevent overloading web servers andto access data responsibly and according to modern standards.  To collect data via webscraping (1), you will learn how to use the programming language R and the library rvest, using the tidy principles Wickham 2014. Intended for beginners as well as advanced programmers, this is meant to make the code presented here easy to access and analyze. Additional references, links and tutorials are included throughout.  ","version":"Next","tagName":"h3"},{"title":"Webscraping: The current state​","type":1,"pageTitle":"Webscraping techniques with R","url":"/docs/data-collection/03_03_web-scraping-intro#webscraping-the-current-state","content":" Meanwhile numerous packages for programming languages are available, which simplify the collection of data significantly. In the Python world Beautiful Soup is one of the most important program libraries, in the R universe rvest takes a key role: Both libraries allow you to capture data from static web pages and extract elements from it.  note Other programming languages and associated libraries also enable comprehensive web scraping -- for example, Puppeteer is a good way to capture both static and dynamic web pages via Node.js. For the purposes of this article, we will take a comprehensive look at the rvest package and its capabilities -- and also point out options when, for example, pages do not have static elements but should still be captured. For an example of how to use puppeteer see this chapter in the knowledge hub.  In this chapter, you will be introduced to the rvest package and its capabilities – and learn about options on how to scrape dynamic pages.  To do so, we will use two sites as examples: the pro-Russian disinformation website Anti-Spiegel.ru of conspiracy ideologist Thomas Röper (see Balzer 2022) and the conspiracy ideology media portal AUF1 (see Scherndl and Thom 2023) run by right-wing extremist Stefan Magnet. While Röper’s Anti-Spiegel is a static website (operated with Wordpress) – i.e. a website which is rendered by the server – AUF1 is a website which, like many other modern websites, is only partially rendered and only fully loaded in the browser.  Note An overview of the different principles of web content delivery can be found in this article.  Both examples are well suited to demonstrate different ways of scraping with rvest. Content-wise, they influence online discourse with anti-democratic worldviews and disinformation regarding, for example, the Russian war on Ukraine or the topic of vaccinations and pandemics. Their content continues to influence (at least parts of) society in the German-speaking world. Therefore, they not only serve as an example for data collection, but also show that it is important to deeply investigate such platforms from a societal cohesion perspective.  ","version":"Next","tagName":"h3"},{"title":"Diving into a pro-Russian disinformation world​","type":1,"pageTitle":"Webscraping techniques with R","url":"/docs/data-collection/03_03_web-scraping-intro#diving-into-a-pro-russian-disinformation-world","content":" The Russian propagandist Thomas Röper published numerous articles on his website in which he belittles the consequences of, for example, the Russian war of aggression against the Ukraine and denies war crimes (see Journalist 2022). He not only writes his own articles, but also publishes news reports from the Russian news agency TASS and the Kremlin’s media outlet Russia Today. To determine which domains are cited particularly frequently, these sections will provide guidance on the necessary steps. Additionally, potential challenges during the scraping process will be addressed.  library(rvest) # Several packages of the **tidyverse** will be used # for example `dplyr` for data manipulation or `ggplot` to visualize the data we accessed. # Therefore, the whole tidyverse is being loaded. library(tidyverse) # Glue is used for better data annotations in the graphs. This is optional. library(glue) page &lt;- read_html(&quot;https://www.anti-spiegel.ru/category/aktuelles/&quot;)   On his website we can find a link with which we can access all his articles. rvest can make the data usable for our further analysis. Via read_html we can read a page, then we can access different elements of the web page via the function html_elements(). The easiest way is to use so called CSS selectors to choose the section which should be extracted: CSS selectors are different patterns which can be used to access different elements of an HTML page.  Note An overview of different CSS selector patterns can be found at W3Schools. To make it easier to choose the right selectors, you can use the Google Chrome extension SelectorGadget: By simply selecting and deselecting on a web page, the right CSS selector for the right segment can often be found quickly, as in the example of the title of each article here. Selecting the title and deselecting the ticker shows, that currently 20 titles are being highlighted using the selector image-left .title  Selectorgadget example on Röpers page  Using the CSS selectors, you can now determine the segments that are relevant to you – afterwards, you will write a function that will store all the relevant information in one record. rvest also has useful support functions here, such as html_text(). This function allows you to convert html types into simple text vectors; html_text2() extends this function and additionally removes unnecessary white spaces.  Hint You can find a good introduction to rvest and documentation on its main page.  The most important functions for this examples case are html_elements() and html_attr() – they allow you to extract elements and attributes (for example, the link from the href attribute of a &lt;a&gt; node). rvest makes use of many functions of the xml2 package, so it’s worth having a look at its documentation.  Note For example, html_structure() from xml2 is useful to get an overview of the structure of a web page.  page %&gt;% html_elements(&quot;.image-left .title&quot;) %&gt;% html_text()    [1] &quot;Der manipulierte Whistleblower-Bericht&quot; [2] &quot;Die UNO als Instrument des Westens&quot; [3] &quot;Der Bevölkerungsrückgang macht die Ukraine wirklich zum „Land 404“&quot; [4] &quot;Wem ist nichts mehr heilig?&quot; [5] &quot;Was Spiegel-Leser über das Treffen von Lukaschenko und Putin (nicht) erfahren&quot; [6] &quot;Droht eine Eskalation zwischen den USA und Russland?&quot; [7] &quot;Die Geschäfte der Bidens in der Ukraine&quot; [8] &quot;Der Schwarze Peter geht an Kiew&quot; [9] &quot;Käuferin von Hunter Bidens Bildern bekommt einen Posten in der US-Regierung&quot; [10] &quot;Internetkonzerne, KI und totale Zensur und Kontrolle&quot; [11] &quot;Estland verbietet Feiern zum Jahrestag der Befreiung von Nazi-Deutschland&quot; [12] &quot;Die Bidens: Eine schrecklich nette Familie&quot; [13] &quot;Die Versuche der USA, die NATO auf den Pazifik auszudehnen&quot; [14] &quot;Die BRICS laden 70 Staatschefs ein, aber niemanden aus dem Westen&quot; [15] &quot;Mein neues Buch ist jetzt im Handel&quot; [16] &quot;Putin schreibt einen Artikel über die Beziehungen zu Afrika&quot; [17] &quot;Wie die US-Demokraten die Korruptionsskandale des Biden-Clans zu verschleiern versuchen&quot; [18] &quot;Was wird aus dem Getreideabkommen?&quot; [19] &quot;Ukrainische Gegenoffensive laut Putin gescheitert: Die Ereignisse des Wochenendes&quot; [20] &quot;Unsere Fahrt an die Front in Saporoschschje&quot;   In most cases, not all content relevant to the investigation can be found on one page but will be distributed over several pages. In this example, you can find over 200 pages that can be extracted. Either way, you can either manually compile the pages you want to scrape or extract them automatically and reproducibly from the overview page.  Next page example on Röpers page  Using the CSS selector .page-numbers we can extract all page numbers - however, here we also extract the Next navigation object. To extract the relevant information from these objects (here for example the last number) you must use regular expressions. Regular expressions are certain terms and symbols that search for patterns in a text, in the following example a number that is not followed by another number.  A useful extension for R is Regexplain. This add-on for RStudio allows to test various regular expressions directly in the interface. Many tips and tools can also be found online, for example regex101.  last_page_nmbr &lt;- page %&gt;% html_elements(&quot;.page-numbers&quot;) %&gt;% html_text() %&gt;% # Collapse the vector to one string separated by a space. paste(collapse = &quot; &quot;) %&gt;% # Regular expression for selecting the last digit in a string. str_extract(&quot;(\\\\d+)(?!.*\\\\d)&quot;)   In the next step you can now write your own function that extracts the relevant sections from each overview page. The result is output as a tibble – a data.frame object that also provides a quick overview of the data in the console and can be used for. To avoid scraping later, you should store the raw HTML in a column of the tibble. This also allows you to extract additional data, such as the number of comments, later. Such a procedure is also a best practice, as it allows you to avoid a larger server load for the site operator due to multiple scraping.  get_article_overview &lt;- function(url) { page &lt;- read_html(url) tibble( title = page %&gt;% html_elements(&quot;.image-left .title&quot;) %&gt;% html_text(), subtitle = page %&gt;% html_elements(&quot;.image-left .dachzeile&quot;) %&gt;% html_text(), url = page %&gt;% html_elements(&quot;.image-left .title&quot;) %&gt;% html_attr(&quot;href&quot;), date = page %&gt;% html_elements(&quot;.image-left .date&quot;) %&gt;% # Dates are presented in a date-mont-year format, which is common in Germany. # The locale must be installed on your computer, in this case &quot;de_DE.UTF-8&quot;. dmy(locale = &quot;de_DE.UTF-8&quot;), raw = page %&gt;% html_elements(&quot;.image-left .listing-item&quot;) %&gt;% # You want to store this as `character` because a `tibble` does not accept a `xml_nodeset` item, # which also points to a serialized object stored in memory. # You can reserialze this object again using `read_html()`. as.character() ) }   Using your just written function and the previously extracted last page number, a vector of all overview pages can now be created. Afterwards, using the function map() from the package purrr, your function can be applied to all pages and in turn and tibble can be formed from them. (map_dfr() applies the function and forms a data.frame(df) by merging the results row by row (r)).  all_articles_summary &lt;- read_rds(&quot;data/all_articles_summary.rds&quot;)   url_part &lt;- &quot;https://www.anti-spiegel.ru/category/aktuelles/?dps_paged=&quot; all_page_urls &lt;- map_chr(1:last_page_nmbr, ~paste0(url_part, .x)) all_articles_summary &lt;- map_dfr(all_page_urls, get_article_overview) all_articles_summary   Because R and rvest work only sequentially via map() the command can often take a long time: one web page is called, scraped and then the next web page is called and scraped. The internet speed, the computing power as well as possible blockades by site operators can cause individual connections to get aborted, or the command would take so long that it is no longer practicable to scrape a web page.  You can improve the scraping process by parallelization: It is important to keep in mind that parallelization leads to additional load on the affected web servers. Several pages are opened in parallel – more in the chapter on parallelization.  Webscraping is a gray area and is regulated differently from country to country or was part of court decisions. Even if site operators want to prevent web scraping, for example through their terms of use, it may still be justified and permitted, e.g. for consumer protection reasons. This is the case in Germany, for example (German Federal Court of Justice 2014).  ","version":"Next","tagName":"h2"},{"title":"Analyzing scraped data and next steps​","type":1,"pageTitle":"Webscraping techniques with R","url":"/docs/data-collection/03_03_web-scraping-intro#analyzing-scraped-data-and-next-steps","content":" Since the data is in tidy format (see Wickham 2014), further analysis and presentation via the packages of the tidyverse is simple and clear. Using ggplot we can, for example, display the number of posts that were written per month.  all_articles_summary %&gt;% mutate(month = floor_date(date, &quot;month&quot;)) %&gt;% count(month) %&gt;% # It is recommended to filter out the current month # because it risks being misrepresented in the dataset. filter(month != today() %&gt;% floor_date(&quot;month&quot;)) %&gt;% ggplot(aes(month, n)) + geom_line() + theme_light() + labs(title = &quot;Posts per Month on antispiegel.ru&quot;, subtitle = glue(&quot;Posts until {today() %&gt;% format('%B %Y')}&quot;), y = &quot;Posts&quot;, x = NULL)   Number of article per month on the german pro-russion propaganda website antispiegel.ru by Thomas Röper  The example of Röper illustrates how to handle data from particularly active sites. While you didn’t scrape the number of comments via your function, you can do so by extracting the number of comments from the raw values in the column raw. Thus, you avoid further scraping of the data.  Note that not all articles contain comments – if comments are missing, there is also no html_element with comments, leading to fewer comment fields than e. g. title fields and a tibble cannot be constructed.  The reason why we did this is also because not all articles contain comments – if comments are missing, there is also no html_element with comments, leading to fewer comment fields then e. g. title fields and a tibble can’t be constructed.  get_comments &lt;- function(raw_data) { element &lt;- read_html(raw_data) html_elements(element, &quot;.comments-link a&quot;) %&gt;% html_text(trim = TRUE) } article_comments &lt;- all_articles_summary %&gt;% # You should wrap this in “possibly” to prevent errors from stopping the code execution. # If you can’t extract a comment string, you give it a NA value. mutate(comments_string = map(raw, possibly(get_comments, otherwise = NA_character_))) %&gt;% unnest(comments_string) %&gt;% mutate(comments = str_extract(comments_string, &quot;\\\\d+&quot;) %&gt;% as.numeric()) article_comments %&gt;% ggplot(aes(comments)) + geom_histogram(binwidth = 1) + theme_light() + labs(title = &quot;Histogram of comments at anti-spiegel.ru&quot;, subtitle = glue(&quot;Posts until {today() %&gt;% format('%B %Y')}&quot;))   Histogram of number of comments under each article from anti-spiegel.ru  As you can see, the most discussed article has 495 (r article_comments %&gt;% top_n(1, comments) %&gt;% pull(comments)) user comments with the headline &quot;Putins Rede zur Vereinigung Russlands mit den ehemals ukrainischen Gebieten&quot; (r article_comments %&gt;% top_n(1, comments) %&gt;% pull(title)) from 01. October 2022 (r article_comments %&gt;% top_n(1, comments) %&gt;% pull(date) %&gt;% format(&quot;%d. %B %Y&quot;)). Most of the articles are not commented.  ","version":"Next","tagName":"h3"},{"title":"Scraping huge datasets via parallelization​","type":1,"pageTitle":"Webscraping techniques with R","url":"/docs/data-collection/03_03_web-scraping-intro#scraping-huge-datasets-via-parallelization","content":" Previously, you learned how to scrape content sequentially and you were able to extract 20 article summaries per scraped page. If you now want to scrape not only the preview of the articles but all pages, the number of pages to be scrapped increases significantly.  With the package furrr you can improve the scraping process by parallelization: It is important to keep in mind that parallelization leads to additional load on the affected web servers. Several pages are opened in parallel. It is therefore important to include pauses in the function definition to distribute the load and to keep the number of cores used for parallelization low - in this case a maximum of five workers working in parallel.  Via plan(mulitsession) you specify that the following code should be executed in parallel sessions - this is not always faster because the sessions must be started and ended. Small data sets are possibly better suited with a sequential approach.  All articles of Anti-Spiegel.ru should be scrapped and analyzed – however, in first tests it could be noticed that a lot of content on Röper’s page is repeated. For example, under each article there is a reference to his book published by J.K-Fischer Verlag. However, this advertisement for his own publication is separated from the rest of the text by a dividing line.  Using simple CSS selectors, however, this part cannot be separated from the rest -- though rvest can be used with extended xpath selectors. These allow us, for example, to only scrape &lt;p&gt; nodes which are followed by a &lt;hr&gt; node (the separator).  Hint Cheat sheets for using xpath selectors are available here.   page &lt;- read_html(&quot;https://www.anti-spiegel.ru/2023/daenemark-deutschland-und-schweden-verweigern-auskunft-beim-un-sicherheitsrat-zur-nord-stream/&quot;) page %&gt;% html_elements(xpath = &quot;//p[following-sibling::hr]&quot;) %&gt;% html_text() %&gt;% paste(collapse = &quot;\\n&quot;)    [1] &quot;UNO, 11. Juli. TASS/ Vertreter Dänemarks, Deutschlands und Schwedens nehmen am Dienstag nicht an der von Russland einberufenen Sitzung des UN-Sicherheitsrates über die Sprengung der Gaspipelines Nord Stream und Nord Stream-2 teil, berichtet der TASS-Korrespondent.\\nZuvor hatte der erste stellvertretende ständige Vertreter Russlands bei dem Weltgremium, Dmitri Poljanski, erklärt, die russische Delegation habe für den 11. Juli eine offene Sitzung des UN-Sicherheitsrates zum Thema der Spreungung der Nord-Stream-Pipeline beantragt. Dem Diplomaten zufolge hat Russland „die britische Präsidentschaft gebeten, Vertreter“ der drei Länder – Dänemark, Deutschland und Schweden – einzuladen, die die Sabotage der Gaspipelines untersuchen, um darüber zu berichten.&quot;   Unfortunately, however, not all entries have this separator, so you won’t get results for some entries with this xpath selector – for example with this article.  page &lt;- read_html(&quot;https://www.anti-spiegel.ru/2022/ab-freitag-gibt-es-russisches-gas-nur-noch-fuer-rubel-die-details-von-putins-dekret-und-was-es-bedeutet/&quot;) page %&gt;% html_elements(xpath = &quot;//p[following-sibling::hr]&quot;) %&gt;% html_text() %&gt;% paste(collapse = &quot;\\n&quot;)    [1] &quot;&quot;   Often you will encounter such problems when webscraping, so it is important to work with a lot of examples first and test the code extensively. In this case, you scrape the data with a CSS selector and remove the always same advertising paragraphs with simple regular expressions. As mentioned above you create a function with pauses of 5 seconds (Sys.sleep(5)) on 5 processors at the same time.  all_articles_full &lt;- read_rds(&quot;data/all_articles_full.rds&quot;)   library(furrr) plan(multisession, workers = 5) get_article_data &lt;- function(url) { page &lt;- read_html(url) # Pause for 5 seconds Sys.sleep(5) # regex for addendum beginning pattern_addendum &lt;- &quot;In meinem neuen Buch.*$|Das Buch ist aktuell erschienen.*$&quot; tibble( url = url, raw = page %&gt;% html_elements(&quot;.article__content &gt; p&quot;) %&gt;% # `html_text2` to trim the data and remove unnecessary white spaces. as.character() %&gt;% str_remove(pattern_addendum) %&gt;% paste0(collapse = &quot;&quot;), text = page %&gt;% html_elements(&quot;.article__content &gt; p&quot;) %&gt;% html_text() %&gt;% str_remove(pattern_addendum) %&gt;% paste(collapse = &quot;\\n&quot;), datetime = page %&gt;% html_elements(&quot;.article-meta__date-time&quot;) %&gt;% html_text2() %&gt;% # &lt;1&gt; dmy_hm(locale = &quot;de_DE.UTF-8&quot;) ) } all_articles_full &lt;- future_map(all_articles_summary %&gt;% filter(date &gt;= &quot;2022-01-01&quot;) %&gt;% pull(url), # Wrapped in `try` to prevent errors from failing the whole process – # failed scrapings can be removed afterwards. ~try(get_article_data(.x)), .progress = TRUE)   Next, you will implement further measures to prevent your code from breaking prematurely and losing the already scraped content. Since the idea is that you wrap the function in a try, even errors do not lead to an abortion of the scraping. Errors can, however, mean connection failures. These failures can be fixed in a further process – if errors occur too often, it is recommended to optimize the code or to have longer pause times during the scraping to avoid overloading the servers.  To avoid scraping all websites, you should set clear limits for data collection, in this example this will be data from 2022 onwards – in a final step, you want to determine which domains are particularly frequently cited by Thomas Röper.  all_articles_df &lt;- all_articles_full %&gt;% keep(is_tibble) %&gt;% bind_rows() get_links &lt;- function(raw_html) { element &lt;- read_html(raw_html) element %&gt;% html_elements(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) } get_domain &lt;- function(link) { # regex for domain domain_pattern &lt;- &quot;^(?:[^@\\\\/\\\\n]+@)?([^:\\\\/?\\\\n]+)&quot; link %&gt;% str_remove(&quot;http://|https://&quot;) %&gt;% str_remove(&quot;^www.&quot;) %&gt;% str_extract(domain_pattern) } all_articles_df %&gt;% mutate(links = map(raw, possibly(get_links, otherwise = NA_character_))) %&gt;% mutate(domain = map(links, possibly(get_domain, otherwise = NA_character_))) %&gt;% unnest(domain) %&gt;% count(domain, sort = TRUE) %&gt;% head(10) %&gt;% # `kable()` from the library `knitr` is only used in this context to generate an html table knitr::kable() # &lt;1&gt;   Table 1: Most shared domains by anti-spiegel.ru  domain\tnanti-spiegel.ru\t5793 tass.ru\t1219 spiegel.de\t611 vesti7.ru\t303 t.me\t245 vesti.ru\t191 deutsch.rt.com\t137 youtube.com\t126 kremlin.ru\t85 mid.ru\t82  To sum up, your first webscraping case highlighted that Röper refers particularly frequently to the Russian news agency TASS and the Kremlin outlet RT DE. From a research perspective, it is noteworthy that there are European sanctions and a broadcast ban against RT DE (see Spiegel 2022; Der Standard 2022) – yet our test case continues to share them (for possible explanations why, see Baeck et al. 2023). Among the most shared domains is also t.me of the platform and messenger service Telegram – this platform is used by Röper particularly often, along with YouTube. Incidentally, if you hadn’t removed the advertising block, J.K. Fischer Verlag would rank first in this table.  ","version":"Next","tagName":"h3"},{"title":"Second example: AUF1 as a dynamic website​","type":1,"pageTitle":"Webscraping techniques with R","url":"/docs/data-collection/03_03_web-scraping-intro#second-example-auf1-as-a-dynamic-website","content":" Röper’s website is comparatively simple in design. The Wordpress site is statically generated. The web server simply outputs HTML pages, which can be downloaded and checked.  Not all sites are built this way, in fact it has probably become more common to host dynamic websites. This is also the case with the Austrian website AUF1 of the right-wing extremist and conspiracy ideologue Stefan Magnet (see Scherndl and Thom 2023), which quickly became one of the most widespread websites of pandemic deniers in the German-speaking world from autumn 2021.  Content scraped via rvest shows that reactions cannot be scraped – it simply shows an empty value.  AUF1 Screenshot with Reactions  rndm_article &lt;- &quot;https://auf1.tv/auf1-spezial/fpoe-gesundheitssprecher-kaniak-zu-who-plaenen-impf-zwang-auch-ohne-pandemie&quot; page &lt;- read_html(rndm_article) page %&gt;% html_elements(&quot;.reactions&quot;)   {xml_nodeset (0)}   A workaround here is to simulate a browser, which calls the page and executes all content. One possibility for this would be to use Selenium. This is a framework for automated software testing of web applications in different browsers –and you can use it for advanced web scraping, too.  With RSelenium you can start different browsers (to replicate this example, you should use Firefox, the corresponding binaries are downloaded via wdman). Afterwards you navigate to the corresponding page and then load the web page contents of the finished page back into rvest via read_html. Then, you can work as before and analyze different contents.  #| label: tbl-emojis #| tbl-cap: Reactions used on a random AUF1 article library(RSelenium) # start the browser # check if java and openjdk is installed first # wdman will install the rest rD &lt;- rsDriver(browser = &quot;firefox&quot;, verbose = FALSE) rD$client$navigate(rndm_article) # Setting a _sleep_ for 3 seconds to guarantee that the page has finished loading. Sys.sleep(3) # We can load in the Selenium page source directly into rvest and use it as before page &lt;- read_html(rD$client$getPageSource()[[1]]) tibble( emoji = page %&gt;% html_elements(&quot;.reactions .emoji&quot;) %&gt;% html_text() %&gt;% unique(), count = page %&gt;% html_elements(&quot;.reactions .count&quot;) %&gt;% html_text() %&gt;% as.integer() ) %&gt;% arrange(-count) %&gt;% pivot_wider(names_from = emoji, values_from = count) %&gt;% # Just for styling purposes, this looks more clean in the browser knitr::kable(align = &quot;c&quot;)   Table 2: Reactions used on a random AUF1 article  👍🏻\t❤️\t💪🏻\t😮\t😢489\t229\t131\t64\t58  With RSelenium you can extract data which wouldn’t be possible via rvest alone. You could now write a function to extract all emojis on videos and find the video that had the most interactions – this is not something, we will show in this chapter, but should be provided for interested (social) scientists and civil society researchers to build webscraping projects on their own. Even for advanced and dynamic web pages.  One last thing: Since you are using an automated browser on Desktop, you need to close it after reading in your data. Otherwise, the browser will keep on running in the background. You should also close your server that provided the background tasks for running Selenium.  rD$client$close() rD$server$stop()   ","version":"Next","tagName":"h3"},{"title":"Literature​","type":1,"pageTitle":"Webscraping techniques with R","url":"/docs/data-collection/03_03_web-scraping-intro#literature","content":" AlgorithmWatch. (2023). DSA Must Empower Public Interest Research with Public Data Access. AlgorithmWatch. https://algorithmwatch.org/en/dsa-empower-public-interest-research-data-access/ Baeck, Jean-Philipp, Fromm, Anne, &amp; Peters, Jean. (2023). Warum Russia Today trotz Sanktionen in Europa weiterläuft. correctiv.org. https://correctiv.org/aktuelles/russland-ukraine-2/2023/02/17/eu-sanktionen-gcore-russia-today/ Balzer, Erika. (2022). Desinformations-Medien: Der Anti-Spiegel - Russische Propaganda und Verschwörungsmythen. Belltower.News. https://www.belltower.news/desinformations-medien-der-anti-spiegel-russische-propaganda-und-verschwoerungsmythen-132357/ Binder, Matt. (2023). Elon Musk Claims Twitter Login Requirement Just 'Temporary'. Mashable. https://mashable.com/article/elon-musk-twitter-login-requirement-temporary DerStandard. (2022). Bis zu 50.000 Euro Strafe für Verbreitung von russischem Staatssender RT – alleine FPÖ stimmt dagegen. DER STANDARD. https://www.derstandard.de/story/2000133980411/bis-50-000-euro-fuer-verbreitung-von-russischem-staatssender-rt EDMO. (2023). Members of the EDMO Task Force on Disinformation on the War in Ukraine Submit Feedback to the EC Call for Evidence on the Provisions in the DSA Related to Data Access. EDMO. https://edmo.eu/2023/05/30/members-of-the-edmo-task-force-on-disinformation-on-the-war-in-ukraine-submit-feedback-to-the-ec-call-for-evidence-on-the-provisions-in-the-dsa-related-to-data-access/ Fung, Brian. (2023a). Academic Researchers Blast Twitter's Data Paywall as 'outrageously Expensive'. CNN Business. https://www.cnn.com/2023/04/05/tech/academic-researchers-blast-twitter-paywall/index.html Fung, Brian. (2023b). Reddit Sparks Outrage after a Popular App Developer Said It Wants Him to Pay $20 Million a Year for Data Access. CNN Business. https://www.cnn.com/2023/06/01/tech/reddit-outrage-data-access-charge/index.html German Federal Court of Justice. (2014). BGH, 30.04.2014 - I ZR 224/12. BGH. Glez-Peña, Daniel, Lourenço, Anália, López-Fernández, Hugo, Reboiro-Jato, Miguel, &amp; Fdez-Riverola, Florentino. (2014). Web Scraping Technologies in an API World. Briefings in Bioinformatics. https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbt026 Journalist. (2022). Im Desinformationskrieg. journalist.de. https://www.journalist.de/startseite/detail/article/im-desinformationskrieg Khder, Moaiad Ahmad. (2021). Web Scraping or Web Crawling: State of Art, Techniques, Approaches and Application. International Journal of Advances in Soft Computing &amp; Its Applications, 13(3). Scherndl, Gabriele, &amp; Thom, Paulina. (2023). Was hinter dem österreichischen Verschwörungssender Auf1 steckt. correctiv.org. https://correctiv.org/faktencheck/hintergrund/2023/04/27/was-hinter-auf1-stefan-magnet-und-der-ausbreitung-des-oesterreichischen-verschwoerungssenders-steckt-desinformation-und-rechte-hetze/ Spiegel. (2022). Maßnahmen gegen russische Staatsmedien: Verbreitung von RT und Sputnik ist in der EU ab sofort verboten. Der Spiegel. https://www.spiegel.de/kultur/ukraine-krieg-verbreitung-von-rt-und-sputnik-ist-in-der-eu-ab-sofort-verboten-a-49597add-c2b2-44da-b1e4-6832d3ea824f Wickham, Hadley. (2014). Tidy Data. The Journal of Statistical Software. http://www.jstatsoft.org/v59/i10/ ","version":"Next","tagName":"h2"},{"title":"Ethical considerations","type":0,"sectionRef":"#","url":"/docs/get-started/01_02_ethical-considerations","content":"","keywords":"","version":"Next"},{"title":"7 principles for handling social media data ethically - Planning checklist​","type":1,"pageTitle":"Ethical considerations","url":"/docs/get-started/01_02_ethical-considerations#7-principles-for-handling-social-media-data-ethically---planning-checklist","content":"  Anonymisation Anonymise data to protect individuals' privacy by replacing personal identifiers with random codes. Data Minimisation Avoid collecting unnecessary or sensitive information by only collecting data directly relevant to your research purposes. Data Retention Delete data when it is no longer needed. Safe Data Storage Ensure proper encryption protocols are used during transmission and that collected data is stored securely. Harm Mitigation Consider social, psychological and local legal consequences that can harm individuals or communities with the results of research. Advocacy Use social media as a tool for responsible advocacy and minimise the data storage and collection about your advocacy and campaigns. Research Funding Transparency if you conduct funded research, be transparent about the funding sources and potential conflicts of interest that could influence it. An improved level of transparency will likely increase trustworthiness of the way proposals are handled and of the grant allocation system in general.  ","version":"Next","tagName":"h2"},{"title":"Context: Data availability, personal data, access rights​","type":1,"pageTitle":"Ethical considerations","url":"/docs/get-started/01_02_ethical-considerations#context-data-availability-personal-data-access-rights","content":"   After two decades of a technological revolution resulting in unprecedented Internet connectivity and social media adoption, technology companies have become increasingly powerful and opaque in how they exercise such power. ​​Despite having become critical to people’s lives, citizens have almost no vehicle to request information and data about how social media platforms operate, beyond the right to access data under GDPR.  Currently, there is no equivalent of access to information laws to request information and data sets shaping our digital lives on social platforms, as the data released by social media companies is limited. Only eventually, through research, whistleblowing, public hearings or legal discovery processes, such fragments of information come into the light.  When there is no accessible data, public interest research conducted by non-governmental organisations permits an understanding of how social media companies operate or how they shape specific topics, areas, or events. Social and digital activists often collect such data by open web scraping, crowdsourcing, or other innovative technology methods. However, such activities often confront them with legal questions and ethical dilemmas related to data collection. In this new context, researchers need to obtain and reuse social media data with ethical standards that prevent harm to individuals and communities.  ","version":"Next","tagName":"h2"},{"title":"A principle-based framework​","type":1,"pageTitle":"Ethical considerations","url":"/docs/get-started/01_02_ethical-considerations#a-principle-based-framework","content":"   In response to the dynamic nature of social media, this chapter introduces you to a principles-based approach to Social Media Monitoring. A set of principles that, when applied, provide a framework and a living process that allow you to mitigate risks by responding promptly to ethical challenges as they emerge. Initially, the framework aims to understand risks and create a robust foundation for addressing the multilayered complexities of handling social media data ethically in various contexts. This includes transparency about data collection practices, privacy concerns, bias and discrimination, and cultural diversity. Understanding that ethical challenges are complex is key to acknowledging that research design will naturally pinpoint ethical risks if some principles are applied to the process.  Next, the chapter includes a Key Risk Indicator (KRI) to identify potential harms at different stages of social media data projects that should be regularly monitored to trigger proactive responses. Conducting a risk control self-assessment and including corrective actions and adjustments to ethical frameworks and practices is essential to improve the safety and quality of social media data research. Finally, the framework provides an easy-to-understand principles table that can be applied to the real-time design, deployment and evaluation of projects involving social media monitoring. This framework can help researchers and activists working on social media monitoring clarify their implicit or explicit decisions when collecting data and help define an ethical approach to their work.  ","version":"Next","tagName":"h2"},{"title":"Ex-ante considerations of handling social media data ethically​","type":1,"pageTitle":"Ethical considerations","url":"/docs/get-started/01_02_ethical-considerations#ex-ante-considerations-of-handling-social-media-data-ethically","content":" When monitoring social media, many researchers could potentially collect data in real-time first and only later consider the ethical implications or potential risks once the dataset is completed. An easy justification for such behaviour could be that the information is “out in the public” anyway and that its massive collection is valid, justified, and harmless. The first step towards more ethical data collection is challenging this implicit “scrape all the data” consideration many data collectors embrace.  The following list is not exhaustive but sheds light on interconnected and multilayered risks that monitoring social media can present across different contexts, and where research is not clearly delineated. As a researcher you should think through these implications before you start your monitoring efforts:  ","version":"Next","tagName":"h2"},{"title":"Human rights considerations​","type":1,"pageTitle":"Ethical considerations","url":"/docs/get-started/01_02_ethical-considerations#human-rights-considerations","content":" Right to Oblivion or Right to be Forgotten The right to oblivion, or the right to be forgotten, is a concept rooted in Article 12 of the Universal Declaration of Human Rights, also recognised by courts in the EU, Argentina and The Philippines. It asserts that no one shall be subjected to arbitrary interference with their privacy, so individuals should be able to request the removal of personal data that is no longer necessary. Social media monitoring can challenge this right, as data collected may persist indefinitely, potentially haunting individuals long after the information loses relevance.  Vulnerability Vulnerable populations, including minors, marginalised communities, individuals with limited digital literacy and people suffering from abuse, can be disproportionately affected by social media monitoring. They may lack the resources and knowledge to challenge the exploitation of their data and make informed choices, including the right to challenge the very basis of monitoring. It is important to note that vulnerability highly depends on local context. A person can become vulnerable because of local laws or due to culture, and a person may be vulnerable in one country but not another. Local context always needs to be considered.  Protection of Personal Data Failing to comply with data protection regulations, such as the EU’s General Data Protection Regulation (GDPR), regarding the processing of personal data can lead to data breaches and misuse of individuals' personal information.  Non-Discrimination Discriminatory practices in social media data collection or algorithmic bias can lead to unequal approaches based on race, gender, or religion and reinforce existing inequalities in certain groups.  ","version":"Next","tagName":"h3"},{"title":"Social and collective harm​","type":1,"pageTitle":"Ethical considerations","url":"/docs/get-started/01_02_ethical-considerations#social-and-collective-harm","content":"  Data Extractivism The mass extraction of social media data involves collecting data from individuals or organisations without their informed or free and informed consent and can be linked to resource extraction. This extractive approach can lead to exploitation, as individuals become commodities in the data-driven economy, raising concerns around fairness and the concentration of power. The expression “data extractivism” establishes an analogy between information management and the mining industry, defining data as a raw material that can be extracted, commercialised, refined, processed, and transformed into other commodities with added value.   Power Dynamics The concentration of data in the hands of a few can shift power dynamics in society and lead to creating data monopolies. This centralised control raises concerns about surveillance states and the potential for abuse of power, as well as the erosion of democratic principles as data owners can significantly influence markets, decision-making, and even public discourse.   Economic Harm The vast amount of data collected from social media users has great economic value in the digital economy, leading to concerns about economic harm to underprivileged groups. Economic harm from unethical or malicious social media monitoring practices can raise questions about fairness and equity.  ","version":"Next","tagName":"h3"},{"title":"First proposal: Develop social media monitoring Key Risk Indicators (KRI)​","type":1,"pageTitle":"Ethical considerations","url":"/docs/get-started/01_02_ethical-considerations#first-proposal-develop-social-media-monitoring-key-risk-indicators-kri","content":" Is the data I am collecting ethical? There is no simple answer to this question, but you can adopt Key Risk Indicators (KRI) that present an easy way to help you identify potential harms. KRI are specific metrics that serve as early warning signs for social media monitoring that can be used when researchers design projects to track ethical risks associated with their projects.  Risk\tIndicator\tImpactOpaque Data Usage\tFull disclosure of documentation regarding data collection practices and purposes.\tA low transparency rate may indicate an ethical risk related to the lack of openness about data usage. Non-Compliance\tPercentage of data where informed consent was obtained for collection.\tA low compliance rate may indicate a potential ethical risk related to data collection practices. Weak Data Accuracy\tQualitative analysis of data in terms of misinformation or manipulated content.\tWeak data accuracy can indicate an ethical risk, especially when used in decision-making processes. Violation of Data Rights\tThe number of disputes related to data ownership.\tAn increased ownership dispute rate can indicate ethical concerns about data rights. Biased data\tNumber of occurrences of bias detected in data collection and analysis.\tRepeated detection of bias can indicate ethical concerns regarding fairness and discrimination.  ","version":"Next","tagName":"h2"},{"title":"Second proposal: Adopt a principles-based approach for social media data handling​","type":1,"pageTitle":"Ethical considerations","url":"/docs/get-started/01_02_ethical-considerations#second-proposal-adopt-a-principles-based-approach-for-social-media-data-handling","content":" In a fast-changing scenario like the social media environment, new circumstances that are not contemplated can arise at any time, making it difficult to stay up to date when using a strict framework. Thus, a rules-based approach can quickly become outdated. A principles-based approach, however, can help you, as researchers, reduce the complexity of compliance. It also promotes collaboration between different stakeholders, which speeds up the process and empowers people to take ownership.  This set of key principles and ethical standards are aimed at guiding organisations and researchers in monitoring social media platform data to ensure its responsible and ethical use, respecting user privacy, and promoting transparency.  Principle\tIssue\tProcessTransparency\tAm I being transparent about the purpose of social media monitoring and the data types I am collecting?\tInform users about how their data may be used and who has access to it. Apply FAIR Principles to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets. Compliance\tAm I respecting the relevant laws and regulations, including data privacy and intellectual property laws?\tEnsure that your monitoring practices comply with contextualised laws and allow users to opt out of being monitored. Data Accuracy\tAm I ensuring that the information gathered from social media is credible and verifiable?\tAvoid spreading misinformation by establishing an information-checking process before considering the data in the research. Data integrity and availability should also be addressed. Explicit Consent\tAm I seeking explicit permissions from users when collecting data that might be considered sensitive?\tDetermine how the data will be used, and respect users' decisions regarding data usage and sharing. Data Minimization and Deletion\tAm I ensuring that only personal data that is directly related to the research is collected?\tLimit the collection of personal information to what is directly relevant and necessary to accomplish a specified purpose. Data that is not needed anymore should be deleted. Bias Detection\tAm I ensuring that social media monitoring and analysis do not lead to discrimination based on race, gender, religion or political angle?\tFoster diversity and inclusion within your monitoring team. If using algorithms for data analysis, make them transparent and auditable. Community Feedback\tAm I being open to feedback from the community regarding potential monitoring activities?\tActively seek input by providing accessible channels for users to voice their concerns, suggestions, and questions. Incorporate feedback when necessary. Accountability\tAm I regularly reviewing and auditing the monitoring practices to ensure compliance with ethical principles?\tEstablish accountability mechanisms by assigning responsibility for social media monitoring within the organisation. Openness\tAm I ensuring people can access and reuse all the tools and materials from the social media monitoring project?\tComply with the Open Definition principles and provide an open licence for other people to access and reuse all the resources. Beware of Vulnerabilities\tAm I recognising the potential harm social media monitoring can cause vulnerable individuals?\tOnly collect, share, or analyse data related to vulnerable individuals' experiences when it serves a legitimate and beneficial purpose. Pseudonymize or anonymize the data as much as possible to prevent harm Periodic Review\tAm I regularly reviewing monitoring practices to align with user behaviour and platform policy evolution over time?\tRegularly adapt monitoring practices to align with ethical standards that relate to the current reality.  ","version":"Next","tagName":"h2"},{"title":"References and further resources​","type":1,"pageTitle":"Ethical considerations","url":"/docs/get-started/01_02_ethical-considerations#references-and-further-resources","content":" Mehtab Khan and Alex Hanna. The Subjects and Stages of AI Dataset Development: A Framework for Dataset Accountability. September 13, 2022. Adriana Alvarado Garcia, Marisol Wong-Villacres, Milagros Miceli, Benjamín Hernández, Christopher A Le Dantec. Mobilizing Social Media Data: Reflections of a Researcher Mediating between Data and Organization. CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. April 2023 Article No.: 866 Pages 1–19 https://doi.org/10.1145/3544548.3580916 . Connie Moon Sehat and Tarunima Prabhakar, with Aleksei Kaminski. Ethical Approaches to Closed Messaging Research. Considerations in Democratic Contexts. March 15, 2021. Sara Mannheimer and Elizabeth A. Hull. Sharing selves: Developing an ethical framework for curating social media data. 12th International Digital Curation Conference (IDCC), Edinburgh, Scotland, 20-23 February 2017. Dr. Leanne Townsend and Prof. Claire Wallace. Social Media Research: A Guide to Ethics. This work was supported by the Economic and Social Research Council [grant number ES/M001628/1] and was carried out at The University of Aberdeen, 2016. Horbach SPJM, Tijdink JK, Bouter L. 2022 Research funders should be more transparent: a plea for open applications. R. Soc.Open Sci.9: 220750. https://doi.org/10.1098/rsos.22075 Celis Bueno y Schultz (2021) Data Extractivism. In: Celis Bueno y Schultz, Imaginación maquínica, http://imaginacionmaquinica.cl/data-extractivism ","version":"Next","tagName":"h2"},{"title":"Literature and illustrative research","type":0,"sectionRef":"#","url":"/docs/literature/","content":"Literature and illustrative research last updated on 06.12.2024 by Cathleen Berger Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Literature library This chapter offers an academic synthesis of the current scientific discourse around social media monitoring. By combining insights from journals, publications, and key policy documents, it helps you read up on foundational references and get a sense of the academic landscape in this field. Learn more Community articles and research You've recently published your findings or insights? Share it with the community. This section provides a running list of all the latest projects our community is working on. Learn more Open for contributions We welcome contributions on a rolling basis. At the moment, we particularly welcome chapters dealing with the following questions Exploration of existing monitoring projectsResearch Gaps and Call4Action","keywords":"","version":"Next"},{"title":"Read up on social media monitoring: What does the literature say?","type":0,"sectionRef":"#","url":"/docs/literature/05_02_literature-overview","content":"","keywords":"","version":"Next"},{"title":"State of the Art: Research on mis- and disinformation​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/docs/literature/05_02_literature-overview#state-of-the-art-research-on-mis--and-disinformation","content":" The following article provides an overview on the current debate in the field of social media monitoring with a focus on mis- and disinformation and intends to serve as a starting point for further research and more in-depth reading. The literature review was conducted based on a screening of recent publications from leading journals (e.g., Political Communication, Nature Human Behaviour), widely cited literature published in the last 15 years, as well as current policy papers and reports from relevant institutions and actors in the field of social media monitoring.  This overview subsequently discusses the role of social media in circulating disinformation, before describing how social media monitoring is employed to track and engage with disinformation.  ","version":"Next","tagName":"h2"},{"title":"Premise of social media​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/docs/literature/05_02_literature-overview#premise-of-social-media","content":" Social media has become a popular playground for various actors seeking to inject false or misleading information into the public information stream – from intelligence agencies over (extremist) political parties to rogue civil society. Simultaneously, social media is increasingly used as an important source of information for citizens around the globe.  ","version":"Next","tagName":"h3"},{"title":"The (dis-)information ecosystem: terminology and definitions​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/docs/literature/05_02_literature-overview#the-dis-information-ecosystem-terminology-and-definitions","content":" While well-informed citizens are the cornerstone of a functioning democracy (Lewandowsky et al., 2020), false as well as misleading information threatens to undermine this informational basis citizens depend on. These types of information are frequently referred to as mis- and/or disinformation. While misinformation is merely false information, for example reporting errors in journalistic pieces, disinformation intentionally seeks to mislead individuals (Wardle &amp; Derakhshan, 2017). Beyond that, various other sub-types of disinformation are prevalent online. These include, for instance, conspiracy theories often linked to beliefs that actors with malign intent operate in secret against the public (for an overview see, Douglas &amp; Sutton, 2023), as well as “fake news”, which are published in the style of legitimate news articles but are partially or fully fabricated (Tandoc et al., 2018).  ","version":"Next","tagName":"h3"},{"title":"How and why mis- and disinformation spreads​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/docs/literature/05_02_literature-overview#how-and-why-mis--and-disinformation-spreads","content":" While the pursuit of a political agenda is an important reason for people to circulate conspiracy theories (Douglas &amp; Sutton, 2023), fake news are more frequently shared due to inattention of users rather than the intention to mislead (Pennycook &amp; Rand, 2021). Disinformation is also not limited to simple texts, tweets, or posts. Technological progress made it possible to alter pictures and videos in a way that serves disinformation purposes. Fabricated videos are now commonly referred to as deepfakes (Chesney &amp; Citron, 2018). This overview subsequently discusses the role of social media in circulating disinformation, before describing how social media monitoring is employed to track and engage with disinformation.  ","version":"Next","tagName":"h3"},{"title":"Social media as vector for disinformation​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/docs/literature/05_02_literature-overview#social-media-as-vector-for-disinformation","content":"   Very broadly, social media can be understood as Internet-based channels in which users can interact and share content, either in real-time or asynchronously (Carr &amp; Hayes, 2015). Various social media platforms exist today, such as Discord, Facebook, Flickr, Instagram, LinkedIn, Pinterest, Reddit, Snapchat, Telegram, TikTok, Twitter, and YouTube. Virtual social and virtual game worlds can also be understood as social media, such as Minecraft or the Metaverse (Kaplan &amp; Haenlein, 2010).  These platforms have different audiences, purposes, and functionalities, creating a complex and continuiously evolving media landscape. To understand the of digital media use, including social media, on democracy, the systematic review by Lorenz-Spreen et al. (2022) is highly recommended. Regardless of their purpose or funtionalities, disinformation is prevalent on most, if not all, platforms. For example, Facebook, Instagram, Reddit, and Twitter all struggle to contain disinformation on their platforms (Hao, 2021; Lukito, 2020; The Economist, 2020; Yang et al., 2021). While these platforms are used to dissiminate disinformation, some platforms are used to prepare and develop disinformation, including conspiracy theories, such as the messaging board 4chan in the case of Pizzagate (Tuters et al., 2018).  Beyond that, individuals frequently use several social media platforms, connecting these technically separate spheres. As a result, disinformation can reemerge at a different point in time on other platforms (Kang &amp; Frenkel, 2020). Thus, disinformation campaigns spread across various platforms (Lukito, 2020), often with tailored content to maximize engagement and visibility, and can therefore reach a large number of people – sometimes in surprising places. For instance, Russian propaganda has recently been reported to be present in various online video games, such as Minecraft (Myers &amp; Borwning, 2023).  This is especially problematic in crises, e.g., terrorist attacks, wars, or natural disasters, or otherwise sensitive situations, such as elections. During these times, many people go on social media to follow the developing situation and try to find the latest information. Monitoring these developments on social media is therefore of crucial importance (Reuter &amp; Kaufhold, 2018; Starbird et al., 2014; Vieweg et al., 2010).  ","version":"Next","tagName":"h2"},{"title":"Monitoring disinformation on social media​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/docs/literature/05_02_literature-overview#monitoring-disinformation-on-social-media","content":"   Social media monitoring (SMM) describes the systematic observation of social media platforms and other digital information sources. Although various approaches to SMM exist, the process can be broadly organized in four steps (Brady, 2020; Karafillakis et al., 2021):  preparation (e.g., defining the problem or goal of the monitoring as well as associated topics and terms),data extraction,data analysis,and dissemination of findings.  Some contexts demand a more comprehensive planning step, that includes a risk assessment to better understand relevant factors such as trust in media, social media consumption, and the political landscape (Brady, 2020). Data collection is to a large extent automatized by crawling a social media platform to retrieve data and identify trends in respect to a certain set of topics or keywords but also specific sources of information. As Brady however points out based on insights from SMM during five European elections, there is no gold standard for SMM, and the field remains largely experimental.  Thus, the precise combination of tools, team qualifications, and organizational settings varies from case to case. The increasing availability of AI technology will also support SMM. Some argue, that given the immense volume of data produced every day, only AI will be able to identify emerging threats in real time (Yankoski et al., 2020). Research on automated fake news detection highlights the potential of this approach (for example, Tacchini et al., 2017).  In the context of disinformation, SMM is primarily employed to protect elections and address disinformation campaigns. Monitoring the information space during elections has become increasingly important. For instance, Russia tried to interfere in the 2016 U.S. presidential elections, although with limited success in respect to changing people’s attitudes or voting behavior (Eady et al., 2023). Various elections of EU member states were targeted by disinformation campaigns as well, such as Slovakia in 2019 and Spain in 2019 and again in 2023 (Fundación Maldita.es &amp; Democracy Reporting International, 2023), although mostly with limited success (Bayer et al., 2021). Another notable case was the Brazilian elections in 2022, which witnessed a comprehensive disinformation campaign, using various tools and channels such as Cheapfakes (i.e., altered images and audio based on off-the-shelf technology) and WhatsApp Status. The campaign aimed to undermine citizens' trust in the elections and democratic institutions in general (Saab et al., 2022). Brady (2020) provides an informative example for SMM during elections.  SMM is also employed by organizations dedicated to counter ongoing foreign interference aimed at undermining public trust in democratic institutions. Distinguishing organic rumors from organized campaigns is a challenging endeavor, as disinformation campaigns often blend false information with facts (Starbird, 2020). Examples of relevant organizations include the NATO Strategic Communications Centre of Excellence (StratCom COE) located in Latvia (for a brief introduction of StratCom COE, see Hanley, 2022) as well as the European External Action Services’ East StratCom Task Force (ESCTF). The flagship product of ESCTF is the EUvsDisinfo initiative, which collects and debunks disinformation campaigns targeting the EU, its member states, and neighboring countries.  ","version":"Next","tagName":"h2"},{"title":"Making SMM-based research actionable​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/docs/literature/05_02_literature-overview#making-smm-based-research-actionable","content":" Monitoring disinformation and similar phenomena is of crucial importance to protect democratic processes and institutions. However, given the versatile category of disinformation, that is not limited to text but increasingly includes videos and audio (for example, deepfakes (Chesney &amp; Citron, 2018; Weikmann &amp; Lecheler, 2022)), and the everchanging social media landscape, SMM faces significant challenges.  From this perspective, SMM should be intertwined with comprehensive outreach and community mobilization efforts, to increase the effectiveness of SMM and disseminate findings to a larger public as fast as possible. Thus, like science communication (Holford et al., 2023), SMM should be understood as a collective intelligence endeavor that builds on the technical and regional expertise of various stakeholders, as illustrated by Brady's (2020) examples as well as the recent monitoring of Spain’s snap general election (Fundación Maldita.es &amp; Democracy Reporting International, 2023). SMM therefore goes beyond merely monitoring the information space on social media, as it tries to contribute to a better understanding of the current challenges democratic information spheres are coping with.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/docs/literature/05_02_literature-overview#references","content":" Bayer, J., Holznagel, B., Lubianiec, K., Pintea, A., Schmitt, J. B., Szakács, J., Uszkiewicz, E., European Parliament. Directorate-General for External Policies of the Union. Policy Department, &amp; European Parliament. Special Committee on Foreign Interference in all Democratic Processes in the European Union, including D. (2021). Disinformation and propaganda: impact on the functioning of the rule of law and democratic processes in the EU and its Member States - 2021 update. European Parliament. https://www.europarl.europa.eu/meetdocs/2014_2019/plmrep/COMMITTEES/INGE/DV/2021/04-13/EXPO_STU2021653633_EN.pdf Brady, M. (2020). Lessons Learned: Social Media Monitoring during Elections. Democracy Reporting International. https://democracy-reporting.org/en/office/global/collection?type=publications Carr, C. T., &amp; Hayes, R. A. (2015). Social Media: Defining, Developing, and Divining. Atlantic Journal of Communication, 23(1), 46–65. https://doi.org/10.1080/15456870.2015.972282 Chesney, R., &amp; Citron, D. K. (2018). Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security (No. 692; Public Law Research Paper). https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3213954 Douglas, K. M., &amp; Sutton, R. M. (2023). What Are Conspiracy Theories? A Definitional Approach to Their Correlates, Consequences, and Communication. Annual Review of Psychology, 74(1), 271–298. https://doi.org/10.1146/annurev-psych-032420-031329 Eady, G., Paskhalis, T., Zilinsky, J., Bonneau, R., Nagler, J., &amp; Tucker, J. A. (2023). Exposure to the Russian Internet Research Agency foreign influence campaign on Twitter in the 2016 US election and its relationship to attitudes and voting behavior. Nature Communications, 14(1). https://doi.org/10.1038/s41467-022-35576-9 Fundación Maldita.es &amp; Democracy Reporting International. (2023, August 8). Disinformation and Hate Online During the Spanish Snap General Election. Democracy Reporting International. https://democracy-reporting.org/en/office/global/publications/disinformation-and-hate-online-during-the-spanish-snap-general-election-of-july-2023 Hanley, M. (2022). NATO’s Response to Information Warfare Threats. In J. Chakars &amp; I. Ikmanis (Eds.), Information Wars in the Baltic States (pp. 205–223). Palgrave Macmillan. https://doi.org/10.1007/978-3-030-99987-2_11 Hao, K. (2021). How Facebook got addicted to spreading misinformation. MIT Technology Review. https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/ Holford, D., Fasce, A., Tapper, K., Demko, M., Lewandowsky, S., Hahn, U., Abels, C. M., Al-Rawi, A., Alladin, S., Sonia Boender, T., Bruns, H., Fischer, H., Gilde, C., Hanel, P. H. P., Herzog, S. M., Kause, A., Lehmann, S., Nurse, M. S., Orr, C., … Wulf, M. (2023). Science Communication as a Collective Intelligence Endeavor: A Manifesto and Examples for Implementation. Science Communication, c. https://doi.org/10.1177/10755470231162634 Kang, C., &amp; Frenkel, S. (2020, June 27). ‘PizzaGate’ Conspiracy Theory Thrives Anew in the TikTok Era. The New York Times, 30–33. https://www.nytimes.com/2020/06/27/technology/pizzagate-justin-bieber-qanon-tiktok.html Kaplan, A. M., &amp; Haenlein, M. (2010). Users of the world, unite! The challenges and opportunities of Social Media. Business Horizons, 53(1), 59–68. https://doi.org/10.1016/j.bushor.2009.09.003 Karafillakis, E., Martin, S., Simas, C., Olsson, K., Takacs, J., Dada, S., &amp; Larson, H. J. (2021). Methods for social media monitoring related to vaccination: Systematic scoping review. JMIR Public Health and Surveillance, 7(2). https://doi.org/10.2196/17149 Lewandowsky, S., Smillie, L., Garcia, D., Hertwig, R., Weatherall, J., Egidy, S., Robertson, R. E., O’Connor, C., Kozyreva, A., Lorenz-Spreen, P., Blaschke, Y., &amp; Leiser, M. (2020). Technology and Democracy: Understanding the influence of online technologies on political behaviour and decision-making. https://doi.org/10.2760/709177 Lorenz-Spreen, P., Oswald, L., Lewandowsky, S., &amp; Hertwig, R. (2022). A systematic review of worldwide causal and correlational evidence on digital media and democracy. Nature Human Behaviour, 7(1), 74–101. https://doi.org/10.1038/s41562-022-01460-1 Lukito, J. (2020). Coordinating a Multi-Platform Disinformation Campaign: Internet Research Agency Activity on Three U.S. Social Media Platforms, 2015 to 2017. Political Communication, 37(2), 238–255. https://doi.org/10.1080/10584609.2019.1661889 Myers, S. L., &amp; Borwning, K. (2023, July 30). Russia Takes Its Ukraine Information War Into Video Games. The New York Times. https://www.nytimes.com/2023/07/30/technology/russia-propaganda-video-games.html#:~:text=Russian propaganda is spreading into,popular social media network%2C VKontakte. Pennycook, G., &amp; Rand, D. G. (2021). The Psychology of Fake News. Trends in Cognitive Sciences, 25(5), 388–402. https://doi.org/10.1016/j.tics.2021.02.007 Reuter, C., &amp; Kaufhold, M.-A. (2018). Fifteen years of social media in emergencies: A retrospective review and future directions for crisis Informatics. Journal of Contingencies and Crisis Management, 26(1), 41–57. https://doi.org/10.1111/1468-5973.12196 Saab, B. A., Beyer, J. N., &amp; Böswald, L.-M. (2022). Beyond the Radar: Emerging Threats, Emerging Solutions. Democracy Reporting International. https://democracy-reporting.org/en/office/global/publications/going-beyond-the-radar-emerging-threats-emerging-solutions Starbird, K. (2020, July). Disinformation campaigns are murky blends of truth, lies and sincere beliefs – lessons from the pandemic. The Conversation. https://theconversation.com/disinformation-campaigns-are-murky-blends-of-truth-lies-and-sincere-beliefs-lessons-from-the-pandemic-140677 Starbird, K., Maddock, J., Orand, M., Achterman, P., &amp; Mason, R. M. (2014, March 1). Rumors, False Flags, and Digital Vigilantes: Misinformation on Twitter after the 2013 Boston Marathon Bombing. IConference 2014 Proceedings. https://doi.org/10.9776/14308 Tacchini, E., Ballarin, G., Della Vedova, M. L., Moret, S., &amp; de Alfaro, L. (2017). Some like it Hoax: Automated fake news detection in social networks. CEUR Workshop Proceedings, 1960, 1–12. Tandoc, E. C., Lim, Z. W., &amp; Ling, R. (2018). Defining “Fake News”: A typology of scholarly definitions. Digital Journalism, 6(2), 137–153. https://doi.org/10.1080/21670811.2017.1360143 The Economist. (2020). Instagram will be the new front-line in the misinformation wars. https://www.economist.com/the-world-in/2020/01/01/instagram-will-be-the-new-front-line-in-the-misinformation-wars Tuters, M., Jokubauskaitė, E., &amp; Bach, D. (2018). Post-Truth Protest: How 4chan Cooked Up the Pizzagate Bullshit. M/C Journal, 21(3), 1–18. https://doi.org/10.5204/mcj.1422 Twetman, H., Paramonova, M., &amp; Hanley, M. (2020). Social Media Monitoring: A Primer. Methods, tools, and applications for monitoring the social media space. NATO Strategic Communications Centre of Excellence. https://stratcomcoe.org/pdfjs/?file=/cuploads/pfiles/social_media_monitoring_a_primer_12-02-2020.pdf?zoom=page-fit. Consultado em 01mai2023, 18:10 Vieweg, S., Hughes, A. L., Starbird, K., &amp; Palen, L. (2010). Microblogging during two natural hazards events. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 2, 1079–1088. https://doi.org/10.1145/1753326.1753486 Wardle, C., &amp; Derakhshan, H. (2017). Information Disorder: Toward an Interdisciplinary framework for research and policy making. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-framework-for-researc/168076277c Weikmann, T., &amp; Lecheler, S. (2022). Visual disinformation in a digital age: A literature synthesis and research agenda. New Media &amp; Society. https://doi.org/10.1177/14614448221141648 Yang, K. C., Pierri, F., Hui, P. M., Axelrod, D., Torres-Lugo, C., Bryden, J., &amp; Menczer, F. (2021). The COVID-19 Infodemic: Twitter versus Facebook. Big Data and Society, 8(1). https://doi.org/10.1177/20539517211013861 Yankoski, M., Weninger, T., &amp; Scheirer, W. (2020). An AI early warning system to monitor online disinformation, stop violence, and protect elections. Bulletin of the Atomic Scientists, 76(2), 85–90. https://doi.org/10.1080/00963402.2020.1728976 ","version":"Next","tagName":"h2"},{"title":"Community articles and research","type":0,"sectionRef":"#","url":"/docs/literature/05_03_articles-and-research","content":"Community articles and research Original post on 30.09.2024 by Clara Ruthardt last updated on 01.08.2025 by Clara Ruthardt Social media research is a rapidly changing field. In this chapter you can find the latest research reports, tools, policy papers and articles from our community. Please note: Despite careful review of the content, the Data Knowledge Hub accepts no liability for the content of external links. The providers of the web pages to which the links lead are solely responsible for their contents. Social Media Monitoring Toolkit by Democracy Reporting International | The Digital Democracy Monitor Toolkit helps civil society, journalists, researchers and anyone trying to research social media and democracy. Read more Custom Portal for Innovative Data Collection and Disinformation Analysis by Inform Africa | Discover Inform Africa's Custom Portal for Disinformation Analysis—a user-friendly tool designed to streamline the collection and analysis of disinformation data from platforms like Facebook, Twitter, YouTube, and TikTok. Fully compatible with the DISARM Framework, it empowers researchers and fact-checkers with structured methodologies to investigate and combat disinformation effectively. Explore its features and access the open-source code to tailor solutions for your unique needs. Read more OSoMe Toolkit for Social Media Research by Indiana University Observatory on Social Media | Exciting tools created by the OSoMe team made available for research, such as the OSoMe Mastodon Search or tools for network visualization and annotation.Take a look! Read more The 101 of Disinformation Detection by Carl Miller and Chloe Colliver | Institute for Strategic Dialogue (ISD) The toolkit lays out an approach that organisations can undertake to begin to track online disinformation on subjects that they care about. The report has a very low barrier to entry, with each stage achievable using either over-the-counter or free-to-use social listening tools. Read more Contribute You have published an article, a new tool or research on the exploration of digital discourses and would like to share it with the community via the Data Knowledge Hub? Add the link to the publication together with a headline and a short teaser text (approx. 50 words) directly via a GitHub pull request. You can find a detailed description of how this works in our ‘How to contribute’ chapter. Alternatively, you can also send it to us by email at upgrade.democracy@bertelsmann-stiftung.de. We look forward to your contribution!","keywords":"","version":"Next"}],"options":{"id":"default"}}